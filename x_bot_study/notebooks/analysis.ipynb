{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debunks per month",
   "id": "501fdf30f5cbf7dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:47:50.930153Z",
     "start_time": "2025-04-16T15:47:50.491969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Convert the 'fact_check_timestamp' column to datetime\n",
    "df['fact_check_timestamp'] = pd.to_datetime(df['fact_check_timestamp'])\n",
    "\n",
    "# Group by month (year-month period) and count the number of debunks per month\n",
    "debunks_per_month = df.groupby(df['fact_check_timestamp'].dt.to_period(\"M\")).size()\n",
    "\n",
    "# Print output text with counts per month\n",
    "print(\"Debunks performed per month:\")\n",
    "print(debunks_per_month)"
   ],
   "id": "199f118105d8bc27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debunks performed per month:\n",
      "fact_check_timestamp\n",
      "2024-10    15\n",
      "2024-11    40\n",
      "2024-12    15\n",
      "2025-01    26\n",
      "2025-02    22\n",
      "2025-03     1\n",
      "Freq: M, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Was the debunked tweet already community noted?",
   "id": "4918f8e724f8fb0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:49:45.991313Z",
     "start_time": "2025-04-16T15:49:45.978829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Check the unique values in the 'is_original_tweet_community_noted' column \n",
    "# to understand its structure (e.g., boolean values True/False, strings, or other)\n",
    "print(\"Unique values in 'is_original_tweet_community_noted':\", df[\"is_original_tweet_community_noted\"].unique())\n",
    "\n",
    "# Calculate value counts and percentages for the community noted column\n",
    "value_counts = df[\"is_original_tweet_community_noted\"].value_counts(dropna=False)\n",
    "total_count = df.shape[0]\n",
    "percentages = (value_counts / total_count) * 100\n",
    "\n",
    "# Print out the plots in a human-friendly way\n",
    "print(\"\\nPercentage of debunked tweets that were already community noted:\")\n",
    "for noted_status, percent in percentages.items():\n",
    "    print(f\"{noted_status}: {percent:.2f}%\")\n"
   ],
   "id": "2286b4568f4af15a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'is_original_tweet_community_noted': [False  True]\n",
      "\n",
      "Percentage of debunked tweets that were already community noted:\n",
      "False: 59.66%\n",
      "True: 40.34%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How many articles per org",
   "id": "cdc23d3add6f666a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:53:20.425172Z",
     "start_time": "2025-04-16T15:53:20.406999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Filter to rows that have a non-null fact_check_article_url\n",
    "df_articles = df.dropna(subset=[\"fact_check_article_url\"]).copy()\n",
    "\n",
    "# Count the number of fact-check articles that contain each substring (case-insensitive)\n",
    "fullfact_count = df_articles[\"fact_check_article_url\"].str.contains(\"fullfact\", case=False, na=False).sum()\n",
    "politifact_count = df_articles[\"fact_check_article_url\"].str.contains(\"politifact\", case=False, na=False).sum()\n",
    "factcheck_count = df_articles[\"fact_check_article_url\"].str.contains(\"factcheck\", case=False, na=False).sum()\n",
    "\n",
    "# Total number of fact-check article URLs present\n",
    "total_articles = len(df_articles)\n",
    "\n",
    "# Calculate percentages for each source\n",
    "percent_fullfact = (fullfact_count / total_articles) * 100\n",
    "percent_politifact = (politifact_count / total_articles) * 100\n",
    "percent_factcheck = (factcheck_count / total_articles) * 100\n",
    "\n",
    "# Print out the plots\n",
    "print(\"Fact-check articles percentage by source:\")\n",
    "print(f\"Fullfact: {percent_fullfact:.2f}%\")\n",
    "print(f\"Politifact: {percent_politifact:.2f}%\")\n",
    "print(f\"Factcheck.org: {percent_factcheck:.2f}%\")"
   ],
   "id": "5215f7a76323abea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact-check articles percentage by source:\n",
      "Fullfact: 62.18%\n",
      "Politifact: 24.37%\n",
      "Factcheck.org: 37.82%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How many tweets debunked per user",
   "id": "988fa016cf0bd7a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:33:29.119088Z",
     "start_time": "2025-04-16T16:33:29.097457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (processed CSV file)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Group by user_id and count the number of debunks per user\n",
    "user_debunk_counts = df.groupby(\"user_id\").size()\n",
    "\n",
    "# Identify users with multiple debunks (more than one tweet debunked)\n",
    "multiple_debunks = user_debunk_counts[user_debunk_counts > 1]\n",
    "\n",
    "# Identify users with a single debunk for percentage calculation\n",
    "single_debunks = user_debunk_counts[user_debunk_counts == 1]\n",
    "\n",
    "# Total unique users\n",
    "total_users = user_debunk_counts.shape[0]\n",
    "multiple_count = multiple_debunks.shape[0]\n",
    "single_count = single_debunks.shape[0]\n",
    "\n",
    "# Calculate percentages\n",
    "percentage_multiple = (multiple_count / total_users) * 100\n",
    "percentage_single = (single_count / total_users) * 100\n",
    "\n",
    "# Print the user IDs with multiple debunks along with the count of debunks per user,\n",
    "# ensuring that the user IDs are printed as full numbers (not in scientific notation).\n",
    "print(\"User IDs with multiple debunked tweets:\")\n",
    "for user_id, debunk_count in multiple_debunks.items():\n",
    "    # Convert user_id to a full integer string for printing if it is numeric.\n",
    "    try:\n",
    "        # This handles cases where user_id may be read as a float\n",
    "        user_id_str = str(int(float(user_id)))\n",
    "    except Exception:\n",
    "        # If user_id is already a string or conversion fails, just use it as-is.\n",
    "        user_id_str = str(user_id)\n",
    "    print(f\"User ID: {user_id_str}, Debunks: {debunk_count}\")\n",
    "\n",
    "# Print summary percentages\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total unique users: {total_users}\")\n",
    "print(f\"Users with multiple debunks: {multiple_count} ({percentage_multiple:.2f}%)\")\n",
    "print(f\"Users with a single debunk: {single_count} ({percentage_single:.2f}%)\")\n"
   ],
   "id": "815e3fac05488036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User IDs with multiple debunked tweets:\n",
      "User ID: 629698642, Debunks: 3\n",
      "User ID: 1214407152896369920, Debunks: 2\n",
      "User ID: 1231314387119429888, Debunks: 3\n",
      "User ID: 1247525230404219904, Debunks: 3\n",
      "User ID: 1257985162572420096, Debunks: 2\n",
      "User ID: 1711926993065860096, Debunks: 2\n",
      "User ID: 1731773353915040000, Debunks: 4\n",
      "\n",
      "Summary:\n",
      "Total unique users: 107\n",
      "Users with multiple debunks: 7 (6.54%)\n",
      "Users with a single debunk: 100 (93.46%)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistics user followers ",
   "id": "c2232bef434d8f88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:02:16.089161Z",
     "start_time": "2025-04-16T16:02:16.080212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the 'user_followers_count' column is numeric\n",
    "df['user_followers_count'] = pd.to_numeric(df['user_followers_count'], errors='coerce')\n",
    "\n",
    "# Compute descriptive statistics for the 'user_followers_count' column\n",
    "followers_stats = df['user_followers_count'].describe()\n",
    "\n",
    "# Print out the statistics with formatted values (using commas and fixed decimals)\n",
    "print(\"Statistics for 'user_followers_count':\")\n",
    "print(f\"Count: {followers_stats['count']:,.0f}\")\n",
    "print(f\"Mean: {followers_stats['mean']:,.2f}\")\n",
    "print(f\"Std Dev: {followers_stats['std']:,.2f}\")\n",
    "print(f\"Min: {followers_stats['min']:,.0f}\")\n",
    "print(f\"25%: {followers_stats['25%']:,.0f}\")\n",
    "print(f\"50% (Median): {followers_stats['50%']:,.0f}\")\n",
    "print(f\"75%: {followers_stats['75%']:,.0f}\")\n",
    "print(f\"Max: {followers_stats['max']:,.0f}\")"
   ],
   "id": "61ad7b279bfe4475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'user_followers_count':\n",
      "Count: 119\n",
      "Mean: 3,104,319.49\n",
      "Std Dev: 21,924,432.55\n",
      "Min: 165\n",
      "25%: 22,954\n",
      "50% (Median): 82,846\n",
      "75%: 364,754\n",
      "Max: 218,001,509\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Is user blue verfied",
   "id": "a627e28a984366d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:05:09.267518Z",
     "start_time": "2025-04-16T16:05:09.249703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# --- Option 1: Calculation per tweet ---\n",
    "\n",
    "# Calculate the percentages of blue verified status per tweet (assuming the column is boolean or strings like 'True'/'False')\n",
    "tweet_verification_percentages = df[\"is_user_blue_verified\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Percentage of debunks by blue verified status (per tweet):\")\n",
    "for status, percentage in tweet_verification_percentages.items():\n",
    "    print(f\"{status}: {percentage:.2f}%\")\n",
    "\n",
    "# --- Option 2: Calculation for unique users ---\n",
    "\n",
    "# Remove duplicate users based on the 'user_id' column.\n",
    "unique_users = df.drop_duplicates(subset=[\"user_id\"])\n",
    "\n",
    "# Calculate the percentages among unique users\n",
    "user_verification_percentages = unique_users[\"is_user_blue_verified\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nPercentage of unique users by blue verified status:\")\n",
    "for status, percentage in user_verification_percentages.items():\n",
    "    print(f\"{status}: {percentage:.2f}%\")\n"
   ],
   "id": "a0049ac0d5048c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of debunks by blue verified status (per tweet):\n",
      "True: 87.39%\n",
      "False: 12.61%\n",
      "\n",
      "Percentage of unique users by blue verified status:\n",
      "True: 87.85%\n",
      "False: 12.15%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Time between misinformation post and meme tweet correction",
   "id": "5f678ccc10162a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:08:11.336577Z",
     "start_time": "2025-04-16T16:08:11.320712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Convert the 'time_difference_hours' column to numeric values\n",
    "df['time_difference_hours'] = pd.to_numeric(df['time_difference_hours'], errors='coerce')\n",
    "\n",
    "# Convert hours to days by dividing by 24\n",
    "df['time_difference_days'] = df['time_difference_hours'] / 24\n",
    "\n",
    "# Compute descriptive statistics for the 'time_difference_days' column\n",
    "time_stats_days = df['time_difference_days'].describe()\n",
    "\n",
    "# Print the statistics using fixed decimal formatting for clarity (in days)\n",
    "print(\"Statistics for 'time_difference_days' (in days):\")\n",
    "print(f\"Count: {time_stats_days['count']:,.0f}\")\n",
    "print(f\"Mean: {time_stats_days['mean']:,.2f} days\")\n",
    "print(f\"Std Dev: {time_stats_days['std']:,.2f} days\")\n",
    "print(f\"Min: {time_stats_days['min']:,.2f} days\")\n",
    "print(f\"25%: {time_stats_days['25%']:,.2f} days\")\n",
    "print(f\"50% (Median): {time_stats_days['50%']:,.2f} days\")\n",
    "print(f\"75%: {time_stats_days['75%']:,.2f} days\")\n",
    "print(f\"Max: {time_stats_days['max']:,.2f} days\")"
   ],
   "id": "c625b9ec4a524512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'time_difference_days' (in days):\n",
      "Count: 119\n",
      "Mean: 14.87 days\n",
      "Std Dev: 30.97 days\n",
      "Min: 0.75 days\n",
      "25%: 4.08 days\n",
      "50% (Median): 7.00 days\n",
      "75%: 14.04 days\n",
      "Max: 259.54 days\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistics views correction posts",
   "id": "efd196e0f9df4442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:09:36.221401Z",
     "start_time": "2025-04-16T16:09:36.208651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the views columns are treated as numeric (coerce errors to NaN if necessary)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction.\n",
    "# The average is computed row-wise, ignoring NaN values (if one value is missing, it'll use the other)\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Compute descriptive statistics for the average views column\n",
    "avg_views_stats = df['avg_views'].describe()\n",
    "\n",
    "# Print the descriptive statistics with clear formatting\n",
    "print(\"Statistics for Average Views of Correction Posts (per debunk):\")\n",
    "print(f\"Count: {avg_views_stats['count']:,.0f}\")\n",
    "print(f\"Mean: {avg_views_stats['mean']:,.2f}\")\n",
    "print(f\"Std Dev: {avg_views_stats['std']:,.2f}\")\n",
    "print(f\"Min: {avg_views_stats['min']:,.0f}\")\n",
    "print(f\"25%: {avg_views_stats['25%']:,.0f}\")\n",
    "print(f\"50% (Median): {avg_views_stats['50%']:,.0f}\")\n",
    "print(f\"75%: {avg_views_stats['75%']:,.0f}\")\n",
    "print(f\"Max: {avg_views_stats['max']:,.0f}\")"
   ],
   "id": "51ab73f64ca4fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Average Views of Correction Posts (per debunk):\n",
      "Count: 119\n",
      "Mean: 12.36\n",
      "Std Dev: 11.16\n",
      "Min: 4\n",
      "25%: 7\n",
      "50% (Median): 10\n",
      "75%: 14\n",
      "Max: 108\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Percentages of misinformation post deleted by user",
   "id": "12eb7d54c56e43a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:11:07.645030Z",
     "start_time": "2025-04-16T16:11:07.624595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Calculate the percentage of misinformation posts deleted by user\n",
    "# This assumes that the column contains boolean values or a similar indicator (e.g., \"True\"/\"False\")\n",
    "deletion_percentages = df[\"misinformation_post_deleted_by_user\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the plots in a human-friendly format\n",
    "print(\"Percentage of misinformation posts deleted by user:\")\n",
    "for deletion_status, percentage in deletion_percentages.items():\n",
    "    print(f\"{deletion_status}: {percentage:.2f}%\")"
   ],
   "id": "8e9097360378fe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of misinformation posts deleted by user:\n",
      "no: 92.44%\n",
      "yes: 7.56%\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debunks per tone and ai disclosure",
   "id": "95ea43155e2144cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:12:38.256598Z",
     "start_time": "2025-04-16T16:12:38.231840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# --- Calculate percentage breakdown for x_post_tone ---\n",
    "tone_pct = df[\"x_post_tone\"].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of debunks by x_post_tone:\")\n",
    "for tone, pct in tone_pct.items():\n",
    "    print(f\"{tone}: {pct:.2f}%\")\n",
    "\n",
    "# --- Calculate percentage breakdown for x_post_ai_gen_disclosure ---\n",
    "ai_pct = df[\"x_post_ai_gen_disclosure\"].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of debunks by x_post_ai_gen_disclosure:\")\n",
    "for disclosure, pct in ai_pct.items():\n",
    "    print(f\"{disclosure}: {pct:.2f}%\")\n",
    "\n",
    "# --- Optional: Joint distribution (cross-tab) ---\n",
    "joint_pct = pd.crosstab(df[\"x_post_tone\"], df[\"x_post_ai_gen_disclosure\"], normalize=\"all\") * 100\n",
    "print(\"\\nJoint distribution percentages between x_post_tone and x_post_ai_gen_disclosure:\")\n",
    "print(joint_pct)"
   ],
   "id": "742ee45b00454512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of debunks by x_post_tone:\n",
      "factual: 57.14%\n",
      "humorous: 42.86%\n",
      "\n",
      "Percentage of debunks by x_post_ai_gen_disclosure:\n",
      "no_ai: 52.94%\n",
      "ai: 47.06%\n",
      "\n",
      "Joint distribution percentages between x_post_tone and x_post_ai_gen_disclosure:\n",
      "x_post_ai_gen_disclosure         ai      no_ai\n",
      "x_post_tone                                   \n",
      "factual                   31.092437  26.050420\n",
      "humorous                  15.966387  26.890756\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views community noted",
   "id": "9236f709880b40e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:18:44.347436Z",
     "start_time": "2025-04-16T16:18:42.386437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (processed CSV file)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Convert the view columns to numeric values (coercing errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction by averaging meme and disclaimer views\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Helper function to convert a value to a binary numeric value (1 for true, 0 for false)\n",
    "def to_binary(val):\n",
    "    if isinstance(val, bool):\n",
    "        return int(val)\n",
    "    val_str = str(val).strip().lower()\n",
    "    return 1 if val_str in ['true', 'yes', '1'] else 0\n",
    "\n",
    "# Convert 'is_original_tweet_community_noted' to a numeric binary column\n",
    "df['is_original_tweet_community_noted_numeric'] = df['is_original_tweet_community_noted'].apply(to_binary)\n",
    "\n",
    "# Drop rows with missing values in 'avg_views' or 'is_original_tweet_community_noted_numeric'\n",
    "df_corr = df.dropna(subset=['avg_views', 'is_original_tweet_community_noted_numeric'])\n",
    "\n",
    "# Determine the number of samples taken into account\n",
    "n_samples = df_corr.shape[0]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and the corresponding p-value\n",
    "corr_coeff, p_value = pearsonr(df_corr['avg_views'], df_corr['is_original_tweet_community_noted_numeric'])\n",
    "\n",
    "# Print the plots\n",
    "print(\"Correlation Type: Pearson correlation\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Correlation coefficient (r): {corr_coeff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")"
   ],
   "id": "1cf398ad4d28a70f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Type: Pearson correlation\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): 0.0583\n",
      "P-value: 5.2848e-01\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views followers count",
   "id": "c3585079a924e4ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:19:38.763659Z",
     "start_time": "2025-04-16T16:19:38.737275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the relevant columns are numeric\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "df['user_followers_count'] = pd.to_numeric(df['user_followers_count'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction (row-wise average of meme and disclaimer views)\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Drop rows with missing values in either 'avg_views' or 'user_followers_count'\n",
    "df_corr = df.dropna(subset=['avg_views', 'user_followers_count'])\n",
    "\n",
    "# Determine the number of samples used in the correlation\n",
    "n_samples = df_corr.shape[0]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and the p-value\n",
    "corr_coeff, p_value = pearsonr(df_corr['avg_views'], df_corr['user_followers_count'])\n",
    "\n",
    "# Print the plots\n",
    "print(\"Correlation Type: Pearson correlation\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Correlation coefficient (r): {corr_coeff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")"
   ],
   "id": "2d4d253b43ad97f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Type: Pearson correlation\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0763\n",
      "P-value: 4.0940e-01\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views user blue verified",
   "id": "3700c555e2810b9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:20:46.116149Z",
     "start_time": "2025-04-16T16:20:46.097300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the view columns are numeric (coerce any errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk by averaging meme and disclaimer views\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Helper function to convert the 'is_user_blue_verified' column to a binary numeric variable\n",
    "def to_binary(val):\n",
    "    if isinstance(val, bool):\n",
    "        return int(val)\n",
    "    val_str = str(val).strip().lower()\n",
    "    return 1 if val_str in ['true', 'yes', '1'] else 0\n",
    "\n",
    "df['is_user_blue_verified_numeric'] = df['is_user_blue_verified'].apply(to_binary)\n",
    "\n",
    "# Drop rows with missing values for avg_views or the verified status\n",
    "df_corr = df.dropna(subset=['avg_views', 'is_user_blue_verified_numeric'])\n",
    "\n",
    "# Determine the number of samples used in the correlation\n",
    "n_samples = df_corr.shape[0]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and the corresponding p-value\n",
    "corr_coeff, p_value = pearsonr(df_corr['avg_views'], df_corr['is_user_blue_verified_numeric'])\n",
    "\n",
    "# Print the plots\n",
    "print(\"Correlation Type: Pearson correlation\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Correlation coefficient (r): {corr_coeff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")"
   ],
   "id": "aff4e9e7eee99dfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Type: Pearson correlation\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0664\n",
      "P-value: 4.7314e-01\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views time difference between misinformation creation and correction",
   "id": "a71bd113891d5942"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:23:17.658475Z",
     "start_time": "2025-04-16T16:23:17.640225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the view columns are numeric (coercing errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk (row-wise average of meme and disclaimer views)\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Ensure the time_difference_hours column is numeric\n",
    "df['time_difference_hours'] = pd.to_numeric(df['time_difference_hours'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in avg_views or time_difference_hours\n",
    "df_corr = df.dropna(subset=['avg_views', 'time_difference_hours'])\n",
    "\n",
    "# Determine the number of samples included in the correlation analysis\n",
    "n_samples = df_corr.shape[0]\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and the corresponding p-value\n",
    "corr_coeff, p_value = pearsonr(df_corr['avg_views'], df_corr['time_difference_hours'])\n",
    "\n",
    "# Print the plots in a human-friendly format\n",
    "print(\"Correlation Type: Pearson correlation\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Correlation coefficient (r): {corr_coeff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")"
   ],
   "id": "75f55af21f31a542",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Type: Pearson correlation\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0198\n",
      "P-value: 8.3043e-01\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views fact check org",
   "id": "ff7012360d04ce06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:23:26.603639Z",
     "start_time": "2025-04-16T16:23:26.585934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the view columns are numeric\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk by averaging the meme and disclaimer views\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Define a helper function to extract the fact-checking organization from the article URL\n",
    "def extract_org(url):\n",
    "    url = str(url).lower()\n",
    "    if \"fullfact\" in url:\n",
    "        return \"fullfact\"\n",
    "    elif \"politifact\" in url:\n",
    "        return \"politifact\"\n",
    "    elif \"factcheck\" in url:\n",
    "        return \"factcheck\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a new column for the fact-checking organization\n",
    "df['fact_check_org'] = df['fact_check_article_url'].apply(extract_org)\n",
    "\n",
    "# Filter rows to only include those with a recognized organization\n",
    "df = df[df['fact_check_org'].isin([\"fullfact\", \"politifact\", \"factcheck\"])].copy()\n",
    "\n",
    "# Drop rows with missing avg_views values\n",
    "df_corr = df.dropna(subset=['avg_views'])\n",
    "\n",
    "print(\"Correlation between average views and fact-checking organizations:\\n\")\n",
    "\n",
    "# For each organization, create a binary flag and compute the correlation.\n",
    "for org in ['fullfact', 'politifact', 'factcheck']:\n",
    "    # Create a binary indicator (1 if the row's organization equals the current org, 0 otherwise)\n",
    "    df_corr[org + '_flag'] = (df_corr['fact_check_org'] == org).astype(int)\n",
    "    \n",
    "    # Determine the number of samples used in the correlation calculation\n",
    "    n_samples = df_corr.shape[0]\n",
    "    \n",
    "    # Compute Pearson (point-biserial) correlation between avg_views and the binary flag\n",
    "    r, p = pearsonr(df_corr['avg_views'], df_corr[org + '_flag'])\n",
    "    \n",
    "    # Print the plots\n",
    "    print(f\"Organization: {org}\")\n",
    "    print(\"Correlation Type: Point-Biserial (via Pearson correlation)\")\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Correlation coefficient (r): {r:.4f}\")\n",
    "    print(f\"P-value: {p:.4e}\\n\")"
   ],
   "id": "e000c44f0fc40b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between average views and fact-checking organizations:\n",
      "\n",
      "Organization: fullfact\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): 0.0235\n",
      "P-value: 7.9974e-01\n",
      "\n",
      "Organization: politifact\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0209\n",
      "P-value: 8.2165e-01\n",
      "\n",
      "Organization: factcheck\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0071\n",
      "P-value: 9.3871e-01\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views post deleted by user",
   "id": "45af371084926259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:24:43.461293Z",
     "start_time": "2025-04-16T16:24:43.440017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the view columns are numeric (coercing errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction (row-wise average of meme and disclaimer views)\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Helper function to convert the deletion status into a numeric binary variable\n",
    "def to_binary(val):\n",
    "    if isinstance(val, bool):\n",
    "        return int(val)\n",
    "    val_str = str(val).strip().lower()\n",
    "    # Interpret common truthy values as 1, else 0.\n",
    "    return 1 if val_str in ['true', 'yes', '1'] else 0\n",
    "\n",
    "# Convert 'misinformation_post_deleted_by_user' to a numeric binary variable\n",
    "df['misinformation_post_deleted_by_user_numeric'] = df['misinformation_post_deleted_by_user'].apply(to_binary)\n",
    "\n",
    "# Drop rows with missing values in either 'avg_views' or 'misinformation_post_deleted_by_user_numeric'\n",
    "df_corr = df.dropna(subset=['avg_views', 'misinformation_post_deleted_by_user_numeric'])\n",
    "\n",
    "# Determine the number of samples used in the correlation calculation\n",
    "n_samples = df_corr.shape[0]\n",
    "\n",
    "# Calculate the Pearson correlation (point-biserial correlation) between avg_views and the deletion binary variable\n",
    "corr_coeff, p_value = pearsonr(df_corr['avg_views'], df_corr['misinformation_post_deleted_by_user_numeric'])\n",
    "\n",
    "# Print the plots\n",
    "print(\"Correlation Type: Pearson (Point-Biserial) correlation\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Correlation coefficient (r): {corr_coeff:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")\n"
   ],
   "id": "42adf0c51353cdf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Type: Pearson (Point-Biserial) correlation\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): 0.3082\n",
      "P-value: 6.4876e-04\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr views tone and ai disclosure",
   "id": "e3a796565d46403c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:27:09.589176Z",
     "start_time": "2025-04-16T16:27:09.559476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the view columns are numeric (coerce errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk as the row-wise average\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Drop rows with missing values in avg_views, x_post_tone, or x_post_ai_gen_disclosure\n",
    "df_corr = df.dropna(subset=['avg_views', 'x_post_tone', 'x_post_ai_gen_disclosure'])\n",
    "\n",
    "# Create a composite column combining tone and AI gen disclosure (values become lowercase)\n",
    "df_corr['tone_ai_combo'] = df_corr['x_post_tone'].str.lower() + \"_\" + df_corr['x_post_ai_gen_disclosure'].str.lower()\n",
    "\n",
    "# Get unique combinations\n",
    "unique_combos = df_corr['tone_ai_combo'].unique()\n",
    "\n",
    "print(\"Correlation (Point-Biserial via Pearson) between average views and combinations of tone and AI gen disclosure:\\n\")\n",
    "\n",
    "# For each combination, create a binary flag and calculate the correlation with avg_views.\n",
    "for combo in unique_combos:\n",
    "    # Create binary indicator: 1 if this row has the given combo, 0 otherwise\n",
    "    dummy = (df_corr['tone_ai_combo'] == combo).astype(int)\n",
    "    \n",
    "    # Number of samples included in the analysis\n",
    "    n_samples = df_corr.shape[0]\n",
    "    \n",
    "    # Compute the Pearson (point-biserial) correlation coefficient and p-value\n",
    "    r, p = pearsonr(df_corr['avg_views'], dummy)\n",
    "    \n",
    "    print(f\"Combination: {combo}\")\n",
    "    print(\"Correlation Type: Point-Biserial (via Pearson correlation)\")\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Correlation coefficient (r): {r:.4f}\")\n",
    "    print(f\"P-value: {p:.4e}\\n\")"
   ],
   "id": "3339d40bdacf7aae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Point-Biserial via Pearson) between average views and combinations of tone and AI gen disclosure:\n",
      "\n",
      "Combination: humorous_no_ai\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0544\n",
      "P-value: 5.5645e-01\n",
      "\n",
      "Combination: humorous_ai\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0027\n",
      "P-value: 9.7715e-01\n",
      "\n",
      "Combination: factual_no_ai\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): 0.0679\n",
      "P-value: 4.6295e-01\n",
      "\n",
      "Combination: factual_ai\n",
      "Correlation Type: Point-Biserial (via Pearson correlation)\n",
      "Number of samples: 119\n",
      "Correlation coefficient (r): -0.0102\n",
      "P-value: 9.1275e-01\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Views per tone and ai disclosure",
   "id": "1f16950fdd7586e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:35:01.605050Z",
     "start_time": "2025-04-16T16:35:01.569503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (processed CSV file)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Convert the view columns to numeric values (coercing errors to NaN)\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction (row-wise average of meme and disclaimer views)\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Drop rows missing tone or AI disclosure information\n",
    "df = df.dropna(subset=['x_post_tone', 'x_post_ai_gen_disclosure'])\n",
    "\n",
    "# Create a composite column combining tone and AI disclosure (e.g., 'humorous_no_ai', 'factual_ai', etc.)\n",
    "df['tone_ai_combo'] = df['x_post_tone'].str.lower() + \"_\" + df['x_post_ai_gen_disclosure'].str.lower()\n",
    "\n",
    "# Group by the composite column and calculate aggregated view statistics\n",
    "grouped = df.groupby('tone_ai_combo')['avg_views'].agg(\n",
    "    mean_views = 'mean', \n",
    "    median_views = 'median', \n",
    "    total_views = 'sum',\n",
    "    count_debunks = 'count'\n",
    ").reset_index()\n",
    "\n",
    "# Print the aggregated views per combination of tone and AI disclosure\n",
    "print(\"Aggregated Views per Combination of Tone and AI Disclosure:\")\n",
    "print(grouped)\n"
   ],
   "id": "5918e5d3d72ccbfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Views per Combination of Tone and AI Disclosure:\n",
      "    tone_ai_combo  mean_views  median_views  total_views  count_debunks\n",
      "0      factual_ai   12.189189         11.00        451.0             37\n",
      "1   factual_no_ai   13.629032          9.00        422.5             31\n",
      "2     humorous_ai   12.289474         10.50        233.5             19\n",
      "3  humorous_no_ai   11.359375         10.75        363.5             32\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Urls for the tweets of user that were debunked more than one time",
   "id": "c20c40ef6836c8b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:37:25.941301Z",
     "start_time": "2025-04-16T16:37:25.913788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (assuming the processed file is in the data directory)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Group the data by the username column\n",
    "user_groups = df.groupby(\"misinformation_user_username\")\n",
    "\n",
    "print(\"URLs of debunked tweets for users that were debunked more than once:\\n\")\n",
    "# Iterate over each group (each user)\n",
    "for username, group in user_groups:\n",
    "    if len(group) > 1:  # Only consider users with more than one debunk\n",
    "        # Get the unique URLs for the original misinformation tweets from that group\n",
    "        urls = group[\"original_misinformation_post_url\"].dropna().unique()\n",
    "        count_urls = len(urls)\n",
    "        print(f\"Username: {username}, Count of URLs: {count_urls}\")\n",
    "        for url in urls:\n",
    "            print(f\"  {url}\")\n",
    "        print()"
   ],
   "id": "cefc264047a1eb44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs of debunked tweets for users that were debunked more than once:\n",
      "\n",
      "Username: Concerned Citizen, Count of URLs: 3\n",
      "  https://x.com/BGatesIsaPyscho/status/1855705055522881611\n",
      "  https://x.com/BGatesIsaPyscho/status/1870069733359304853\n",
      "  https://x.com/BGatesIsaPyscho/status/1887011190078844973\n",
      "\n",
      "Username: HustleBitch, Count of URLs: 3\n",
      "  https://x.com/HustleBitch_/status/1853795691362951677\n",
      "  https://x.com/HustleBitch_/status/1853882904474157337\n",
      "  https://x.com/HustleBitch_/status/1877770103229485090\n",
      "\n",
      "Username: Jack, Count of URLs: 4\n",
      "  https://x.com/jackunheard/status/1848931111147036830\n",
      "  https://x.com/jackunheard/status/1841549601188483269\n",
      "  https://x.com/jackunheard/status/1850243828424597550\n",
      "  https://x.com/jackunheard/status/1854374044524568822\n",
      "\n",
      "Username: Pugh Himple, Count of URLs: 2\n",
      "  https://x.com/gbullstein/status/1859847926752661710\n",
      "  https://x.com/GBullstein/status/1873653486476579109\n",
      "\n",
      "Username: SilencedSirs◼️, Count of URLs: 2\n",
      "  https://x.com/SilentlySirs/status/1856480964991406144\n",
      "  https://x.com/SilentlySirs/status/1878115599034810442\n",
      "\n",
      "Username: ZAINABZEHRA🇮🇷🇱🇧🇾🇪🇵🇸, Count of URLs: 3\n",
      "  https://x.com/ZAINABALI_72/status/1841182994595565706\n",
      "  https://x.com/ZAINABALI_72/status/1855338186366738622\n",
      "  https://x.com/ZAINABALI_72/status/1860744753559523455\n",
      "\n",
      "Username: jamiemcintyre, Count of URLs: 2\n",
      "  https://x.com/jamiemcintyre21/status/1853964819017126092\n",
      "  https://x.com/jamiemcintyre21/status/1861559886129308142\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Characteristics user that deleted post",
   "id": "aba39433f6f1c428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:50:49.509317Z",
     "start_time": "2025-04-16T16:50:49.481545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (processed CSV file)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure numeric conversion for the view columns and followers count\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "df['views_disclaimer_post'] = pd.to_numeric(df['views_disclaimer_post'], errors='coerce')\n",
    "df['user_followers_count'] = pd.to_numeric(df['user_followers_count'], errors='coerce')\n",
    "\n",
    "# Compute the average views per debunk correction\n",
    "df['avg_views'] = df[['views_meme_post', 'views_disclaimer_post']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Create a composite column for tone and AI disclosure (e.g., \"humorous_no_ai\", \"factual_ai\", etc.)\n",
    "df['tone_ai_combo'] = df['x_post_tone'].str.lower() + \"_\" + df['x_post_ai_gen_disclosure'].str.lower()\n",
    "\n",
    "# Filter for rows where the misinformation post was deleted by the user.\n",
    "# We handle different possible formats by converting the value to a string and then checking for common \"truthy\" values.\n",
    "df_deleted = df[df['misinformation_post_deleted_by_user']\n",
    "                .astype(str)\n",
    "                .str.lower()\n",
    "                .isin(['true', 'yes', '1'])]\n",
    "\n",
    "print(\"Debunks where the user deleted the misinformation post:\\n\")\n",
    "\n",
    "# For each debunk, print the requested information\n",
    "for index, row in df_deleted.iterrows():\n",
    "    print(f\"Misinformation Post URL: {row['original_misinformation_post_url']}\")\n",
    "    print(f\"Debunking Org Article: {row['fact_check_article_url']}\")\n",
    "    print(f\"User Followers: {row['user_followers_count']}\")\n",
    "    print(f\"Blue Verified: {row['is_user_blue_verified']}\")\n",
    "    print(f\"Average Views: {row['avg_views']}\")\n",
    "    print(f\"Tone and AI Disclosure Combo: {row['tone_ai_combo']}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "da078ea879040a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debunks where the user deleted the misinformation post:\n",
      "\n",
      "Misinformation Post URL: https://x.com/ZAINABALI_72/status/1841182994595565706\n",
      "Debunking Org Article: https://www.politifact.com/factchecks/2024/oct/03/tweets/no-this-video-does-not-show-iran-missile-attack/\n",
      "User Followers: 79593\n",
      "Blue Verified: False\n",
      "Average Views: 6.5\n",
      "Tone and AI Disclosure Combo: factual_no_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/AmericaPapaBear/status/1852123157802684476\n",
      "Debunking Org Article: https://www.politifact.com/factchecks/2024/nov/01/tweets/ohio-voting-officials-say-they-have-not-received-r/\n",
      "User Followers: 103713\n",
      "Blue Verified: True\n",
      "Average Views: 6.0\n",
      "Tone and AI Disclosure Combo: humorous_no_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/TheRoyalSerf/status/1853162645425447265\n",
      "Debunking Org Article: https://www.politifact.com/factchecks/2024/nov/04/tweets/no-michael-jordan-hadnt-endorsed-donald-trump-as-o/\n",
      "User Followers: 31277\n",
      "Blue Verified: True\n",
      "Average Views: 10.0\n",
      "Tone and AI Disclosure Combo: factual_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/umar_says_/status/1857190523330867566\n",
      "Debunking Org Article: https://fullfact.org/online/mike-tyson-palestine-flag-ai/\n",
      "User Followers: 19668\n",
      "Blue Verified: False\n",
      "Average Views: 12.5\n",
      "Tone and AI Disclosure Combo: factual_no_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/NavCom24/status/1855261797407686955\n",
      "Debunking Org Article: https://fullfact.org/online/india-brics-core-committee-removal-false/\n",
      "User Followers: 23130\n",
      "Blue Verified: True\n",
      "Average Views: 5.5\n",
      "Tone and AI Disclosure Combo: humorous_no_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/ZAINABALI_72/status/1860744753559523455\n",
      "Debunking Org Article: https://fullfact.org/online/factory-fire-west-bank-not-hezbollah-attack-israel/\n",
      "User Followers: 84218\n",
      "Blue Verified: False\n",
      "Average Views: 10.5\n",
      "Tone and AI Disclosure Combo: humorous_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/MaxessTv/status/1864775244583469070\n",
      "Debunking Org Article: https://fullfact.org/news/earthquake-california-video-taiwan-taipei-false/\n",
      "User Followers: 3132\n",
      "Blue Verified: True\n",
      "Average Views: 108.5\n",
      "Tone and AI Disclosure Combo: factual_no_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/LilMoonLambo/status/1877210618257719784\n",
      "Debunking Org Article: https://fullfact.org/us/hollyood-sign-la-burning-ai-generated/\n",
      "User Followers: 211516\n",
      "Blue Verified: True\n",
      "Average Views: 9.5\n",
      "Tone and AI Disclosure Combo: humorous_ai\n",
      "--------------------------------------------------\n",
      "Misinformation Post URL: https://x.com/montef/status/1896973767190176125\n",
      "Debunking Org Article: https://www.politifact.com/factchecks/2025/mar/05/tweets/no-trump-didnt-post-that-the-president-should-be-i/\n",
      "User Followers: 484\n",
      "Blue Verified: False\n",
      "Average Views: 50.0\n",
      "Tone and AI Disclosure Combo: factual_ai\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of top engaging memes ",
   "id": "1a3679f57dae1f6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:03:51.666137Z",
     "start_time": "2025-04-16T17:03:51.653032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input file path (processed CSV file)\n",
    "input_file_path = \"../data/x_bot_data_phase1.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the 'views_meme_post' column is numeric\n",
    "df['views_meme_post'] = pd.to_numeric(df['views_meme_post'], errors='coerce')\n",
    "\n",
    "# Sort the DataFrame by 'views_meme_post' in descending order and take the top 10 rows\n",
    "top10_memes = df.sort_values(by=\"views_meme_post\", ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Memes with the Most Views:\")\n",
    "for index, row in top10_memes.iterrows():\n",
    "    print(\"Meme Upload URL:\", row['meme_upload_url'])\n",
    "    print(\"Meme Image URL:\", row['meme_image_url'])\n",
    "    print(\"-\" * 50)\n"
   ],
   "id": "3c0510609a726227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Memes with the Most Views:\n",
      "Meme Upload URL: https://i.imgflip.com/9df62c.jpg\n",
      "Meme Image URL: https://i.imgflip.com/2odckz.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9md54c.jpg\n",
      "Meme Image URL: https://i.imgflip.com/2m20oc.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9g4y34.jpg\n",
      "Meme Image URL: https://i.imgflip.com/dzrtk.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9gnuvd.jpg\n",
      "Meme Image URL: https://i.imgflip.com/3xog.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9cgmvq.jpg\n",
      "Meme Image URL: https://i.imgflip.com/5312.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9awdpc.jpg\n",
      "Meme Image URL: https://i.imgflip.com/4acd7j.png\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9hp6wf.jpg\n",
      "Meme Image URL: https://i.imgflip.com/wxica.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9g4xe0.jpg\n",
      "Meme Image URL: https://i.imgflip.com/9iz9.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9gvypc.jpg\n",
      "Meme Image URL: https://i.imgflip.com/8tlh.jpg\n",
      "--------------------------------------------------\n",
      "Meme Upload URL: https://i.imgflip.com/9gjfa5.jpg\n",
      "Meme Image URL: https://i.imgflip.com/28s2gu.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
