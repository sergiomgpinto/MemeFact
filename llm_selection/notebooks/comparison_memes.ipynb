{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-14T15:15:22.921730Z",
     "start_time": "2025-02-14T15:15:22.915862Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:15:25.111506Z",
     "start_time": "2025-02-14T15:15:25.106619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_git_root():\n",
    "    try:\n",
    "        git_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'],\n",
    "                                           stderr=subprocess.STDOUT).decode().strip()\n",
    "        return Path(git_root)\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Warning: Not in a git repository. Using current working directory.\")\n",
    "        return Path.cwd()"
   ],
   "id": "b1f4b057c5403ea6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:16:03.278689Z",
     "start_time": "2025-02-14T15:16:03.223161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'postselection_meme_ratings.csv'\n",
    "output_dir = get_git_root() / 'user_study_effort'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "df = pd.read_csv(get_git_root() / 'llm_selection' / 'data' / 'postselection_meme_ratings_processed.csv')\n",
    "print(f'Number of answers pre filtering: {len(df)}')"
   ],
   "id": "f80aa670ff17c7b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /Users/sergiopinto/Desktop/MemeFact/user_study_effort\n",
      "Number of answers pre filtering: 110\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# I want to calculate for every claim, which were the memes with the three worst ratings( average for coherence, clarity, hilarity, persuasiveness and template appropriateness) and 3 best ratings. I want you to print the meme numbers for every claim.",
   "id": "4bef1e8b97d39838"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:21:37.147688Z",
     "start_time": "2025-02-14T15:21:37.097009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CLAIMS = 12\n",
    "NUM_MEMES = 8\n",
    "\n",
    "for claim_i in range(1, NUM_CLAIMS + 1):\n",
    "    # We'll store (meme_number, average_score) tuples here\n",
    "    meme_scores = []\n",
    "\n",
    "    for meme_i in range(1, NUM_MEMES + 1):\n",
    "        # Columns for each sub-rating of this particular meme\n",
    "        coherence_col     = f\"claim{claim_i}_meme{meme_i}_coherence\"\n",
    "        clarity_col       = f\"claim{claim_i}_meme{meme_i}_clarity\"\n",
    "        hilarity_col      = f\"claim{claim_i}_meme{meme_i}_hilarity\"\n",
    "        persuasiveness_col= f\"claim{claim_i}_meme{meme_i}_persuasiveness\"\n",
    "        template_col      = f\"claim{claim_i}_meme{meme_i}_template_conveyance\"\n",
    "\n",
    "\n",
    "        if (coherence_col not in df.columns \n",
    "            or clarity_col not in df.columns \n",
    "            or hilarity_col not in df.columns \n",
    "            or persuasiveness_col not in df.columns\n",
    "            or template_col not in df.columns):\n",
    "            # If any column is missing, skip\n",
    "            continue\n",
    "\n",
    "        # 1) For each participant (each row), get the average across these 5 sub-ratings\n",
    "        # 2) Then take the mean across all participants (rows)\n",
    "        #    so that we get one final average rating for this meme\n",
    "        rowwise_means = df[[coherence_col, clarity_col, \n",
    "                            hilarity_col, persuasiveness_col, \n",
    "                            template_col]].mean(axis=1)\n",
    "        meme_avg_score = rowwise_means.mean()  # average across participants\n",
    "\n",
    "        # Store (meme_number, that average score) so we can sort later\n",
    "        meme_scores.append((meme_i, meme_avg_score))\n",
    "\n",
    "    # Sort the memes by average rating\n",
    "    meme_scores.sort(key=lambda x: x[1])  # ascending order\n",
    "\n",
    "    # The three worst memes (lowest averages)\n",
    "    worst_3 = meme_scores[:3]\n",
    "\n",
    "    # The three best memes (highest averages)\n",
    "    best_3 = meme_scores[-3:]\n",
    "\n",
    "    print(f\"=== Claim {claim_i} ===\")\n",
    "    print(\"Worst 3 memes by average rating:\")\n",
    "    for meme_num, score in worst_3:\n",
    "        print(f\"  Meme {meme_num}: {score:.2f}\")\n",
    "\n",
    "    print(\"Best 3 memes by average rating:\")\n",
    "    for meme_num, score in best_3:\n",
    "        print(f\"  Meme {meme_num}: {score:.2f}\")\n",
    "\n",
    "    print()  # blank line for readability\n"
   ],
   "id": "de4eb4c2dc99589c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Claim 1 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 3.40\n",
      "  Meme 4: 3.43\n",
      "  Meme 6: 3.46\n",
      "Best 3 memes by average rating:\n",
      "  Meme 3: 4.06\n",
      "  Meme 1: 4.08\n",
      "  Meme 7: 4.30\n",
      "\n",
      "=== Claim 2 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 6: 3.20\n",
      "  Meme 5: 3.27\n",
      "  Meme 2: 3.54\n",
      "Best 3 memes by average rating:\n",
      "  Meme 4: 3.82\n",
      "  Meme 7: 3.90\n",
      "  Meme 3: 4.00\n",
      "\n",
      "=== Claim 3 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 7: 1.20\n",
      "  Meme 6: 3.00\n",
      "  Meme 4: 3.20\n",
      "Best 3 memes by average rating:\n",
      "  Meme 1: 3.60\n",
      "  Meme 5: 3.84\n",
      "  Meme 8: 3.89\n",
      "\n",
      "=== Claim 4 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 4: 2.10\n",
      "  Meme 3: 3.40\n",
      "  Meme 5: 3.50\n",
      "Best 3 memes by average rating:\n",
      "  Meme 7: 3.87\n",
      "  Meme 2: 3.93\n",
      "  Meme 8: 3.97\n",
      "\n",
      "=== Claim 5 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 7: 2.70\n",
      "  Meme 2: 3.10\n",
      "  Meme 3: 3.28\n",
      "Best 3 memes by average rating:\n",
      "  Meme 8: 3.71\n",
      "  Meme 5: 3.71\n",
      "  Meme 1: 3.73\n",
      "\n",
      "=== Claim 6 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 2.47\n",
      "  Meme 3: 3.58\n",
      "  Meme 7: 3.75\n",
      "Best 3 memes by average rating:\n",
      "  Meme 8: 3.89\n",
      "  Meme 2: 3.91\n",
      "  Meme 4: 4.07\n",
      "\n",
      "=== Claim 7 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 2: 3.20\n",
      "  Meme 7: 3.20\n",
      "  Meme 3: 3.50\n",
      "Best 3 memes by average rating:\n",
      "  Meme 1: 3.86\n",
      "  Meme 5: 3.94\n",
      "  Meme 6: 4.07\n",
      "\n",
      "=== Claim 8 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 2.78\n",
      "  Meme 7: 3.40\n",
      "  Meme 4: 3.50\n",
      "Best 3 memes by average rating:\n",
      "  Meme 3: 3.78\n",
      "  Meme 8: 3.78\n",
      "  Meme 1: 3.80\n",
      "\n",
      "=== Claim 9 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 2.76\n",
      "  Meme 6: 3.23\n",
      "  Meme 3: 3.30\n",
      "Best 3 memes by average rating:\n",
      "  Meme 2: 3.82\n",
      "  Meme 7: 4.00\n",
      "  Meme 4: 4.20\n",
      "\n",
      "=== Claim 10 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 3: 2.88\n",
      "  Meme 7: 3.09\n",
      "  Meme 4: 3.30\n",
      "Best 3 memes by average rating:\n",
      "  Meme 5: 3.47\n",
      "  Meme 1: 3.80\n",
      "  Meme 6: 4.09\n",
      "\n",
      "=== Claim 11 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 2.80\n",
      "  Meme 3: 3.13\n",
      "  Meme 7: 3.27\n",
      "Best 3 memes by average rating:\n",
      "  Meme 4: 3.63\n",
      "  Meme 2: 3.85\n",
      "  Meme 6: 4.00\n",
      "\n",
      "=== Claim 12 ===\n",
      "Worst 3 memes by average rating:\n",
      "  Meme 5: 2.23\n",
      "  Meme 3: 3.20\n",
      "  Meme 4: 3.40\n",
      "Best 3 memes by average rating:\n",
      "  Meme 2: 3.70\n",
      "  Meme 7: 3.95\n",
      "  Meme 6: 4.00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:26:09.136936Z",
     "start_time": "2025-02-14T15:26:09.082127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your raw data is already in a DataFrame named df\n",
    "\n",
    "NUM_CLAIMS = 12\n",
    "NUM_MEMES = 8\n",
    "\n",
    "results = []  # we will store rows for our CSV here\n",
    "\n",
    "for claim_i in range(1, NUM_CLAIMS + 1):\n",
    "    meme_scores = []\n",
    "\n",
    "    for meme_i in range(1, NUM_MEMES + 1):\n",
    "        # Construct column names\n",
    "        coherence_col      = f\"claim{claim_i}_meme{meme_i}_coherence\"\n",
    "        clarity_col        = f\"claim{claim_i}_meme{meme_i}_clarity\"\n",
    "        hilarity_col       = f\"claim{claim_i}_meme{meme_i}_hilarity\"\n",
    "        persuasiveness_col = f\"claim{claim_i}_meme{meme_i}_persuasiveness\"\n",
    "        template_col       = f\"claim{claim_i}_meme{meme_i}_template_conveyance\"\n",
    "\n",
    "        # Skip if any columns don't exist in df\n",
    "        columns_exist = all(col in df.columns for col in [\n",
    "            coherence_col, clarity_col, hilarity_col, persuasiveness_col, template_col\n",
    "        ])\n",
    "        if not columns_exist:\n",
    "            continue\n",
    "\n",
    "        # Average rating per participant (row) for these 5 sub-ratings\n",
    "        rowwise_means = df[[coherence_col, clarity_col, \n",
    "                            hilarity_col, persuasiveness_col, \n",
    "                            template_col]].mean(axis=1)\n",
    "        # Overall mean across participants\n",
    "        meme_avg_score = rowwise_means.mean()\n",
    "\n",
    "        meme_scores.append((meme_i, meme_avg_score))\n",
    "\n",
    "    # Sort from lowest to highest average\n",
    "    meme_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # The three worst\n",
    "    worst_3 = meme_scores[:3]   # first 3\n",
    "    # The three best\n",
    "    best_3  = meme_scores[-3:]  # last 3\n",
    "\n",
    "    # Save these to our plots list\n",
    "    for meme_num, score in worst_3:\n",
    "        results.append({\n",
    "            'claim': claim_i,\n",
    "            'meme': meme_num,\n",
    "            'average_score': score,\n",
    "            'evaluation_type': 'worst'\n",
    "        })\n",
    "    for meme_num, score in best_3:\n",
    "        results.append({\n",
    "            'claim': claim_i,\n",
    "            'meme': meme_num,\n",
    "            'average_score': score,\n",
    "            'evaluation_type': 'best'\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('worst_best_memes.csv', index=False)\n",
    "\n",
    "print(\"CSV saved: 'worst_best_memes.csv'\")\n"
   ],
   "id": "b24553b595d9d439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: 'worst_best_memes.csv'\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:29:10.904811Z",
     "start_time": "2025-02-14T15:29:10.860081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your raw data is already in a DataFrame named 'df'\n",
    "NUM_CLAIMS = 12\n",
    "NUM_MEMES = 8\n",
    "\n",
    "results = []\n",
    "\n",
    "for claim_i in range(1, NUM_CLAIMS + 1):\n",
    "    meme_records = []\n",
    "\n",
    "    for meme_i in range(1, NUM_MEMES + 1):\n",
    "        # Build column names for this claim+meme\n",
    "        coherence_col      = f\"claim{claim_i}_meme{meme_i}_coherence\"\n",
    "        clarity_col        = f\"claim{claim_i}_meme{meme_i}_clarity\"\n",
    "        hilarity_col       = f\"claim{claim_i}_meme{meme_i}_hilarity\"\n",
    "        persuasiveness_col = f\"claim{claim_i}_meme{meme_i}_persuasiveness\"\n",
    "        template_col       = f\"claim{claim_i}_meme{meme_i}_template_conveyance\"\n",
    "\n",
    "        # Check if all columns exist\n",
    "        columns_exist = all(col in df.columns for col in [\n",
    "            coherence_col, clarity_col, hilarity_col, \n",
    "            persuasiveness_col, template_col\n",
    "        ])\n",
    "        if not columns_exist:\n",
    "            # Skip if any dimension column missing\n",
    "            continue\n",
    "\n",
    "        # Calculate the mean rating across participants for each of the 5 dimensions\n",
    "        coherence_avg      = df[coherence_col].mean()\n",
    "        clarity_avg        = df[clarity_col].mean()\n",
    "        hilarity_avg       = df[hilarity_col].mean()\n",
    "        persuasiveness_avg = df[persuasiveness_col].mean()\n",
    "        template_avg       = df[template_col].mean()\n",
    "\n",
    "        # Overall average for this meme is the mean of the 5 dimension averages\n",
    "        overall_avg = np.mean([\n",
    "            coherence_avg, clarity_avg, \n",
    "            hilarity_avg, persuasiveness_avg, \n",
    "            template_avg\n",
    "        ])\n",
    "\n",
    "        # Store a record for sorting later\n",
    "        meme_records.append({\n",
    "            'meme': meme_i,\n",
    "            'overall_avg': overall_avg,\n",
    "            'coherence_avg': coherence_avg,\n",
    "            'clarity_avg': clarity_avg,\n",
    "            'hilarity_avg': hilarity_avg,\n",
    "            'persuasiveness_avg': persuasiveness_avg,\n",
    "            'template_avg': template_avg\n",
    "        })\n",
    "\n",
    "    # Sort by the overall average in ascending order\n",
    "    meme_records.sort(key=lambda x: x['overall_avg'])\n",
    "\n",
    "    # Take the 3 worst (lowest averages) and 3 best (highest averages)\n",
    "    worst_3 = meme_records[:3]\n",
    "    best_3  = meme_records[-3:]\n",
    "\n",
    "    # Build plots rows\n",
    "    # We'll record claim, meme, dimension-level averages, overall average, and \"worst\"/\"best\"\n",
    "    for item in worst_3:\n",
    "        results.append({\n",
    "            'claim': claim_i,\n",
    "            'meme': item['meme'],\n",
    "            'coherence_avg': item['coherence_avg'],\n",
    "            'clarity_avg': item['clarity_avg'],\n",
    "            'hilarity_avg': item['hilarity_avg'],\n",
    "            'persuasiveness_avg': item['persuasiveness_avg'],\n",
    "            'template_avg': item['template_avg'],\n",
    "            'overall_avg': item['overall_avg'],\n",
    "            'evaluation_type': 'worst'\n",
    "        })\n",
    "\n",
    "    for item in best_3:\n",
    "        results.append({\n",
    "            'claim': claim_i,\n",
    "            'meme': item['meme'],\n",
    "            'coherence_avg': item['coherence_avg'],\n",
    "            'clarity_avg': item['clarity_avg'],\n",
    "            'hilarity_avg': item['hilarity_avg'],\n",
    "            'persuasiveness_avg': item['persuasiveness_avg'],\n",
    "            'template_avg': item['template_avg'],\n",
    "            'overall_avg': item['overall_avg'],\n",
    "            'evaluation_type': 'best'\n",
    "        })\n",
    "\n",
    "# Convert to a DataFrame and save as CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('worst_best_memes_with_dimensions.csv', index=False)\n",
    "\n",
    "print(\"CSV saved as 'worst_best_memes_with_dimensions.csv'\")\n"
   ],
   "id": "7e04f059c8900531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as 'worst_best_memes_with_dimensions.csv'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T15:33:37.390607Z",
     "start_time": "2025-02-14T15:33:37.346001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume your raw data is in a DataFrame named 'df'\n",
    "NUM_CLAIMS = 12\n",
    "NUM_MEMES = 8\n",
    "\n",
    "results = []\n",
    "\n",
    "for claim_i in range(1, NUM_CLAIMS + 1):\n",
    "    meme_stats = []\n",
    "\n",
    "    for meme_i in range(1, NUM_MEMES + 1):\n",
    "        # Build column names for this (claim, meme)\n",
    "        coherence_col      = f\"claim{claim_i}_meme{meme_i}_coherence\"\n",
    "        clarity_col        = f\"claim{claim_i}_meme{meme_i}_clarity\"\n",
    "        hilarity_col       = f\"claim{claim_i}_meme{meme_i}_hilarity\"\n",
    "        persuasiveness_col = f\"claim{claim_i}_meme{meme_i}_persuasiveness\"\n",
    "        template_col       = f\"claim{claim_i}_meme{meme_i}_template_conveyance\"\n",
    "\n",
    "        # Ensure columns exist\n",
    "        required_cols = [\n",
    "            coherence_col, clarity_col, \n",
    "            hilarity_col, persuasiveness_col, \n",
    "            template_col\n",
    "        ]\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            continue\n",
    "\n",
    "        # 1) Compute the dimension-level mean across participants\n",
    "        coherence_avg      = df[coherence_col].mean()\n",
    "        clarity_avg        = df[clarity_col].mean()\n",
    "        hilarity_avg       = df[hilarity_col].mean()\n",
    "        persuasiveness_avg = df[persuasiveness_col].mean()\n",
    "        template_avg       = df[template_col].mean()\n",
    "\n",
    "        # 2) Overall average = average of these 5 dimension means\n",
    "        overall_avg = np.mean([\n",
    "            coherence_avg, clarity_avg, \n",
    "            hilarity_avg, persuasiveness_avg, \n",
    "            template_avg\n",
    "        ])\n",
    "\n",
    "        meme_stats.append({\n",
    "            'meme_number': meme_i,\n",
    "            'overall_avg': overall_avg,\n",
    "            'coherence_col': coherence_col,\n",
    "            'clarity_col': clarity_col,\n",
    "            'hilarity_col': hilarity_col,\n",
    "            'persuasiveness_col': persuasiveness_col,\n",
    "            'template_col': template_col\n",
    "        })\n",
    "\n",
    "    # Sort memes for this claim by overall average rating\n",
    "    meme_stats.sort(key=lambda x: x['overall_avg'])\n",
    "\n",
    "    # 3) The 3 worst (lowest overall average) and 3 best (highest overall average)\n",
    "    worst_3 = meme_stats[:3]\n",
    "    best_3  = meme_stats[-3:]\n",
    "\n",
    "    # 4) For each group, find dimension-level min (worst) or max (best) among participants\n",
    "    #    and build rows for the final CSV.\n",
    "\n",
    "    # --- WORS(T) GROUP ---\n",
    "    for item in worst_3:\n",
    "        mem = item['meme_number']\n",
    "        # dimension columns\n",
    "        coh_col = item['coherence_col']\n",
    "        cla_col = item['clarity_col']\n",
    "        hil_col = item['hilarity_col']\n",
    "        per_col = item['persuasiveness_col']\n",
    "        tem_col = item['template_col']\n",
    "\n",
    "        # For the \"worst\" group, we take the minimum rating among all participants\n",
    "        # for each dimension.\n",
    "        coherence_worst = df[coh_col].min()\n",
    "        clarity_worst   = df[cla_col].min()\n",
    "        hilarity_worst  = df[hil_col].min()\n",
    "        persua_worst    = df[per_col].min()\n",
    "        template_worst  = df[tem_col].min()\n",
    "\n",
    "        results.append({\n",
    "            'claim_number': claim_i,\n",
    "            'meme_number': mem,\n",
    "            'overall_average': item['overall_avg'],\n",
    "            'evaluation_type': 'worst',\n",
    "            # The actual min rating for each dimension:\n",
    "            'coherence': coherence_worst,\n",
    "            'clarity': clarity_worst,\n",
    "            'hilarity': hilarity_worst,\n",
    "            'persuasiveness': persua_worst,\n",
    "            'template_appropriateness': template_worst\n",
    "        })\n",
    "\n",
    "    # --- BEST GROUP ---\n",
    "    for item in best_3:\n",
    "        mem = item['meme_number']\n",
    "        # dimension columns\n",
    "        coh_col = item['coherence_col']\n",
    "        cla_col = item['clarity_col']\n",
    "        hil_col = item['hilarity_col']\n",
    "        per_col = item['persuasiveness_col']\n",
    "        tem_col = item['template_col']\n",
    "\n",
    "        # For the \"best\" group, we take the maximum rating among all participants\n",
    "        # for each dimension.\n",
    "        coherence_best = df[coh_col].max()\n",
    "        clarity_best   = df[cla_col].max()\n",
    "        hilarity_best  = df[hil_col].max()\n",
    "        persua_best    = df[per_col].max()\n",
    "        template_best  = df[tem_col].max()\n",
    "\n",
    "        results.append({\n",
    "            'claim_number': claim_i,\n",
    "            'meme_number': mem,\n",
    "            'overall_average': item['overall_avg'],\n",
    "            'evaluation_type': 'best',\n",
    "            # The actual max rating for each dimension:\n",
    "            'coherence': coherence_best,\n",
    "            'clarity': clarity_best,\n",
    "            'hilarity': hilarity_best,\n",
    "            'persuasiveness': persua_best,\n",
    "            'template_appropriateness': template_best\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('worst_best_memes_min_max.csv', index=False)\n",
    "\n",
    "print(\"CSV saved: 'worst_best_memes_min_max.csv'\")\n"
   ],
   "id": "b81c183757061c4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: 'worst_best_memes_min_max.csv'\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
