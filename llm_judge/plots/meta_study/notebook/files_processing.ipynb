{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.325987Z",
     "start_time": "2025-03-07T16:22:50.323017Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.336319Z",
     "start_time": "2025-03-07T16:22:50.334153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_working_dir():\n",
    "    return Path.cwd()"
   ],
   "id": "199832e517e5b3a7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.345447Z",
     "start_time": "2025-03-07T16:22:50.342206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_rejected_ids(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            rejected_ids = [line.strip() for line in f.readlines()]\n",
    "        return rejected_ids\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Rejected IDs file not found at {file_path}\")\n",
    "        return []\n",
    "\n",
    "def check_duplicates(df, dataset_name):\n",
    "    duplicate_ids = df[df.duplicated(subset=[prolific_id_column], keep=False)][prolific_id_column].tolist()\n",
    "    if duplicate_ids:\n",
    "        print(f\"Found duplicate IDs in {dataset_name} dataset: {duplicate_ids}\")\n",
    "    return duplicate_ids"
   ],
   "id": "22d1fed7c7c51629",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.374851Z",
     "start_time": "2025-03-07T16:22:50.351043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_dir = get_working_dir() / 'data' / 'processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "df_t = pd.read_csv(get_working_dir() / 'data' / 't.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(get_working_dir() /  'data' / 'm.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(get_working_dir() /  'data' / 'm_t.csv', encoding='utf-8')\n",
    "\n",
    "rejected_t = read_rejected_ids(get_working_dir() / 'data' / 'rejected_ids_t.txt')\n",
    "rejected_m = read_rejected_ids(get_working_dir() / 'data' / 'rejected_ids_m.txt')\n",
    "rejected_m_t = read_rejected_ids(get_working_dir() / 'data' / 'rejected_ids_m_t.txt')"
   ],
   "id": "d7a32eb61fcc8ae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sergiopinto/Desktop/MemeFact/meta_study\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.440558Z",
     "start_time": "2025-03-07T16:22:50.437793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Dataset t: {df_t.shape[0]} rows, {df_t.shape[1]} columns\")\n",
    "print(f\"Dataset m: {df_m.shape[0]} rows, {df_m.shape[1]} columns\")\n",
    "print(f\"Dataset m_t: {df_m_t.shape[0]} rows, {df_m_t.shape[1]} columns\")\n",
    "print(f\"Rejected IDs - t: {len(rejected_t)}\")\n",
    "print(f\"Rejected IDs - m: {len(rejected_m)}\")\n",
    "print(f\"Rejected IDs - m_t: {len(rejected_m_t)}\")"
   ],
   "id": "7c6ac6e9eb9c8942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset t: 118 rows, 132 columns\n",
      "Dataset m: 110 rows, 136 columns\n",
      "Dataset m_t: 107 rows, 133 columns\n",
      "Rejected IDs - t: 8\n",
      "Rejected IDs - m: 9\n",
      "Rejected IDs - m_t: 5\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:22:50.525822Z",
     "start_time": "2025-03-07T16:22:50.505884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prolific_id_column = \"Please enter your prolific ID.\"\n",
    "\n",
    "# Check for duplicates in each dataset\n",
    "duplicate_t = check_duplicates(df_t, \"t\")\n",
    "duplicate_m = check_duplicates(df_m, \"m\")\n",
    "duplicate_m_t = check_duplicates(df_m_t, \"m_t\")\n",
    "\n",
    "# Remove rejected IDs from each dataset\n",
    "df_t_clean = df_t[~df_t[prolific_id_column].isin(rejected_t)]\n",
    "df_m_clean = df_m[~df_m[prolific_id_column].isin(rejected_m)]\n",
    "df_m_t_clean = df_m_t[~df_m_t[prolific_id_column].isin(rejected_m_t)]\n",
    "\n",
    "# If there are duplicates, keep the first occurrence\n",
    "if duplicate_t:\n",
    "    df_t_clean = df_t_clean.drop_duplicates(subset=[prolific_id_column], keep='first')\n",
    "if duplicate_m:\n",
    "    df_m_clean = df_m_clean.drop_duplicates(subset=[prolific_id_column], keep='first')\n",
    "if duplicate_m_t:\n",
    "    df_m_t_clean = df_m_t_clean.drop_duplicates(subset=[prolific_id_column], keep='first')\n",
    "\n",
    "# Display cleaned dataset information\n",
    "print(\"\\nAfter removing rejected IDs and duplicates:\")\n",
    "print(f\"Dataset t: {df_t_clean.shape[0]} rows, {df_t_clean.shape[1]} columns\")\n",
    "print(f\"Dataset m: {df_m_clean.shape[0]} rows, {df_m_clean.shape[1]} columns\")\n",
    "print(f\"Dataset m_t: {df_m_t_clean.shape[0]} rows, {df_m_t_clean.shape[1]} columns\")\n",
    "\n",
    "# Save cleaned datasets to the processed directory\n",
    "df_t_clean.to_csv(processed_dir / 't_phase1.csv', index=False, encoding='utf-8')\n",
    "df_m_clean.to_csv(processed_dir / 'm_phase1.csv', index=False, encoding='utf-8')\n",
    "df_m_t_clean.to_csv(processed_dir / 'm_t_phase1.csv', index=False, encoding='utf-8')"
   ],
   "id": "edb0da2f5c92ab51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After removing rejected IDs and duplicates:\n",
      "Dataset t: 110 rows, 132 columns\n",
      "Dataset m: 101 rows, 136 columns\n",
      "Dataset m_t: 102 rows, 133 columns\n",
      "\n",
      "Checking for missing values in key columns:\n",
      "t dataset missing values: {'Please enter your prolific ID.': 0, 'Please indicate your age group.': 0}\n",
      "m dataset missing values: {'Please enter your prolific ID.': 0, 'Please indicate your age group.': 0}\n",
      "m_t dataset missing values: {'Please enter your prolific ID.': 0, 'Please indicate your age group.': 0}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:47:31.241203Z",
     "start_time": "2025-03-07T16:47:31.186949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loading Phase 1 processed datasets...\")\n",
    "df_t = pd.read_csv(processed_dir / 't_phase1.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(processed_dir / 'm_phase1.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(processed_dir / 'm_t_phase1.csv', encoding='utf-8')\n",
    "\n",
    "# Define columns to rename (same for all datasets)\n",
    "rename_map = {\n",
    "    \"Please enter your prolific ID.\": \"prolific_id\",\n",
    "    \"Please indicate your age group.\": \"age_group\",\n",
    "    \"Please indicate your citizenship status in the United States.\": \"citizenship_status\",\n",
    "    \"Please rate your proficiency in English language comprehension.\": \"english_proficiency\",\n",
    "    \"Please indicate your highest completed level of education.\": \"education_level\",\n",
    "    \"Please indicate your political orientation.\\n\\n'Moderately Liberal' means someone who supports gradual social progress, a mix of government intervention and market freedom, and strong individual rights. \\n\\n'Moderately Conservative' means someone who emphasizes tradition, limited government, free markets, and cautious, gradual change.\": \"political_orientation\",\n",
    "    \"Please indicate how frequently you interact with political content on social media platforms.\\n\\nInteracting includes any form of engagement such as liking/reacting, sharing (to your stories), reposting, commenting, or messaging content to others.\\n\\nPolitical content includes news stories, videos, memes, posts, stories, tweets, or any other material containing political claims, statements, or discussions about policies, elections, political figures, or social issues with political implications.\": \"engagement_with_political_content\",\n",
    "    \"How would you rate your familiarity with internet meme culture, specifically your ability to understand the context and meaning of image macro memes?\": \"meme_culture_familiarity\"\n",
    "}\n",
    "\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    \"I hereby confirm that I have read the Data Collection & Privacy Information and consent to take part in this study by selecting the 'I agree' option below:\",\n",
    "    \"Timestamp\",\n",
    "    \"Please select 'Often' to show you are paying attention to this question.\",\n",
    "    \"Please provide your email address if you'd like to be contacted for future studies.\"\n",
    "]\n",
    "\n",
    "# Define demographic columns for separate files (using original column names before renaming)\n",
    "demographic_columns = [\n",
    "    \"Please enter your prolific ID.\",\n",
    "    \"Please indicate your age group.\",\n",
    "    \"Please indicate your citizenship status in the United States.\",\n",
    "    \"Please rate your proficiency in English language comprehension.\",\n",
    "    \"Please indicate your highest completed level of education.\",\n",
    "    \"Please indicate your political orientation.\\n\\n'Moderately Liberal' means someone who supports gradual social progress, a mix of government intervention and market freedom, and strong individual rights. \\n\\n'Moderately Conservative' means someone who emphasizes tradition, limited government, free markets, and cautious, gradual change.\",\n",
    "    \"Please indicate how frequently you interact with political content on social media platforms.\\n\\nInteracting includes any form of engagement such as liking/reacting, sharing (to your stories), reposting, commenting, or messaging content to others.\\n\\nPolitical content includes news stories, videos, memes, posts, stories, tweets, or any other material containing political claims, statements, or discussions about policies, elections, political figures, or social issues with political implications.\"\n",
    "]\n",
    "\n",
    "# Add meme familiarity column for m and m_t datasets\n",
    "m_mt_demographic_columns = demographic_columns + [\n",
    "    \"How would you rate your familiarity with internet meme culture, specifically your ability to understand the context and meaning of image macro memes?\"\n",
    "]\n",
    "\n",
    "# Function to process datasets for phase 2\n",
    "def process_phase2(df, dataset_name, demographic_cols):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for Phase 2...\")\n",
    "    \n",
    "    # Create a copy of the original dataframe to work with\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create a copy of the demographics for separate file\n",
    "    if dataset_name in ['m', 'm_t']:\n",
    "        demo_df = df_copy[demographic_cols].copy()\n",
    "    else:\n",
    "        demo_df = df_copy[demographic_columns].copy()\n",
    "    \n",
    "    # Rename demographic columns\n",
    "    demo_df = demo_df.rename(columns=rename_map)\n",
    "    \n",
    "    # Save demographics file with renamed prolific_id\n",
    "    demo_file_path = processed_dir / f\"{dataset_name}_demographics.csv\"\n",
    "    demo_df.to_csv(demo_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Demographics file saved to {demo_file_path}\")\n",
    "    \n",
    "    # Process main dataset for Phase 2\n",
    "    # 1. Remove specified columns that exist in the dataset\n",
    "    for col in columns_to_remove:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy = df_copy.drop(columns=[col])\n",
    "    \n",
    "    # 2. Rename columns including prolific_id\n",
    "    df_copy = df_copy.rename(columns=rename_map)\n",
    "    \n",
    "    # Save Phase 2 processed file (with prolific_id included)\n",
    "    phase2_file_path = processed_dir / f\"{dataset_name}_phase2.csv\"\n",
    "    df_copy.to_csv(phase2_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 2 processed file saved to {phase2_file_path}\")\n",
    "    \n",
    "    return demo_df, df_copy\n",
    "\n",
    "# Process each dataset\n",
    "t_demo, df_t_phase2 = process_phase2(df_t, 't', demographic_columns)\n",
    "m_demo, df_m_phase2 = process_phase2(df_m, 'm', m_mt_demographic_columns)\n",
    "m_t_demo, df_m_t_phase2 = process_phase2(df_m_t, 'm_t', m_mt_demographic_columns)\n",
    "\n",
    "# Verify column renaming in demographic files\n",
    "print(\"\\nVerifying column renaming in demographic files:\")\n",
    "demographic_renamed_columns = [\n",
    "    \"prolific_id\", \"age_group\", \"citizenship_status\", \"english_proficiency\", \n",
    "    \"education_level\", \"political_orientation\", \"engagement_with_political_content\"\n",
    "]\n",
    "\n",
    "for dataset_name, df in [(\"t\", t_demo), (\"m\", m_demo), (\"m_t\", m_t_demo)]:\n",
    "    print(f\"\\n{dataset_name}_demographics.csv:\")\n",
    "    for col in demographic_renamed_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"  ✓ {col}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {col} not found\")\n",
    "    \n",
    "    # Check meme familiarity column for m and m_t datasets\n",
    "    if dataset_name in ['m', 'm_t']:\n",
    "        if \"meme_culture_familiarity\" in df.columns:\n",
    "            print(f\"  ✓ meme_culture_familiarity\")\n",
    "        else:\n",
    "            print(f\"  ✗ meme_culture_familiarity not found\")\n",
    "\n",
    "# Verify prolific_id is retained in phase 2 files\n",
    "print(\"\\nVerifying prolific_id is retained in phase 2 files:\")\n",
    "for dataset_name, df in [(\"t\", df_t_phase2), (\"m\", df_m_phase2), (\"m_t\", df_m_t_phase2)]:\n",
    "    if \"prolific_id\" in df.columns:\n",
    "        print(f\"  ✓ {dataset_name}_phase2.csv has prolific_id column\")\n",
    "    else:\n",
    "        print(f\"  ✗ {dataset_name}_phase2.csv missing prolific_id column\")\n",
    "\n",
    "# Count columns removed\n",
    "print(\"\\nColumns removed from each dataset:\")\n",
    "for dataset_name, original, processed in [\n",
    "    (\"t\", df_t, df_t_phase2), \n",
    "    (\"m\", df_m, df_m_phase2), \n",
    "    (\"m_t\", df_m_t, df_m_t_phase2)\n",
    "]:\n",
    "    removed_count = len(original.columns) - len(processed.columns)\n",
    "    print(f\"  {dataset_name}: {removed_count} columns removed\")\n",
    "\n",
    "print(\"\\nPhase 2 processing completed successfully.\")"
   ],
   "id": "a9b85fe9232582a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phase 1 processed datasets...\n",
      "\n",
      "Processing t dataset for Phase 2...\n",
      "  Demographics file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_demographics.csv\n",
      "  Phase 2 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_phase2.csv\n",
      "\n",
      "Processing m dataset for Phase 2...\n",
      "  Demographics file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_demographics.csv\n",
      "  Phase 2 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase2.csv\n",
      "\n",
      "Processing m_t dataset for Phase 2...\n",
      "  Demographics file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_demographics.csv\n",
      "  Phase 2 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_phase2.csv\n",
      "\n",
      "Verifying column renaming in demographic files:\n",
      "\n",
      "t_demographics.csv:\n",
      "  ✓ prolific_id\n",
      "  ✓ age_group\n",
      "  ✓ citizenship_status\n",
      "  ✓ english_proficiency\n",
      "  ✓ education_level\n",
      "  ✓ political_orientation\n",
      "  ✓ engagement_with_political_content\n",
      "\n",
      "m_demographics.csv:\n",
      "  ✓ prolific_id\n",
      "  ✓ age_group\n",
      "  ✓ citizenship_status\n",
      "  ✓ english_proficiency\n",
      "  ✓ education_level\n",
      "  ✓ political_orientation\n",
      "  ✓ engagement_with_political_content\n",
      "  ✓ meme_culture_familiarity\n",
      "\n",
      "m_t_demographics.csv:\n",
      "  ✓ prolific_id\n",
      "  ✓ age_group\n",
      "  ✓ citizenship_status\n",
      "  ✓ english_proficiency\n",
      "  ✓ education_level\n",
      "  ✓ political_orientation\n",
      "  ✓ engagement_with_political_content\n",
      "  ✓ meme_culture_familiarity\n",
      "\n",
      "Verifying prolific_id is retained in phase 2 files:\n",
      "  ✓ t_phase2.csv has prolific_id column\n",
      "  ✓ m_phase2.csv has prolific_id column\n",
      "  ✓ m_t_phase2.csv has prolific_id column\n",
      "\n",
      "Columns removed from each dataset:\n",
      "  t: 4 columns removed\n",
      "  m: 4 columns removed\n",
      "  m_t: 4 columns removed\n",
      "\n",
      "Phase 2 processing completed successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:53:40.401762Z",
     "start_time": "2025-03-07T16:53:40.343272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load phase 2 processed datasets\n",
    "df_t = pd.read_csv(processed_dir / 't_phase2.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(processed_dir / 'm_phase2.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(processed_dir / 'm_t_phase2.csv', encoding='utf-8')\n",
    "\n",
    "# Load demographic files to identify demographic columns\n",
    "df_t_demo = pd.read_csv(processed_dir / 't_demographics.csv', encoding='utf-8')\n",
    "df_m_demo = pd.read_csv(processed_dir / 'm_demographics.csv', encoding='utf-8')\n",
    "df_m_t_demo = pd.read_csv(processed_dir / 'm_t_demographics.csv', encoding='utf-8')\n",
    "\n",
    "# Define demographic columns to remove (excluding prolific_id)\n",
    "demographic_columns = [\n",
    "    \"age_group\",\n",
    "    \"citizenship_status\",\n",
    "    \"english_proficiency\",\n",
    "    \"education_level\",\n",
    "    \"political_orientation\",\n",
    "    \"engagement_with_political_content\"\n",
    "]\n",
    "\n",
    "# Add meme familiarity for m and m_t datasets\n",
    "meme_column = [\"meme_culture_familiarity\"]\n",
    "\n",
    "# Define emotions for renaming\n",
    "emotions = ['upset', 'hostile', 'alert', 'ashamed', 'inspired', \n",
    "            'nervous', 'determined', 'attentive', 'afraid', 'active']\n",
    "\n",
    "# Function to process datasets for phase 3\n",
    "def process_phase3(df, dataset_name, demo_columns, meme_column):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for Phase 3...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Identify pre and post affect columns based on text patterns\n",
    "    pre_affect_cols = []\n",
    "    post_affect_cols = []\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if \"normally feel\" in col.lower():\n",
    "            pre_affect_cols.append(col)\n",
    "        elif \"prior to the study\" in col.lower():\n",
    "            post_affect_cols.append(col)\n",
    "    \n",
    "    print(f\"  Found {len(pre_affect_cols)} pre-affect columns and {len(post_affect_cols)} post-affect columns\")\n",
    "    \n",
    "    # Extract affect columns with prolific_id for the separate file\n",
    "    affect_df = df_copy[['prolific_id'] + pre_affect_cols + post_affect_cols].copy()\n",
    "    \n",
    "    # Rename affect columns to standard names\n",
    "    # For pre-affect columns\n",
    "    for emotion in emotions:\n",
    "        for col in pre_affect_cols:\n",
    "            if f\"feel {emotion}\" in col.lower():\n",
    "                affect_df = affect_df.rename(columns={col: f\"pre_{emotion}\"})\n",
    "    \n",
    "    # For post-affect columns\n",
    "    for emotion in emotions:\n",
    "        for col in post_affect_cols:\n",
    "            if f\"feel {emotion}\" in col.lower():\n",
    "                affect_df = affect_df.rename(columns={col: f\"post_{emotion}\"})\n",
    "    \n",
    "    # Save affect data to separate file\n",
    "    affect_file_path = processed_dir / f\"{dataset_name}_affect.csv\"\n",
    "    affect_df.to_csv(affect_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Affect data saved to {affect_file_path}\")\n",
    "    \n",
    "    # Remove demographic columns from main dataset (except prolific_id)\n",
    "    columns_to_remove = set(demo_columns)\n",
    "    \n",
    "    # Add meme familiarity to columns to remove if it exists and is in the dataset\n",
    "    if dataset_name in ['m', 'm_t'] and meme_column[0] in df_copy.columns:\n",
    "        columns_to_remove.update(meme_column)\n",
    "    \n",
    "    # Remove affected columns from main dataset\n",
    "    columns_to_remove.update(pre_affect_cols)\n",
    "    columns_to_remove.update(post_affect_cols)\n",
    "    \n",
    "    # Ensure all columns to remove exist in the dataframe\n",
    "    columns_to_remove = [col for col in columns_to_remove if col in df_copy.columns]\n",
    "    \n",
    "    # Remove columns\n",
    "    df_phase3 = df_copy.drop(columns=columns_to_remove)\n",
    "    \n",
    "    # Save Phase 3 processed file\n",
    "    phase3_file_path = processed_dir / f\"{dataset_name}_phase3.csv\"\n",
    "    df_phase3.to_csv(phase3_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 3 processed file saved to {phase3_file_path}\")\n",
    "    \n",
    "    return affect_df, df_phase3\n",
    "\n",
    "# Process each dataset\n",
    "t_affect, df_t_phase3 = process_phase3(df_t, 't', demographic_columns, meme_column)\n",
    "m_affect, df_m_phase3 = process_phase3(df_m, 'm', demographic_columns, meme_column)\n",
    "m_t_affect, df_m_t_phase3 = process_phase3(df_m_t, 'm_t', demographic_columns, meme_column)\n",
    "\n",
    "# Verify column renaming in affect files\n",
    "print(\"\\nVerifying column renaming in affect files:\")\n",
    "expected_affect_columns = ['prolific_id'] + [f\"pre_{e}\" for e in emotions] + [f\"post_{e}\" for e in emotions]\n",
    "\n",
    "for dataset_name, df in [(\"t\", t_affect), (\"m\", m_affect), (\"m_t\", m_t_affect)]:\n",
    "    print(f\"\\n{dataset_name}_affect.csv columns check:\")\n",
    "    \n",
    "    # Count matching columns\n",
    "    found_columns = [col for col in expected_affect_columns if col in df.columns]\n",
    "    print(f\"  Found {len(found_columns)} of {len(expected_affect_columns)} expected columns\")\n",
    "    \n",
    "    # Check for any missing columns\n",
    "    missing_columns = [col for col in expected_affect_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"  Missing columns: {missing_columns}\")\n",
    "    else:\n",
    "        print(f\"  All expected columns are present\")\n",
    "\n",
    "# Verify removal of affect and demographic columns in phase 3 files\n",
    "print(\"\\nVerifying removal of affect and demographic columns in phase 3 files:\")\n",
    "for dataset_name, df in [(\"t\", df_t_phase3), (\"m\", df_m_phase3), (\"m_t\", df_m_t_phase3)]:\n",
    "    print(f\"\\n{dataset_name}_phase3.csv:\")\n",
    "    \n",
    "    # Check if prolific_id is still present\n",
    "    if \"prolific_id\" in df.columns:\n",
    "        print(f\"  ✓ prolific_id column is present\")\n",
    "    else:\n",
    "        print(f\"  ✗ prolific_id column is missing\")\n",
    "    \n",
    "    # Check that affect columns are removed\n",
    "    affect_columns_present = []\n",
    "    for col in df.columns:\n",
    "        if any(emotion in col.lower() for emotion in emotions) and any(prefix in col.lower() for prefix in [\"normally feel\", \"prior to the study\"]):\n",
    "            affect_columns_present.append(col)\n",
    "    \n",
    "    if affect_columns_present:\n",
    "        print(f\"  ✗ Found {len(affect_columns_present)} affect columns that should have been removed\")\n",
    "        print(f\"    {affect_columns_present[:5]}{'...' if len(affect_columns_present) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All affect columns have been removed\")\n",
    "    \n",
    "    # Check that demographic columns are removed\n",
    "    demo_columns_present = [col for col in demographic_columns if col in df.columns]\n",
    "    if demo_columns_present:\n",
    "        print(f\"  ✗ Found demographic columns that should have been removed: {demo_columns_present}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All demographic columns have been removed\")\n",
    "\n",
    "print(\"\\nPhase 3 processing completed successfully.\")"
   ],
   "id": "12ed996abcce8428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing t dataset for Phase 3...\n",
      "  Found 10 pre-affect columns and 10 post-affect columns\n",
      "  Affect data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_affect.csv\n",
      "  Phase 3 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_phase3.csv\n",
      "\n",
      "Processing m dataset for Phase 3...\n",
      "  Found 10 pre-affect columns and 10 post-affect columns\n",
      "  Affect data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_affect.csv\n",
      "  Phase 3 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase3.csv\n",
      "\n",
      "Processing m_t dataset for Phase 3...\n",
      "  Found 10 pre-affect columns and 10 post-affect columns\n",
      "  Affect data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_affect.csv\n",
      "  Phase 3 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_phase3.csv\n",
      "\n",
      "Verifying column renaming in affect files:\n",
      "\n",
      "t_affect.csv columns check:\n",
      "  Found 21 of 21 expected columns\n",
      "  All expected columns are present\n",
      "\n",
      "m_affect.csv columns check:\n",
      "  Found 21 of 21 expected columns\n",
      "  All expected columns are present\n",
      "\n",
      "m_t_affect.csv columns check:\n",
      "  Found 21 of 21 expected columns\n",
      "  All expected columns are present\n",
      "\n",
      "Verifying removal of affect and demographic columns in phase 3 files:\n",
      "\n",
      "t_phase3.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All affect columns have been removed\n",
      "  ✓ All demographic columns have been removed\n",
      "\n",
      "m_phase3.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All affect columns have been removed\n",
      "  ✓ All demographic columns have been removed\n",
      "\n",
      "m_t_phase3.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All affect columns have been removed\n",
      "  ✓ All demographic columns have been removed\n",
      "\n",
      "Phase 3 processing completed successfully.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T17:06:36.829561Z",
     "start_time": "2025-03-07T17:06:36.768591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_t = pd.read_csv(processed_dir / 't_phase3.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(processed_dir / 'm_phase3.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(processed_dir / 'm_t_phase3.csv', encoding='utf-8')\n",
    "\n",
    "# Define the claim columns and their new names\n",
    "claim_mapping = {\n",
    "    \"Government officials have manipulated stock prices to hide scandals.\": \"claim_stock_manipulation\",\n",
    "    \"New study: Left-wingers are more likely to lie to get a higher salary.\": \"claim_left_wing_salary_lie\",\n",
    "    \"Certain vaccines are loaded with dangerous chemicals and toxins.\": \"claim_vaccine_toxins\",\n",
    "    \"The government is knowingly spreading disease through the airwaves and food supply.\": \"claim_government_disease_spread\",\n",
    "    \"Attitudes toward the European Union are largely positive, both within Europe and outside it.\": \"claim_eu_positive_attitudes\",\n",
    "    \"Hyatt will remove small bottles from hotel bathrooms by 2021.\": \"claim_hyatt_small_bottles\",\n",
    "    \"Republicans divided in views of Trump's conduct, democrats are broadly critical.\": \"claim_republicans_trump_conduct\",\n",
    "    \"Global warming age gap: Younger Americans most worried.\": \"claim_climate_worry_age_gap\"\n",
    "}\n",
    "\n",
    "# Function to find the best matching column name\n",
    "def find_matching_column(columns, claim_text):\n",
    "    exact_match = [col for col in columns if col == claim_text]\n",
    "    if exact_match:\n",
    "        return exact_match[0]\n",
    "    \n",
    "    # Try partial matching if no exact match\n",
    "    partial_matches = [col for col in columns if claim_text.split(':')[0] in col]\n",
    "    if partial_matches:\n",
    "        return partial_matches[0]\n",
    "    \n",
    "    # Try even more fuzzy matching for specific claims\n",
    "    if \"Trump\" in claim_text:\n",
    "        trump_matches = [col for col in columns if \"Trump\" in col]\n",
    "        if trump_matches:\n",
    "            return trump_matches[0]\n",
    "    \n",
    "    if \"warming\" in claim_text:\n",
    "        warming_matches = [col for col in columns if \"warming\" in col]\n",
    "        if warming_matches:\n",
    "            return warming_matches[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to process datasets for phase 4\n",
    "def process_phase4(df, dataset_name, claim_mapping):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for Phase 4...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Print all column names for debugging\n",
    "    print(f\"  Available columns in dataset: {len(df_copy.columns)}\")\n",
    "    \n",
    "    # Extract claim columns with prolific_id\n",
    "    claims_df = df_copy[['prolific_id']].copy()\n",
    "    \n",
    "    # Find and extract claim columns\n",
    "    extracted_claims = []\n",
    "    not_found_claims = []\n",
    "    \n",
    "    for original_name, new_name in claim_mapping.items():\n",
    "        # First try direct matching\n",
    "        if original_name in df_copy.columns:\n",
    "            claims_df[new_name] = df_copy[original_name]\n",
    "            extracted_claims.append(original_name)\n",
    "        else:\n",
    "            # Try fuzzy matching if direct match fails\n",
    "            match = find_matching_column(df_copy.columns, original_name)\n",
    "            if match:\n",
    "                claims_df[new_name] = df_copy[match]\n",
    "                extracted_claims.append(match)\n",
    "                print(f\"  Matched '{original_name}' to column '{match}'\")\n",
    "            else:\n",
    "                not_found_claims.append(original_name)\n",
    "    \n",
    "    print(f\"  Found and extracted {len(extracted_claims)} of {len(claim_mapping)} claim columns\")\n",
    "    \n",
    "    if not_found_claims:\n",
    "        print(f\"  Could not find columns for these claims:\")\n",
    "        for claim in not_found_claims:\n",
    "            print(f\"    - {claim}\")\n",
    "    \n",
    "    # Print the first few characters of each found column for verification\n",
    "    for col in extracted_claims:\n",
    "        if col in df_copy.columns:\n",
    "            first_value = str(df_copy[col].iloc[0])\n",
    "            print(f\"  Column '{col}' found with first value: {first_value[:30]}...\")\n",
    "    \n",
    "    # Remove claim columns from main dataset\n",
    "    df_phase4 = df_copy.drop(columns=extracted_claims)\n",
    "    \n",
    "    # Save claims data to separate file\n",
    "    claims_file_path = processed_dir / f\"{dataset_name}_claims.csv\"\n",
    "    claims_df.to_csv(claims_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Claims data saved to {claims_file_path}\")\n",
    "    \n",
    "    # Save Phase 4 processed file\n",
    "    phase4_file_path = processed_dir / f\"{dataset_name}_phase4.csv\"\n",
    "    df_phase4.to_csv(phase4_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 4 processed file saved to {phase4_file_path}\")\n",
    "    \n",
    "    return claims_df, df_phase4, not_found_claims\n",
    "\n",
    "# Process each dataset\n",
    "t_claims, df_t_phase4, t_not_found = process_phase4(df_t, 't', claim_mapping)\n",
    "m_claims, df_m_phase4, m_not_found = process_phase4(df_m, 'm', claim_mapping)\n",
    "m_t_claims, df_m_t_phase4, m_t_not_found = process_phase4(df_m_t, 'm_t', claim_mapping)\n",
    "\n",
    "# Verify claim extraction\n",
    "print(\"\\nVerifying claim extraction in claim files:\")\n",
    "expected_claim_columns = ['prolific_id'] + list(claim_mapping.values())\n",
    "\n",
    "for dataset_name, df in [(\"t\", t_claims), (\"m\", m_claims), (\"m_t\", m_t_claims)]:\n",
    "    print(f\"\\n{dataset_name}_claims.csv columns check:\")\n",
    "    \n",
    "    # Count matching columns\n",
    "    found_columns = [col for col in expected_claim_columns if col in df.columns]\n",
    "    print(f\"  Found {len(found_columns) - 1} of {len(expected_claim_columns) - 1} expected claim columns\")\n",
    "    \n",
    "    # Check for any missing claim columns\n",
    "    missing_columns = [col for col in expected_claim_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"  Missing columns: {missing_columns}\")\n",
    "    else:\n",
    "        print(f\"  All expected columns are present\")\n",
    "\n",
    "# Verify removal of claim columns in phase 4 files\n",
    "print(\"\\nVerifying removal of claim columns in phase 4 files:\")\n",
    "for dataset_name, df, not_found in [\n",
    "    (\"t\", df_t_phase4, t_not_found), \n",
    "    (\"m\", df_m_phase4, m_not_found), \n",
    "    (\"m_t\", df_m_t_phase4, m_t_not_found)\n",
    "]:\n",
    "    print(f\"\\n{dataset_name}_phase4.csv:\")\n",
    "    \n",
    "    # Check if prolific_id is still present\n",
    "    if \"prolific_id\" in df.columns:\n",
    "        print(f\"  ✓ prolific_id column is present\")\n",
    "    else:\n",
    "        print(f\"  ✗ prolific_id column is missing\")\n",
    "    \n",
    "    # Check for any remaining claim-related columns\n",
    "    claim_words = ['government', 'vaccine', 'trump', 'republican', 'democrat', 'left-wing', 'eu', 'hyatt', 'warming']\n",
    "    remaining_claim_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if any(word.lower() in col.lower() for word in claim_words):\n",
    "            remaining_claim_cols.append(col)\n",
    "    \n",
    "    if remaining_claim_cols:\n",
    "        print(f\"  ⚠ Found {len(remaining_claim_cols)} possible claim-related columns that might need review:\")\n",
    "        for col in remaining_claim_cols[:5]:\n",
    "            print(f\"    - {col}\")\n",
    "        if len(remaining_claim_cols) > 5:\n",
    "            print(f\"    ... and {len(remaining_claim_cols) - 5} more\")\n",
    "    else:\n",
    "        print(f\"  ✓ No obvious claim-related columns remain\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nPhase 4 processing completed successfully.\")\n",
    "print(\"\\nSummary of files created:\")\n",
    "print(\"  - t_claims.csv, m_claims.csv, m_t_claims.csv: Contain prolific_id and renamed claim columns\")\n",
    "print(\"  - t_phase4.csv, m_phase4.csv, m_t_phase4.csv: Contain remaining data with claim columns removed\")"
   ],
   "id": "b81f15059ee332c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing t dataset for Phase 4...\n",
      "  Available columns in dataset: 102\n",
      "  Matched 'Republicans divided in views of Trump's conduct, democrats are broadly critical.' to column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.'\n",
      "  Found and extracted 8 of 8 claim columns\n",
      "  Column 'Government officials have manipulated stock prices to hide scandals.' found with first value: Real News...\n",
      "  Column 'New study: Left-wingers are more likely to lie to get a higher salary.' found with first value: Fake News...\n",
      "  Column 'Certain vaccines are loaded with dangerous chemicals and toxins.' found with first value: Fake News...\n",
      "  Column 'The government is knowingly spreading disease through the airwaves and food supply.' found with first value: Fake News...\n",
      "  Column 'Attitudes toward the European Union are largely positive, both within Europe and outside it.' found with first value: Real News...\n",
      "  Column 'Hyatt will remove small bottles from hotel bathrooms by 2021.' found with first value: Fake News...\n",
      "  Column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.' found with first value: Real News...\n",
      "  Column 'Global warming age gap: Younger Americans most worried.' found with first value: Real News...\n",
      "  Claims data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_claims.csv\n",
      "  Phase 4 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_phase4.csv\n",
      "\n",
      "Processing m dataset for Phase 4...\n",
      "  Available columns in dataset: 105\n",
      "  Matched 'Republicans divided in views of Trump's conduct, democrats are broadly critical.' to column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.'\n",
      "  Found and extracted 8 of 8 claim columns\n",
      "  Column 'Government officials have manipulated stock prices to hide scandals.' found with first value: Fake News...\n",
      "  Column 'New study: Left-wingers are more likely to lie to get a higher salary.' found with first value: Fake News...\n",
      "  Column 'Certain vaccines are loaded with dangerous chemicals and toxins.' found with first value: Fake News...\n",
      "  Column 'The government is knowingly spreading disease through the airwaves and food supply.' found with first value: Fake News...\n",
      "  Column 'Attitudes toward the European Union are largely positive, both within Europe and outside it.' found with first value: Real News...\n",
      "  Column 'Hyatt will remove small bottles from hotel bathrooms by 2021.' found with first value: Real News...\n",
      "  Column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.' found with first value: Real News...\n",
      "  Column 'Global warming age gap: Younger Americans most worried.' found with first value: Real News...\n",
      "  Claims data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_claims.csv\n",
      "  Phase 4 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase4.csv\n",
      "\n",
      "Processing m_t dataset for Phase 4...\n",
      "  Available columns in dataset: 102\n",
      "  Matched 'Republicans divided in views of Trump's conduct, democrats are broadly critical.' to column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.'\n",
      "  Found and extracted 8 of 8 claim columns\n",
      "  Column 'Government officials have manipulated stock prices to hide scandals.' found with first value: Fake News...\n",
      "  Column 'New study: Left-wingers are more likely to lie to get a higher salary.' found with first value: Fake News...\n",
      "  Column 'Certain vaccines are loaded with dangerous chemicals and toxins.' found with first value: Fake News...\n",
      "  Column 'The government is knowingly spreading disease through the airwaves and food supply.' found with first value: Fake News...\n",
      "  Column 'Attitudes toward the European Union are largely positive, both within Europe and outside it.' found with first value: Real News...\n",
      "  Column 'Hyatt will remove small bottles from hotel bathrooms by 2021.' found with first value: Real News...\n",
      "  Column 'Republicans divided in views of Trump’s conduct, democrats are broadly critical.' found with first value: Real News...\n",
      "  Column 'Global warming age gap: Younger Americans most worried.' found with first value: Real News...\n",
      "  Claims data saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_claims.csv\n",
      "  Phase 4 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_phase4.csv\n",
      "\n",
      "Verifying claim extraction in claim files:\n",
      "\n",
      "t_claims.csv columns check:\n",
      "  Found 8 of 8 expected claim columns\n",
      "  All expected columns are present\n",
      "\n",
      "m_claims.csv columns check:\n",
      "  Found 8 of 8 expected claim columns\n",
      "  All expected columns are present\n",
      "\n",
      "m_t_claims.csv columns check:\n",
      "  Found 8 of 8 expected claim columns\n",
      "  All expected columns are present\n",
      "\n",
      "Verifying removal of claim columns in phase 4 files:\n",
      "\n",
      "t_phase4.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ No obvious claim-related columns remain\n",
      "\n",
      "m_phase4.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ No obvious claim-related columns remain\n",
      "\n",
      "m_t_phase4.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ No obvious claim-related columns remain\n",
      "\n",
      "Phase 4 processing completed successfully.\n",
      "\n",
      "Summary of files created:\n",
      "  - t_claims.csv, m_claims.csv, m_t_claims.csv: Contain prolific_id and renamed claim columns\n",
      "  - t_phase4.csv, m_phase4.csv, m_t_phase4.csv: Contain remaining data with claim columns removed\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T17:17:23.281699Z",
     "start_time": "2025-03-07T17:17:23.218278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "feedback_dir = get_working_dir() / 'plots' / 'feedback'\n",
    "os.makedirs(feedback_dir, exist_ok=True)\n",
    "\n",
    "df_t = pd.read_csv(processed_dir / 't_phase4.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(processed_dir / 'm_phase4.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(processed_dir / 'm_t_phase4.csv', encoding='utf-8')\n",
    "\n",
    "# Define patterns to identify different types of feedback columns\n",
    "optional_feedback_pattern = r\"Optional.*observations.*suggestions.*concerns.*claim.*explanation\"\n",
    "survey_feedback_pattern = r\"Do you have any comments or suggestions about this survey\\?\"\n",
    "creative_explanation_pattern = r\"Optional.*creative explanation format.*effective\"\n",
    "\n",
    "# Function to extract and write feedback to text files\n",
    "def extract_feedback(df, dataset_name, feedback_dir):\n",
    "    print(f\"\\nExtracting feedback from {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find all feedback columns\n",
    "    optional_feedback_cols = []\n",
    "    survey_feedback_col = None\n",
    "    creative_explanation_col = None\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if re.search(optional_feedback_pattern, col, re.IGNORECASE):\n",
    "            optional_feedback_cols.append(col)\n",
    "        elif re.search(survey_feedback_pattern, col, re.IGNORECASE):\n",
    "            survey_feedback_col = col\n",
    "        elif re.search(creative_explanation_pattern, col, re.IGNORECASE):\n",
    "            creative_explanation_col = col\n",
    "    \n",
    "    # Sort optional feedback columns (assuming they are numbered sequentially)\n",
    "    optional_feedback_cols.sort()\n",
    "    \n",
    "    print(f\"  Found {len(optional_feedback_cols)} optional feedback columns\")\n",
    "    if survey_feedback_col:\n",
    "        print(f\"  Found survey feedback column: {survey_feedback_col}\")\n",
    "    if creative_explanation_col:\n",
    "        print(f\"  Found creative explanation column: {creative_explanation_col}\")\n",
    "    \n",
    "    # Process optional feedback columns\n",
    "    optional_feedback_path = feedback_dir / f\"{dataset_name}_optional_feedback.txt\"\n",
    "    with open(optional_feedback_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"===== Optional Feedback for {dataset_name.upper()} Study =====\\n\\n\")\n",
    "        \n",
    "        for i, col in enumerate(optional_feedback_cols):\n",
    "            f.write(f\"===== CLAIM {i+1} FEEDBACK =====\\n\\n\")\n",
    "            \n",
    "            # Extract feedback for this claim\n",
    "            feedback_items = df_copy[df_copy[col].notna()][['prolific_id', col]]\n",
    "            \n",
    "            if feedback_items.empty:\n",
    "                f.write(\"No feedback provided for this claim.\\n\\n\")\n",
    "            else:\n",
    "                for _, row in feedback_items.iterrows():\n",
    "                    f.write(f\"Participant {row['prolific_id']}:\\n\")\n",
    "                    f.write(f\"{row[col]}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"  Optional feedback saved to {optional_feedback_path}\")\n",
    "    \n",
    "    # Process survey feedback\n",
    "    removed_columns = optional_feedback_cols.copy()\n",
    "    \n",
    "    if survey_feedback_col:\n",
    "        # Append to survey feedback file (create if doesn't exist)\n",
    "        survey_feedback_path = feedback_dir / \"feedback_survey.txt\"\n",
    "        \n",
    "        # Check if file exists to determine if we need to write headers\n",
    "        file_exists = os.path.isfile(survey_feedback_path)\n",
    "        \n",
    "        with open(survey_feedback_path, 'a', encoding='utf-8') as f:\n",
    "            if not file_exists:\n",
    "                f.write(\"===== SURVEY FEEDBACK FROM ALL STUDIES =====\\n\\n\")\n",
    "            \n",
    "            f.write(f\"===== {dataset_name.upper()} STUDY FEEDBACK =====\\n\\n\")\n",
    "            \n",
    "            # Extract survey feedback\n",
    "            feedback_items = df_copy[df_copy[survey_feedback_col].notna()][['prolific_id', survey_feedback_col]]\n",
    "            \n",
    "            if feedback_items.empty:\n",
    "                f.write(\"No survey feedback provided.\\n\\n\")\n",
    "            else:\n",
    "                for _, row in feedback_items.iterrows():\n",
    "                    f.write(f\"Participant {row['prolific_id']}:\\n\")\n",
    "                    f.write(f\"{row[survey_feedback_col]}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"  Survey feedback appended to {survey_feedback_path}\")\n",
    "        removed_columns.append(survey_feedback_col)\n",
    "    \n",
    "    # Process creative explanation feedback (only for 'm' dataset)\n",
    "    if creative_explanation_col:\n",
    "        creative_feedback_path = feedback_dir / \"m_creative_explanation_feedback.txt\"\n",
    "        \n",
    "        with open(creative_feedback_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"===== CREATIVE EXPLANATION FORMAT FEEDBACK =====\\n\\n\")\n",
    "            \n",
    "            # Extract creative explanation feedback\n",
    "            feedback_items = df_copy[df_copy[creative_explanation_col].notna()][['prolific_id', creative_explanation_col]]\n",
    "            \n",
    "            if feedback_items.empty:\n",
    "                f.write(\"No creative explanation feedback provided.\\n\\n\")\n",
    "            else:\n",
    "                for _, row in feedback_items.iterrows():\n",
    "                    f.write(f\"Participant {row['prolific_id']}:\\n\")\n",
    "                    f.write(f\"{row[creative_explanation_col]}\\n\\n\")\n",
    "        \n",
    "        print(f\"  Creative explanation feedback saved to {creative_feedback_path}\")\n",
    "        removed_columns.append(creative_explanation_col)\n",
    "    \n",
    "    # Create Phase 5 file by removing feedback columns\n",
    "    df_phase5 = df_copy.drop(columns=removed_columns)\n",
    "    \n",
    "    # Save Phase 5 processed file\n",
    "    phase5_file_path = processed_dir / f\"{dataset_name}_phase5.csv\"\n",
    "    df_phase5.to_csv(phase5_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 5 processed file saved to {phase5_file_path}\")\n",
    "    \n",
    "    return df_phase5, removed_columns\n",
    "\n",
    "# Extract feedback and create Phase 5 files\n",
    "df_t_phase5, t_removed = extract_feedback(df_t, 't', feedback_dir)\n",
    "df_m_phase5, m_removed = extract_feedback(df_m, 'm', feedback_dir)\n",
    "df_m_t_phase5, m_t_removed = extract_feedback(df_m_t, 'm_t', feedback_dir)\n",
    "\n",
    "# Verify removal of feedback columns\n",
    "print(\"\\nVerifying removal of feedback columns in Phase 5 files:\")\n",
    "for dataset_name, df, removed in [\n",
    "    (\"t\", df_t_phase5, t_removed), \n",
    "    (\"m\", df_m_phase5, m_removed), \n",
    "    (\"m_t\", df_m_t_phase5, m_t_removed)\n",
    "]:\n",
    "    print(f\"\\n{dataset_name}_phase5.csv:\")\n",
    "    \n",
    "    # Check if prolific_id is still present\n",
    "    if \"prolific_id\" in df.columns:\n",
    "        print(f\"  ✓ prolific_id column is present\")\n",
    "    else:\n",
    "        print(f\"  ✗ prolific_id column is missing\")\n",
    "    \n",
    "    # Check that all feedback columns were removed\n",
    "    remaining_feedback_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if (re.search(optional_feedback_pattern, col, re.IGNORECASE) or\n",
    "            re.search(survey_feedback_pattern, col, re.IGNORECASE) or\n",
    "            re.search(creative_explanation_pattern, col, re.IGNORECASE)):\n",
    "            remaining_feedback_cols.append(col)\n",
    "    \n",
    "    if remaining_feedback_cols:\n",
    "        print(f\"  ⚠ Found {len(remaining_feedback_cols)} possible feedback columns that might need review:\")\n",
    "        for col in remaining_feedback_cols:\n",
    "            print(f\"    - {col}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All feedback columns have been removed\")\n",
    "    \n",
    "    print(f\"  Removed {len(removed)} feedback columns total\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nPhase 5 processing completed successfully.\")\n",
    "print(\"\\nSummary of files created:\")\n",
    "print(\"  - t_phase5.csv, m_phase5.csv, m_t_phase5.csv: Contain remaining data with feedback columns removed\")\n",
    "print(\"\\nFeedback files created:\")\n",
    "print(\"  - t_optional_feedback.txt: Optional feedback for Text study\")\n",
    "print(\"  - m_optional_feedback.txt: Optional feedback for Meme study\")\n",
    "print(\"  - m_t_optional_feedback.txt: Optional feedback for Meme+Context study\")\n",
    "print(\"  - feedback_survey.txt: Combined survey feedback from all studies\")\n",
    "print(\"  - m_creative_explanation_feedback.txt: Creative explanation format feedback from Meme study\")"
   ],
   "id": "6ae07b4ea3e6cacf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting feedback from t dataset...\n",
      "  Found 10 optional feedback columns\n",
      "  Found survey feedback column: Do you have any comments or suggestions about this survey? Your feedback will help us improve future studies.\n",
      "  Optional feedback saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/t_optional_feedback.txt\n",
      "  Survey feedback appended to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/feedback_survey.txt\n",
      "  Phase 5 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_phase5.csv\n",
      "\n",
      "Extracting feedback from m dataset...\n",
      "  Found 10 optional feedback columns\n",
      "  Found survey feedback column: Do you have any comments or suggestions about this survey? Your feedback will help us improve future studies.\n",
      "  Found creative explanation column: Optional: What do you think makes a creative explanation format effective for explaining claims?\n",
      "  Optional feedback saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/m_optional_feedback.txt\n",
      "  Survey feedback appended to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/feedback_survey.txt\n",
      "  Creative explanation feedback saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/m_creative_explanation_feedback.txt\n",
      "  Phase 5 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase5.csv\n",
      "\n",
      "Extracting feedback from m_t dataset...\n",
      "  Found 10 optional feedback columns\n",
      "  Found survey feedback column: Do you have any comments or suggestions about this survey? Your feedback will help us improve future studies.\n",
      "  Optional feedback saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/m_t_optional_feedback.txt\n",
      "  Survey feedback appended to /Users/sergiopinto/Desktop/MemeFact/meta_study/results/feedback/feedback_survey.txt\n",
      "  Phase 5 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_phase5.csv\n",
      "\n",
      "Verifying removal of feedback columns in Phase 5 files:\n",
      "\n",
      "t_phase5.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All feedback columns have been removed\n",
      "  Removed 11 feedback columns total\n",
      "\n",
      "m_phase5.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All feedback columns have been removed\n",
      "  Removed 12 feedback columns total\n",
      "\n",
      "m_t_phase5.csv:\n",
      "  ✓ prolific_id column is present\n",
      "  ✓ All feedback columns have been removed\n",
      "  Removed 11 feedback columns total\n",
      "\n",
      "Phase 5 processing completed successfully.\n",
      "\n",
      "Summary of files created:\n",
      "  - t_phase5.csv, m_phase5.csv, m_t_phase5.csv: Contain remaining data with feedback columns removed\n",
      "\n",
      "Feedback files created:\n",
      "  - t_optional_feedback.txt: Optional feedback for Text study\n",
      "  - m_optional_feedback.txt: Optional feedback for Meme study\n",
      "  - m_t_optional_feedback.txt: Optional feedback for Meme+Context study\n",
      "  - feedback_survey.txt: Combined survey feedback from all studies\n",
      "  - m_creative_explanation_feedback.txt: Creative explanation format feedback from Meme study\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T18:06:22.225319Z",
     "start_time": "2025-03-07T18:06:22.180555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_t = pd.read_csv(processed_dir / 't_phase5.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(processed_dir / 'm_phase5.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(processed_dir / 'm_t_phase5.csv', encoding='utf-8')\n",
    "\n",
    "# Function to process datasets for phase 6\n",
    "def process_phase6(df, dataset_name):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for Phase 6...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Define patterns for different types of questions\n",
    "    accuracy_pattern = r\"How accurate do you think the claim is\\?(\\.[\\d]+)?\"\n",
    "    confidence_pattern = r\"How confident are you in your accuracy assessment of the claim\\?(\\.[\\d]+)?\"\n",
    "    engagement_pattern = r\"If you were to encounter the claim on social media, how likely would you be to interact with content containing it\\?(\\.[\\d]+)?\"\n",
    "    \n",
    "    # Patterns for explanation assessment columns with different variants\n",
    "    if dataset_name == 'm_t':\n",
    "        # Meme+Context patterns\n",
    "        explainability_pattern = r\"How well do the explanation and the context convey their stance on the claim\\?(\\.[\\d]+)?\"\n",
    "        credibility_pattern = r\"How credible do you find the explanation and the context\\?(\\.[\\d]+)?\"\n",
    "    else:\n",
    "        # Text and Meme patterns\n",
    "        explainability_pattern = r\"How well does the explanation convey its stance on the claim\\?(\\.[\\d]+)?\"\n",
    "        credibility_pattern = r\"How credible do you find the explanation\\?(\\.[\\d]+)?\"\n",
    "    \n",
    "    # Attention check pattern\n",
    "    attention_check_pattern = r\"How carefully are you reading this question\\?(\\.[\\d]+)?\"\n",
    "    \n",
    "    # Find columns that match each pattern\n",
    "    accuracy_cols = [col for col in df_copy.columns if re.search(accuracy_pattern, col, re.IGNORECASE)]\n",
    "    confidence_cols = [col for col in df_copy.columns if re.search(confidence_pattern, col, re.IGNORECASE)]\n",
    "    engagement_cols = [col for col in df_copy.columns if re.search(engagement_pattern, col, re.IGNORECASE)]\n",
    "    explainability_cols = [col for col in df_copy.columns if re.search(explainability_pattern, col, re.IGNORECASE)]\n",
    "    credibility_cols = [col for col in df_copy.columns if re.search(credibility_pattern, col, re.IGNORECASE)]\n",
    "    attention_check_cols = [col for col in df_copy.columns if re.search(attention_check_pattern, col, re.IGNORECASE)]\n",
    "    \n",
    "    # Print what we found\n",
    "    print(f\"  Found {len(accuracy_cols)} accuracy columns\")\n",
    "    print(f\"  Found {len(confidence_cols)} confidence columns\")\n",
    "    print(f\"  Found {len(engagement_cols)} engagement columns\")\n",
    "    print(f\"  Found {len(explainability_cols)} explainability columns\")\n",
    "    print(f\"  Found {len(credibility_cols)} credibility columns\")\n",
    "    print(f\"  Found {len(attention_check_cols)} attention check columns to remove\")\n",
    "    \n",
    "    # Helper function to extract the suffix number from a column name\n",
    "    def get_suffix_num(col_name):\n",
    "        match = re.search(r\"\\.(\\d+)$\", col_name)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    # Group columns by claim\n",
    "    claim_columns = []\n",
    "    for claim_idx in range(10):  # For 10 claims\n",
    "        claim_group = {}\n",
    "        \n",
    "        # For first claim, look for columns without suffix and with suffix .1\n",
    "        # For subsequent claims, look for columns with suffix .2*claim_idx and .2*claim_idx+1\n",
    "        if claim_idx == 0:\n",
    "            # Claim 1 pre columns (no suffix)\n",
    "            pre_accuracy = [col for col in accuracy_cols if get_suffix_num(col) == 0]\n",
    "            pre_confidence = [col for col in confidence_cols if get_suffix_num(col) == 0]\n",
    "            pre_engagement = [col for col in engagement_cols if get_suffix_num(col) == 0]\n",
    "            \n",
    "            # Claim 1 post columns (suffix .1)\n",
    "            post_accuracy = [col for col in accuracy_cols if get_suffix_num(col) == 1]\n",
    "            post_confidence = [col for col in confidence_cols if get_suffix_num(col) == 1]\n",
    "            post_engagement = [col for col in engagement_cols if get_suffix_num(col) == 1]\n",
    "            \n",
    "            # Claim 1 correction columns\n",
    "            correction_explainability = [col for col in explainability_cols if get_suffix_num(col) == 0]\n",
    "            correction_credibility = [col for col in credibility_cols if get_suffix_num(col) == 0]\n",
    "        else:\n",
    "            # Pre columns (suffix .2*claim_idx)\n",
    "            suffix_pre = 2 * claim_idx\n",
    "            pre_accuracy = [col for col in accuracy_cols if get_suffix_num(col) == suffix_pre]\n",
    "            pre_confidence = [col for col in confidence_cols if get_suffix_num(col) == suffix_pre]\n",
    "            pre_engagement = [col for col in engagement_cols if get_suffix_num(col) == suffix_pre]\n",
    "            \n",
    "            # Post columns (suffix .2*claim_idx+1)\n",
    "            suffix_post = 2 * claim_idx + 1\n",
    "            post_accuracy = [col for col in accuracy_cols if get_suffix_num(col) == suffix_post]\n",
    "            post_confidence = [col for col in confidence_cols if get_suffix_num(col) == suffix_post]\n",
    "            post_engagement = [col for col in engagement_cols if get_suffix_num(col) == suffix_post]\n",
    "            \n",
    "            # Correction columns\n",
    "            correction_explainability = [col for col in explainability_cols if get_suffix_num(col) == claim_idx]\n",
    "            correction_credibility = [col for col in credibility_cols if get_suffix_num(col) == claim_idx]\n",
    "        \n",
    "        # Store found columns for this claim\n",
    "        if pre_accuracy:\n",
    "            claim_group['pre_accuracy'] = pre_accuracy[0]\n",
    "        if pre_confidence:\n",
    "            claim_group['pre_confidence'] = pre_confidence[0]\n",
    "        if pre_engagement:\n",
    "            claim_group['pre_engagement'] = pre_engagement[0]\n",
    "        if post_accuracy:\n",
    "            claim_group['post_accuracy'] = post_accuracy[0]\n",
    "        if post_confidence:\n",
    "            claim_group['post_confidence'] = post_confidence[0]\n",
    "        if post_engagement:\n",
    "            claim_group['post_engagement'] = post_engagement[0]\n",
    "        if correction_explainability:\n",
    "            claim_group['correction_explainability'] = correction_explainability[0]\n",
    "        if correction_credibility:\n",
    "            claim_group['correction_credibility'] = correction_credibility[0]\n",
    "        \n",
    "        # Only add if we found columns for this claim\n",
    "        if claim_group:\n",
    "            claim_columns.append((claim_idx + 1, claim_group))\n",
    "    \n",
    "    # Create rename mapping\n",
    "    rename_mapping = {}\n",
    "    \n",
    "    for claim_num, columns in claim_columns:\n",
    "        for col_type, col_name in columns.items():\n",
    "            new_name = f\"claim{claim_num}_{col_type}\"\n",
    "            rename_mapping[col_name] = new_name\n",
    "    \n",
    "    # Apply renaming and remove attention check columns\n",
    "    df_renamed = df_copy.rename(columns=rename_mapping)\n",
    "    df_without_attention = df_renamed.drop(columns=attention_check_cols)\n",
    "    \n",
    "    # Reorder columns to keep claim-related columns together\n",
    "    # First, get non-claim columns (including prolific_id)\n",
    "    non_claim_cols = [col for col in df_without_attention.columns if not col.startswith('claim')]\n",
    "    \n",
    "    ordered_cols = non_claim_cols.copy()\n",
    "    \n",
    "    for claim_num, _ in claim_columns:\n",
    "        # Define the desired order of columns for each claim\n",
    "        claim_col_order = [\n",
    "            f'claim{claim_num}_pre_accuracy',\n",
    "            f'claim{claim_num}_pre_confidence',\n",
    "            f'claim{claim_num}_pre_engagement',\n",
    "            f'claim{claim_num}_post_accuracy',\n",
    "            f'claim{claim_num}_post_confidence',\n",
    "            f'claim{claim_num}_post_engagement',\n",
    "            f'claim{claim_num}_correction_explainability',\n",
    "            f'claim{claim_num}_correction_credibility'\n",
    "        ]\n",
    "        \n",
    "        # Add only columns that exist\n",
    "        for col in claim_col_order:\n",
    "            if col in df_without_attention.columns:\n",
    "                ordered_cols.append(col)\n",
    "    \n",
    "    # Reorder the dataframe columns\n",
    "    df_final = df_without_attention[ordered_cols]\n",
    "    \n",
    "    # Save the processed file\n",
    "    phase6_file_path = processed_dir / f\"{dataset_name}_phase6.csv\"\n",
    "    df_final.to_csv(phase6_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 6 processed file saved to {phase6_file_path}\")\n",
    "    \n",
    "    # Return processed dataframe and summary\n",
    "    return df_final, {\n",
    "        'renamed': len(rename_mapping),\n",
    "        'removed': len(attention_check_cols),\n",
    "        'claims_processed': len(claim_columns)\n",
    "    }\n",
    "\n",
    "# Process each dataset\n",
    "df_t_phase6, t_summary = process_phase6(df_t, 't')\n",
    "df_m_phase6, m_summary = process_phase6(df_m, 'm')\n",
    "df_m_t_phase6, m_t_summary = process_phase6(df_m_t, 'm_t')\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nPhase 6 processing completed successfully.\")\n",
    "print(\"\\nSummary of changes:\")\n",
    "print(f\"  t dataset: {t_summary['renamed']} columns renamed, {t_summary['removed']} columns removed, {t_summary['claims_processed']} claims processed\")\n",
    "print(f\"  m dataset: {m_summary['renamed']} columns renamed, {m_summary['removed']} columns removed, {m_summary['claims_processed']} claims processed\")\n",
    "print(f\"  m_t dataset: {m_t_summary['renamed']} columns renamed, {m_t_summary['removed']} columns removed, {m_t_summary['claims_processed']} claims processed\")\n",
    "\n",
    "print(\"\\nColumn order in final files:\")\n",
    "print(\"  1. Non-claim columns (including prolific_id)\")\n",
    "print(\"  2. For each claim (1 through 10), in order:\")\n",
    "print(\"     - Pre-assessment columns (pre_accuracy, pre_confidence, pre_engagement)\")\n",
    "print(\"     - Post-assessment columns (post_accuracy, post_confidence, post_engagement)\")\n",
    "print(\"     - Correction assessment columns (correction_explainability, correction_credibility)\")\n",
    "\n",
    "# Verify final column structure\n",
    "if df_t_phase6.shape[0] > 0:\n",
    "    claim_cols = [col for col in df_t_phase6.columns if col.startswith('claim')]\n",
    "    if claim_cols:\n",
    "        print(\"\\nExample of column structure in t_phase6.csv:\")\n",
    "        claim_nums = sorted(list(set([int(re.search(r'claim(\\d+)', col).group(1)) for col in claim_cols])))\n",
    "        for claim_num in claim_nums[:2]:  # Show first two claims\n",
    "            cols = [col for col in df_t_phase6.columns if f'claim{claim_num}_' in col]\n",
    "            print(f\"\\nClaim {claim_num} columns:\")\n",
    "            for col in cols:\n",
    "                print(f\"  - {col}\")\n",
    "        if len(claim_nums) > 2:\n",
    "            print(f\"  ...and {len(claim_nums)-2} more claims\")"
   ],
   "id": "e1dee09bbfa562ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing t dataset for Phase 6...\n",
      "  Found 20 accuracy columns\n",
      "  Found 20 confidence columns\n",
      "  Found 20 engagement columns\n",
      "  Found 10 explainability columns\n",
      "  Found 10 credibility columns\n",
      "  Found 2 attention check columns to remove\n",
      "  Phase 6 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/t_phase6.csv\n",
      "\n",
      "Processing m dataset for Phase 6...\n",
      "  Found 20 accuracy columns\n",
      "  Found 20 confidence columns\n",
      "  Found 20 engagement columns\n",
      "  Found 10 explainability columns\n",
      "  Found 10 credibility columns\n",
      "  Found 2 attention check columns to remove\n",
      "  Phase 6 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase6.csv\n",
      "\n",
      "Processing m_t dataset for Phase 6...\n",
      "  Found 20 accuracy columns\n",
      "  Found 20 confidence columns\n",
      "  Found 20 engagement columns\n",
      "  Found 10 explainability columns\n",
      "  Found 10 credibility columns\n",
      "  Found 2 attention check columns to remove\n",
      "  Phase 6 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_t_phase6.csv\n",
      "\n",
      "Phase 6 processing completed successfully.\n",
      "\n",
      "Summary of changes:\n",
      "  t dataset: 80 columns renamed, 2 columns removed, 10 claims processed\n",
      "  m dataset: 80 columns renamed, 2 columns removed, 10 claims processed\n",
      "  m_t dataset: 80 columns renamed, 2 columns removed, 10 claims processed\n",
      "\n",
      "Column order in final files:\n",
      "  1. Non-claim columns (including prolific_id)\n",
      "  2. For each claim (1 through 10), in order:\n",
      "     - Pre-assessment columns (pre_accuracy, pre_confidence, pre_engagement)\n",
      "     - Post-assessment columns (post_accuracy, post_confidence, post_engagement)\n",
      "     - Correction assessment columns (correction_explainability, correction_credibility)\n",
      "\n",
      "Example of column structure in t_phase6.csv:\n",
      "\n",
      "Claim 1 columns:\n",
      "  - claim1_pre_accuracy\n",
      "  - claim1_pre_confidence\n",
      "  - claim1_pre_engagement\n",
      "  - claim1_post_accuracy\n",
      "  - claim1_post_confidence\n",
      "  - claim1_post_engagement\n",
      "  - claim1_correction_explainability\n",
      "  - claim1_correction_credibility\n",
      "\n",
      "Claim 2 columns:\n",
      "  - claim2_pre_accuracy\n",
      "  - claim2_pre_confidence\n",
      "  - claim2_pre_engagement\n",
      "  - claim2_post_accuracy\n",
      "  - claim2_post_confidence\n",
      "  - claim2_post_engagement\n",
      "  - claim2_correction_explainability\n",
      "  - claim2_correction_credibility\n",
      "  ...and 8 more claims\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T18:11:29.720185Z",
     "start_time": "2025-03-07T18:11:29.685219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_m = pd.read_csv(processed_dir / 'm_phase6.csv', encoding='utf-8')\n",
    "\n",
    "# Define patterns to identify the creative explanation preference columns\n",
    "most_effective_pattern = r\"Which of these creative explanations do you find most effective at explaining this specific claim\\?\"\n",
    "creative_likeability_pattern = r\"On a scale from 1 to 5, how much would you like to see more fact-checking content that uses creative formats\"\n",
    "\n",
    "# Process the dataset for phase 7\n",
    "def process_phase7(df, dataset_name='m'):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for Phase 7...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find columns that match the patterns\n",
    "    most_effective_col = None\n",
    "    creative_likeability_col = None\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        if re.search(most_effective_pattern, col, re.IGNORECASE):\n",
    "            most_effective_col = col\n",
    "        elif re.search(creative_likeability_pattern, col, re.IGNORECASE):\n",
    "            creative_likeability_col = col\n",
    "    \n",
    "    # Extract the creative explanation preference columns with prolific_id\n",
    "    creative_prefs_df = df_copy[['prolific_id']].copy()\n",
    "    \n",
    "    # Add the preference columns if found\n",
    "    removed_columns = []\n",
    "    \n",
    "    if most_effective_col:\n",
    "        print(f\"  Found 'most effective creative explanation' column: {most_effective_col}\")\n",
    "        creative_prefs_df['most_effective_creative_explanation'] = df_copy[most_effective_col]\n",
    "        removed_columns.append(most_effective_col)\n",
    "    else:\n",
    "        print(\"  Warning: Could not find 'most effective creative explanation' column\")\n",
    "    \n",
    "    if creative_likeability_col:\n",
    "        print(f\"  Found 'creative explanations likeability' column: {creative_likeability_col}\")\n",
    "        creative_prefs_df['creative_explanations_likeability'] = df_copy[creative_likeability_col]\n",
    "        removed_columns.append(creative_likeability_col)\n",
    "    else:\n",
    "        print(\"  Warning: Could not find 'creative explanations likeability' column\")\n",
    "    \n",
    "    # Create Phase 7 file by removing the creative preference columns\n",
    "    df_phase7 = df_copy.drop(columns=removed_columns)\n",
    "    \n",
    "    # Save the creative preferences to a separate file\n",
    "    creative_prefs_path = processed_dir / f\"{dataset_name}_creative_preferences.csv\"\n",
    "    creative_prefs_df.to_csv(creative_prefs_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Creative preferences saved to {creative_prefs_path}\")\n",
    "    \n",
    "    # Save Phase 7 processed file\n",
    "    phase7_file_path = processed_dir / f\"{dataset_name}_phase7.csv\"\n",
    "    df_phase7.to_csv(phase7_file_path, index=False, encoding='utf-8')\n",
    "    print(f\"  Phase 7 processed file saved to {phase7_file_path}\")\n",
    "    \n",
    "    return df_phase7, creative_prefs_df, len(removed_columns)\n",
    "\n",
    "# Process the dataset\n",
    "df_m_phase7, df_m_creative_prefs, columns_removed = process_phase7(df_m)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nPhase 7 processing completed successfully.\")\n",
    "print(f\"\\nRemoved {columns_removed} creative preference columns from 'm_phase6.csv'\")\n",
    "print(\"Created files:\")\n",
    "print(\"  - m_phase7.csv: Contains all columns from m_phase6.csv except creative preference columns\")\n",
    "print(\"  - m_creative_preferences.csv: Contains prolific_id and renamed creative preference columns\")\n",
    "\n",
    "# Verify the creative preferences file\n",
    "if df_m_creative_prefs.shape[0] > 0:\n",
    "    print(\"\\nStructure of m_creative_preferences.csv:\")\n",
    "    print(f\"  Rows: {df_m_creative_prefs.shape[0]}\")\n",
    "    print(f\"  Columns: {df_m_creative_prefs.columns.tolist()}\")\n",
    "    \n",
    "    # Show a few sample values if available\n",
    "    if 'most_effective_creative_explanation' in df_m_creative_prefs.columns:\n",
    "        unique_values = df_m_creative_prefs['most_effective_creative_explanation'].unique()\n",
    "        if len(unique_values) > 0:\n",
    "            print(f\"\\nSample unique values for 'most_effective_creative_explanation':\")\n",
    "            for val in unique_values[:5]:\n",
    "                print(f\"  - {val}\")\n",
    "            if len(unique_values) > 5:\n",
    "                print(f\"  ...and {len(unique_values)-5} more unique values\")\n",
    "    \n",
    "    if 'creative_explanations_likeability' in df_m_creative_prefs.columns:\n",
    "        unique_values = df_m_creative_prefs['creative_explanations_likeability'].unique()\n",
    "        if len(unique_values) > 0:\n",
    "            print(f\"\\nSample unique values for 'creative_explanations_likeability':\")\n",
    "            for val in unique_values[:5]:\n",
    "                print(f\"  - {val}\")\n",
    "            if len(unique_values) > 5:\n",
    "                print(f\"  ...and {len(unique_values)-5} more unique values\")"
   ],
   "id": "6b7bccf42c01cf39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing m dataset for Phase 7...\n",
      "  Found 'most effective creative explanation' column: Which of these creative explanations do you find most effective at explaining this specific claim?\n",
      "  Found 'creative explanations likeability' column: On a scale from 1 to 5, how much would you like to see more fact-checking content that uses creative formats (like poems, jokes, or memes) instead of traditional text-based explanations?\n",
      "  Creative preferences saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_creative_preferences.csv\n",
      "  Phase 7 processed file saved to /Users/sergiopinto/Desktop/MemeFact/meta_study/data/processed/m_phase7.csv\n",
      "\n",
      "Phase 7 processing completed successfully.\n",
      "\n",
      "Removed 2 creative preference columns from 'm_phase6.csv'\n",
      "Created files:\n",
      "  - m_phase7.csv: Contains all columns from m_phase6.csv except creative preference columns\n",
      "  - m_creative_preferences.csv: Contains prolific_id and renamed creative preference columns\n",
      "\n",
      "Structure of m_creative_preferences.csv:\n",
      "  Rows: 101\n",
      "  Columns: ['prolific_id', 'most_effective_creative_explanation', 'creative_explanations_likeability']\n",
      "\n",
      "Sample unique values for 'most_effective_creative_explanation':\n",
      "  - The joke\n",
      "  - The meme\n",
      "  - The poem\n",
      "\n",
      "Sample unique values for 'creative_explanations_likeability':\n",
      "  - Strongly dislike\n",
      "  - Neither like nor dislike\n",
      "  - Strongly like\n",
      "  - Somewhat like\n",
      "  - Somewhat dislike\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
