{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T16:32:10.954090Z",
     "start_time": "2025-03-12T16:32:09.731899Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:32:10.974232Z",
     "start_time": "2025-03-12T16:32:10.968243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "processed_dir = get_working_dir() / 'data' / 'processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "df_t = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m = pd.read_csv(get_working_dir() /  'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(get_working_dir() /  'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "dataset_mist = pd.read_csv(get_working_dir() / 'data' / 'dataset_mist.csv', encoding='utf-8')"
   ],
   "id": "b784a8b45d5a389e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# V, R, F, d and n values for the three studies",
   "id": "a419232398914407"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:37:54.380901Z",
     "start_time": "2025-03-12T16:37:53.722932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to compute the metrics.\n",
    "def compute_metrics(df):\n",
    "    # Identify claim columns (assumes columns starting with \"claim_\"\n",
    "    # and that the first four are fake claims and the last four are real claims)\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    \n",
    "    metrics = pd.DataFrame()\n",
    "    metrics[\"prolific_id\"] = df[\"prolific_id\"]\n",
    "    # f: Count of fake claims correctly identified (\"Fake News\")\n",
    "    metrics[\"f\"] = df[fake_cols].apply(lambda row: sum(row == \"Fake News\"), axis=1)\n",
    "    # r: Count of real claims correctly identified (\"Real News\")\n",
    "    metrics[\"r\"] = df[real_cols].apply(lambda row: sum(row == \"Real News\"), axis=1)\n",
    "    # v: Overall veracity discernment = f + r\n",
    "    metrics[\"v\"] = metrics[\"f\"] + metrics[\"r\"]\n",
    "    # d: Distrust bias = number of real claims misclassified as \"Fake News\"\n",
    "    metrics[\"d\"] = df[real_cols].apply(lambda row: sum(row == \"Fake News\"), axis=1)\n",
    "    # n: Naïvité = number of fake claims misclassified as \"Real News\"\n",
    "    metrics[\"n\"] = df[fake_cols].apply(lambda row: sum(row == \"Real News\"), axis=1)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute metrics for each study.\n",
    "metrics_t = compute_metrics(df_t)\n",
    "metrics_m = compute_metrics(df_m)\n",
    "metrics_m_t = compute_metrics(df_m_t)\n",
    "\n",
    "# Add a study identifier.\n",
    "metrics_t[\"study\"] = \"Text\"\n",
    "metrics_m[\"study\"] = \"Memes\"\n",
    "metrics_m_t[\"study\"] = \"Memes+Context\"\n",
    "\n",
    "# Combine all metrics into one DataFrame.\n",
    "metrics_all = pd.concat([metrics_t, metrics_m, metrics_m_t], ignore_index=True)\n",
    "\n",
    "# List of metrics for plotting.\n",
    "metric_list = [\"v\", \"r\", \"f\", \"d\", \"n\"]\n",
    "\n",
    "# Function to compute mean and standard error for a given dataframe and metric list.\n",
    "def compute_summary_stats(df, metrics):\n",
    "    summary = {}\n",
    "    for m in metrics:\n",
    "        mean_val = df[m].mean()\n",
    "        sem_val = df[m].std() / np.sqrt(df[m].count())\n",
    "        summary[m] = (mean_val, sem_val)\n",
    "    return summary\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Overall Summary: Bar plot for all participants (across all studies)\n",
    "overall_stats = compute_summary_stats(metrics_all, metric_list)\n",
    "overall_means = [overall_stats[m][0] for m in metric_list]\n",
    "overall_sems  = [overall_stats[m][1] for m in metric_list]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = ax.bar(metric_list, overall_means, yerr=overall_sems, capsize=5)\n",
    "ax.set_xlabel(\"Metric\")\n",
    "ax.set_ylabel(\"Mean Score\")\n",
    "ax.set_title(\"Overall Mean Scores with Standard Error (All Studies)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_dir / \"overall_barplot.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Study-Specific Bar Plots: One bar plot per study\n",
    "for study in metrics_all[\"study\"].unique():\n",
    "    df_study = metrics_all[metrics_all[\"study\"] == study]\n",
    "    stats = compute_summary_stats(df_study, metric_list)\n",
    "    means = [stats[m][0] for m in metric_list]\n",
    "    sems = [stats[m][1] for m in metric_list]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.bar(metric_list, means, yerr=sems, capsize=5)\n",
    "    ax.set_xlabel(\"Metric\")\n",
    "    ax.set_ylabel(\"Mean Score\")\n",
    "    ax.set_title(f\"Mean Scores with Standard Error ({study} Study)\")\n",
    "    fig.tight_layout()\n",
    "    filename = f\"barplot_{study.lower().replace(' ', '_')}.png\"\n",
    "    fig.savefig(results_dir / filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Comparison Across Studies: Bar plots for each metric grouped by study.\n",
    "# We'll create one figure with five subplots (one for each metric).\n",
    "study_order = [\"Text\", \"Memes\", \"Memes+Context\"]\n",
    "fig, axs = plt.subplots(1, len(metric_list), figsize=(20, 5))\n",
    "for i, m in enumerate(metric_list):\n",
    "    means = []\n",
    "    sems = []\n",
    "    for study in study_order:\n",
    "        df_temp = metrics_all[metrics_all[\"study\"] == study]\n",
    "        mean_val = df_temp[m].mean()\n",
    "        sem_val = df_temp[m].std() / np.sqrt(df_temp[m].count())\n",
    "        means.append(mean_val)\n",
    "        sems.append(sem_val)\n",
    "    axs[i].bar(study_order, means, yerr=sems, capsize=5)\n",
    "    axs[i].set_title(f\"{m} (by Study)\")\n",
    "    axs[i].set_xlabel(\"Study\")\n",
    "    axs[i].set_ylabel(\"Mean Score\")\n",
    "fig.suptitle(\"Comparison of Mean Scores Across Studies for Each Metric\")\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.savefig(results_dir / \"comparison_barplots.png\")\n",
    "plt.close(fig)"
   ],
   "id": "da06c138a9fe2611",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CFA Realiability",
   "id": "fa86cdb81c4b69a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:55:57.283508Z",
     "start_time": "2025-03-12T16:55:53.919789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from semopy import Model, calc_stats\n",
    "\n",
    "# --- Step 1: Load the three CSV files for MIST‑8 data ---\n",
    "df_t   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "# --- Step 2: Recode claim responses into binary scores ---\n",
    "# Define the fake and real claim columns.\n",
    "# (Assumes the first four claims are fake and the last four are real.)\n",
    "fake_cols = ['claim_stock_manipulation', 'claim_left_wing_salary_lie', \n",
    "             'claim_vaccine_toxins', 'claim_government_disease_spread']\n",
    "real_cols = ['claim_eu_positive_attitudes', 'claim_hyatt_small_bottles', \n",
    "             'claim_republicans_trump_conduct', 'claim_climate_worry_age_gap']\n",
    "\n",
    "def recode_mist8(df):\n",
    "    # For fake items: correct response is \"Fake News\"\n",
    "    for col in fake_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x.strip().lower() == \"fake news\" else 0)\n",
    "    # For real items: correct response is \"Real News\"\n",
    "    for col in real_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x.strip().lower() == \"real news\" else 0)\n",
    "    return df\n",
    "\n",
    "df_t   = recode_mist8(df_t)\n",
    "df_m   = recode_mist8(df_m)\n",
    "df_m_t = recode_mist8(df_m_t)\n",
    "\n",
    "# --- Step 3: Combine the data to form the overall sample ---\n",
    "df_overall = pd.concat([df_t, df_m, df_m_t], ignore_index=True)\n",
    "\n",
    "# Define the list of MIST‑8 item columns.\n",
    "mist8_cols = fake_cols + real_cols\n",
    "\n",
    "# Remove any cases with missing values on the mist‑8 items.\n",
    "df_overall_complete = df_overall.dropna(subset=mist8_cols)\n",
    "\n",
    "# --- Step 4: Define the Two-Factor CFA Model ---\n",
    "# Factor F represents fake news detection and R represents real news detection.\n",
    "model_desc = \"\"\"\n",
    "F =~ claim_stock_manipulation + claim_left_wing_salary_lie + claim_vaccine_toxins + claim_government_disease_spread\n",
    "R =~ claim_eu_positive_attitudes + claim_hyatt_small_bottles + claim_republicans_trump_conduct + claim_climate_worry_age_gap\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 5: Fit the CFA Model using semopy ---\n",
    "model = Model(model_desc)\n",
    "model.fit(df_overall_complete)\n",
    "\n",
    "# --- Step 6: Calculate and Print Fit Statistics ---\n",
    "stats   = calc_stats(model)\n",
    "chi2    = stats.loc[\"Value\", \"chi2\"]\n",
    "dof     = stats.loc[\"Value\", \"DoF\"]\n",
    "cfi     = stats.loc[\"Value\", \"CFI\"]\n",
    "rmsea   = stats.loc[\"Value\", \"RMSEA\"]\n",
    "cmin_df = chi2 / dof\n",
    "\n",
    "print(\"Baseline CFA Model Fit Indices for Mist-8 Overall Sample:\")\n",
    "print(f\"  Chi-square: {chi2:.2f}\")\n",
    "print(f\"  Degrees of Freedom: {dof:.0f}\")\n",
    "print(f\"  CMIN/df: {cmin_df:.2f}\")\n",
    "print(f\"  CFI: {cfi:.2f}\")\n",
    "print(f\"  RMSEA: {rmsea:.3f}\")\n",
    "\n",
    "# --- Step 7: Save the plots to a text file ---\n",
    "results_dir = get_working_dir() / 'plots' / 'cfa'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = results_dir / f\"mist8_cfa_results_overall_{timestamp}.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Baseline CFA Model Fit Indices for Mist-8 Overall Sample:\\n\")\n",
    "    f.write(f\"  Chi-square: {chi2:.2f}\\n\")\n",
    "    f.write(f\"  Degrees of Freedom: {dof:.0f}\\n\")\n",
    "    f.write(f\"  CMIN/df: {cmin_df:.2f}\\n\")\n",
    "    f.write(f\"  CFI: {cfi:.2f}\\n\")\n",
    "    f.write(f\"  RMSEA: {rmsea:.3f}\\n\")\n"
   ],
   "id": "afecf7ca0e2f6f99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CFA Model Fit Indices for Mist-8 Overall Sample:\n",
      "  Chi-square: 28.96\n",
      "  Degrees of Freedom: 19\n",
      "  CMIN/df: 1.52\n",
      "  CFI: 0.95\n",
      "  RMSEA: 0.041\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data validity",
   "id": "2b26bb4217158e52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:01:42.382050Z",
     "start_time": "2025-03-12T17:01:42.342377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pingouin as pg  # Install via: pip install pingouin\n",
    "\n",
    "\n",
    "# --- Step 1: Load the three CSV files for MIST‑8 data ---\n",
    "df_t   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_m_t = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "# --- Step 2: Recode claim responses into binary scores ---\n",
    "# Define columns: (Assumes the first four are fake items and the last four are real items)\n",
    "fake_cols = ['claim_stock_manipulation', 'claim_left_wing_salary_lie', \n",
    "             'claim_vaccine_toxins', 'claim_government_disease_spread']\n",
    "real_cols = ['claim_eu_positive_attitudes', 'claim_hyatt_small_bottles', \n",
    "             'claim_republicans_trump_conduct', 'claim_climate_worry_age_gap']\n",
    "mist8_cols = fake_cols + real_cols\n",
    "\n",
    "def recode_mist8(df):\n",
    "    # For fake news items: correct response is \"Fake News\"\n",
    "    for col in fake_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if isinstance(x, str) and x.strip().lower() == \"fake news\" else 0)\n",
    "    # For real news items: correct response is \"Real News\"\n",
    "    for col in real_cols:\n",
    "        df[col] = df[col].apply(lambda x: 1 if isinstance(x, str) and x.strip().lower() == \"real news\" else 0)\n",
    "    return df\n",
    "\n",
    "df_t   = recode_mist8(df_t)\n",
    "df_m   = recode_mist8(df_m)\n",
    "df_m_t = recode_mist8(df_m_t)\n",
    "\n",
    "# --- Step 3: Combine the data to form the overall MIST‑8 sample ---\n",
    "df_mist8 = pd.concat([df_t, df_m, df_m_t], ignore_index=True)\n",
    "df_mist8 = df_mist8.dropna(subset=mist8_cols)  # Remove cases with missing responses\n",
    "\n",
    "# --- Step 4: Compute overall MIST‑8 scale score ---\n",
    "# (Typically, the MIST‑8 score is the sum of correct responses across the 8 items.)\n",
    "df_mist8['mist8_score'] = df_mist8[mist8_cols].sum(axis=1)\n",
    "\n",
    "# --- Step 5: Compute Reliability (Cronbach's alpha) for the 8-item scale ---\n",
    "alpha_full, ci_full = pg.cronbach_alpha(data=df_mist8[mist8_cols])\n",
    "\n",
    "# --- Step 6: Compute Descriptive Statistics for the MIST‑8 Scale Score ---\n",
    "mean_score = df_mist8['mist8_score'].mean()\n",
    "sd_score   = df_mist8['mist8_score'].std()\n",
    "\n",
    "# --- Step 7: Print the Results ---\n",
    "print(\"MIST-8 Full Scale Reliability:\")\n",
    "print(f\"  Cronbach's alpha: {alpha_full:.2f}\")\n",
    "print(f\"  95% CI: [{ci_full[0]:.2f}, {ci_full[1]:.2f}]\")\n",
    "print(f\"  Mean (score): {mean_score:.2f}\")\n",
    "print(f\"  SD (score): {sd_score:.2f}\")\n",
    "\n",
    "# --- (Optional) Compute Reliability for Fake and Real Items Separately ---\n",
    "alpha_fake, ci_fake = pg.cronbach_alpha(data=df_mist8[fake_cols])\n",
    "alpha_real, ci_real = pg.cronbach_alpha(data=df_mist8[real_cols])\n",
    "mean_fake = df_mist8[fake_cols].sum(axis=1).mean()\n",
    "sd_fake   = df_mist8[fake_cols].sum(axis=1).std()\n",
    "mean_real = df_mist8[real_cols].sum(axis=1).mean()\n",
    "sd_real   = df_mist8[real_cols].sum(axis=1).std()\n",
    "\n",
    "print(\"\\nMIST-8 Fake Items:\")\n",
    "print(f\"  Cronbach's alpha: {alpha_fake:.2f}\")\n",
    "print(f\"  95% CI: [{ci_fake[0]:.2f}, {ci_fake[1]:.2f}]\")\n",
    "print(f\"  Mean (score): {mean_fake:.2f}\")\n",
    "print(f\"  SD (score): {sd_fake:.2f}\")\n",
    "\n",
    "print(\"\\nMIST-8 Real Items:\")\n",
    "print(f\"  Cronbach's alpha: {alpha_real:.2f}\")\n",
    "print(f\"  95% CI: [{ci_real[0]:.2f}, {ci_real[1]:.2f}]\")\n",
    "print(f\"  Mean (score): {mean_real:.2f}\")\n",
    "print(f\"  SD (score): {sd_real:.2f}\")\n",
    "\n",
    "# --- Step 8: Save the Results to a Text File ---\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = results_dir / f\"mist8_reliability_results_{timestamp}.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"MIST-8 Full Scale Reliability:\\n\")\n",
    "    f.write(f\"  Cronbach's alpha: {alpha_full:.2f}\\n\")\n",
    "    f.write(f\"  95% CI: [{ci_full[0]:.2f}, {ci_full[1]:.2f}]\\n\")\n",
    "    f.write(f\"  Mean (score): {mean_score:.2f}\\n\")\n",
    "    f.write(f\"  SD (score): {sd_score:.2f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"MIST-8 Fake Items:\\n\")\n",
    "    f.write(f\"  Cronbach's alpha: {alpha_fake:.2f}\\n\")\n",
    "    f.write(f\"  95% CI: [{ci_fake[0]:.2f}, {ci_fake[1]:.2f}]\\n\")\n",
    "    f.write(f\"  Mean (score): {mean_fake:.2f}\\n\")\n",
    "    f.write(f\"  SD (score): {sd_fake:.2f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"MIST-8 Real Items:\\n\")\n",
    "    f.write(f\"  Cronbach's alpha: {alpha_real:.2f}\\n\")\n",
    "    f.write(f\"  95% CI: [{ci_real[0]:.2f}, {ci_real[1]:.2f}]\\n\")\n",
    "    f.write(f\"  Mean (score): {mean_real:.2f}\\n\")\n",
    "    f.write(f\"  SD (score): {sd_real:.2f}\\n\")\n"
   ],
   "id": "2e5207b87211ed42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIST-8 Full Scale Reliability:\n",
      "  Cronbach's alpha: 0.61\n",
      "  95% CI: [0.54, 0.67]\n",
      "  Mean (score): 6.07\n",
      "  SD (score): 1.74\n",
      "\n",
      "MIST-8 Fake Items:\n",
      "  Cronbach's alpha: 0.40\n",
      "  95% CI: [0.28, 0.50]\n",
      "  Mean (score): 2.83\n",
      "  SD (score): 1.07\n",
      "\n",
      "MIST-8 Real Items:\n",
      "  Cronbach's alpha: 0.47\n",
      "  95% CI: [0.36, 0.56]\n",
      "  Mean (score): 3.23\n",
      "  SD (score): 0.96\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlations between demographic factors and mist results",
   "id": "2453f3a214eab694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:52:09.054936Z",
     "start_time": "2025-03-12T17:52:09.004927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ----- Step 1: Load and combine Demographics Data -----\n",
    "# df_t_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_demographics.csv', encoding='utf-8')\n",
    "# df_m_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_demographics.csv', encoding='utf-8')\n",
    "# df_mt_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_demographics.csv', encoding='utf-8')\n",
    "# \n",
    "# # Combine the three demographics files\n",
    "# df_dem = pd.concat([df_t_dem, df_m_dem, df_mt_dem], ignore_index=True)\n",
    "# \n",
    "# # Selected demographic variables\n",
    "# selected_dem = ['age_group', 'political_orientation', 'engagement_with_political_content']\n",
    "# \n",
    "# # Filter out rows where any of these variables is \"I prefer not to answer\"\n",
    "# for var in selected_dem:\n",
    "#     df_dem = df_dem[df_dem[var] != \"I prefer not to answer\"]\n",
    "# \n",
    "# # ----- Step 2: Load MIST Claims Data and compute v -----\n",
    "# def compute_v_metric(df):\n",
    "#     # Assumes columns starting with \"claim_\" are in order:\n",
    "#     # first 4 fake items and next 4 real items.\n",
    "#     claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "#     fake_cols = claim_cols[:4]\n",
    "#     real_cols = claim_cols[4:8]\n",
    "#     # For fake items, correct response is \"Fake News\"\n",
    "#     f_score = df[fake_cols].apply(lambda row: sum(1 for x in row \n",
    "#                                                   if isinstance(x, str) and x.strip().lower() == \"fake news\"),\n",
    "#                                    axis=1)\n",
    "#     # For real items, correct response is \"Real News\"\n",
    "#     r_score = df[real_cols].apply(lambda row: sum(1 for x in row \n",
    "#                                                   if isinstance(x, str) and x.strip().lower() == \"real news\"),\n",
    "#                                    axis=1)\n",
    "#     # Overall veracity discernment v = f + r\n",
    "#     return f_score + r_score\n",
    "# \n",
    "# # Load the three MIST claims files\n",
    "# df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "# df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "# df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "# \n",
    "# # Compute the v metric for each study\n",
    "# df_t_claims['v']  = compute_v_metric(df_t_claims)\n",
    "# df_m_claims['v']  = compute_v_metric(df_m_claims)\n",
    "# df_mt_claims['v'] = compute_v_metric(df_mt_claims)\n",
    "# \n",
    "# # Combine the v metrics (only keeping prolific_id and v)\n",
    "# df_claims_all = pd.concat([df_t_claims[['prolific_id', 'v']],\n",
    "#                            df_m_claims[['prolific_id', 'v']],\n",
    "#                            df_mt_claims[['prolific_id', 'v']]], ignore_index=True)\n",
    "# \n",
    "# # ----- Step 3: Merge Demographics with MIST Metrics -----\n",
    "# df_merged = pd.merge(df_claims_all, df_dem, on=\"prolific_id\", how=\"inner\")\n",
    "# \n",
    "# # ----- Step 4: Define functions for correlation ratio and one-way ANOVA -----\n",
    "# def correlation_ratio(categories, measurements):\n",
    "#     \"\"\"Compute correlation ratio (η) between a categorical and a numeric variable.\"\"\"\n",
    "#     categories = np.array(categories)\n",
    "#     measurements = np.array(measurements)\n",
    "#     # Factorize the categories\n",
    "#     fcat, _ = pd.factorize(categories)\n",
    "#     cat_num = np.unique(fcat)\n",
    "#     overall_mean = np.mean(measurements)\n",
    "#     numerator = sum([len(measurements[fcat == cat]) * (np.mean(measurements[fcat == cat]) - overall_mean)**2 \n",
    "#                      for cat in cat_num])\n",
    "#     denominator = sum((measurements - overall_mean)**2)\n",
    "#     return np.sqrt(numerator/denominator) if denominator != 0 else 0.0\n",
    "# \n",
    "# def anova_pvalue(categories, measurements):\n",
    "#     \"\"\"Perform one-way ANOVA and return F statistic and p-value.\"\"\"\n",
    "#     groups = [measurements[categories == group] for group in np.unique(categories)]\n",
    "#     f_stat, p_val = f_oneway(*groups)\n",
    "#     return f_stat, p_val\n",
    "# \n",
    "# # ----- Step 5: Compare each Demographic Variable with v and print significance -----\n",
    "# for dem in selected_dem:\n",
    "#     cat_values = df_merged[dem].values\n",
    "#     num_values = df_merged['v'].values\n",
    "#     eta = correlation_ratio(cat_values, num_values)\n",
    "#     f_stat, p_val = anova_pvalue(cat_values, num_values)\n",
    "#     significance = \"Statistically Significant\" if p_val < 0.05 else \"Not Statistically Significant\"\n",
    "#     print(f\"For demographic '{dem}' vs. v:\")\n",
    "#     print(f\"  Correlation Ratio (η): {eta:.3f}\")\n",
    "#     print(f\"  ANOVA F-statistic: {f_stat:.3f}, p-value: {p_val:.3e} --> {significance}\")\n",
    "#     print(\"-\"*50)"
   ],
   "id": "95b8e71fba8cb7fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For demographic 'age_group' vs. v:\n",
      "  Correlation Ratio (η): 0.208\n",
      "  ANOVA F-statistic: 4.596, p-value: 3.662e-03 --> Statistically Significant\n",
      "--------------------------------------------------\n",
      "For demographic 'political_orientation' vs. v:\n",
      "  Correlation Ratio (η): 0.417\n",
      "  ANOVA F-statistic: 15.899, p-value: 7.943e-12 --> Statistically Significant\n",
      "--------------------------------------------------\n",
      "For demographic 'engagement_with_political_content' vs. v:\n",
      "  Correlation Ratio (η): 0.173\n",
      "  ANOVA F-statistic: 2.334, p-value: 5.568e-02 --> Not Statistically Significant\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between veracity discernment and age group by converting age group to numeric",
   "id": "75dac66c22a82255"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:58:49.356475Z",
     "start_time": "2025-03-17T16:58:49.207104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Function to compute veracity score from a claims file.\n",
    "# Assumes the file has 8 claim columns (other than prolific_id), \n",
    "# where the first 4 should be \"Fake News\" and the last 4 \"Real News\".\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude the 'prolific_id' column.\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # Define expected responses.\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "def process_veracity_age_correlation(claims_filename, demo_filename, label):\n",
    "    # Load the claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load the demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where age_group is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo['age_group'] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Map age_group to an ordinal numeric scale.\n",
    "    # Adjust the mapping if your labels differ.\n",
    "    age_mapping = {\n",
    "        \"18-25 years old\": 1,\n",
    "        \"26-35 years old\": 2,\n",
    "        \"36-50 years old\": 3,\n",
    "        \"Over 50 years old\": 4\n",
    "    }\n",
    "    df_demo[\"age_numeric\"] = df_demo[\"age_group\"].str.strip().map(age_mapping)\n",
    "    \n",
    "    # Merge the two datasets on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'age_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Drop rows with missing values.\n",
    "    valid_df = df_merged.dropna(subset=[\"veracity_score\", \"age_numeric\"])\n",
    "    \n",
    "    if len(valid_df) < 2:\n",
    "        print(f\"{label}: Insufficient data to compute correlation (n = {len(valid_df)}).\")\n",
    "    else:\n",
    "        # Compute Spearman correlation (suitable since age_numeric is ordinal).\n",
    "        corr, p_val = spearmanr(valid_df[\"veracity_score\"], valid_df[\"age_numeric\"])\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  Spearman correlation between veracity discernment and age group: r = {corr:.3f}, p = {p_val:.3f}\")\n",
    "    print(\"-------\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# For Text Explanation (adjust file names as needed)\n",
    "process_veracity_age_correlation(\"t_claims.csv\", \"t_demographics.csv\", \"Text Explanation\")\n",
    "# For Meme Explanation\n",
    "process_veracity_age_correlation(\"m_claims.csv\", \"m_demographics.csv\", \"Meme Explanation\")\n",
    "# For Meme+Context Explanation\n",
    "process_veracity_age_correlation(\"m_t_claims.csv\", \"m_t_demographics.csv\", \"Meme+Context Explanation\")"
   ],
   "id": "11b6ec2522206081",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Explanation:\n",
      "  Spearman correlation between veracity discernment and age group: r = 0.158, p = 0.100\n",
      "-------\n",
      "\n",
      "Meme Explanation:\n",
      "  Spearman correlation between veracity discernment and age group: r = 0.397, p = 0.000\n",
      "-------\n",
      "\n",
      "Meme+Context Explanation:\n",
      "  Spearman correlation between veracity discernment and age group: r = 0.118, p = 0.237\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation beteen veracity discernment and political orientation by converting the latter to numeric",
   "id": "7ab9b84826b65583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:00:46.426753Z",
     "start_time": "2025-03-17T17:00:46.365856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Function to compute veracity score from a claims file.\n",
    "# Assumes that aside from the 'prolific_id' column, the file contains 8 columns\n",
    "# in order where the first 4 should be answered \"Fake News\" and the last 4 \"Real News\".\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude the 'prolific_id' column.\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # Define the expected answers.\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "def process_veracity_pol_correlation(claims_filename, demo_filename, label):\n",
    "    # Load the claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load the demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where political_orientation is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo['political_orientation'] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Map political_orientation into an ordinal numeric scale.\n",
    "    pol_mapping = {\n",
    "        \"Very Liberal\": 1,\n",
    "        \"Moderately Liberal\": 2,\n",
    "        \"Moderate\": 3,\n",
    "        \"Moderately Conservative\": 4,\n",
    "        \"Very Conservative\": 5\n",
    "    }\n",
    "    df_demo[\"pol_numeric\"] = df_demo[\"political_orientation\"].str.strip().map(pol_mapping)\n",
    "    \n",
    "    # Merge the claims and demographics data on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'pol_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Drop rows with missing values.\n",
    "    valid_df = df_merged.dropna(subset=[\"veracity_score\", \"pol_numeric\"])\n",
    "    \n",
    "    if len(valid_df) < 2:\n",
    "        print(f\"{label}: Insufficient data to compute correlation (n = {len(valid_df)}).\")\n",
    "    else:\n",
    "        # Compute Spearman correlation.\n",
    "        corr, p_val = spearmanr(valid_df[\"veracity_score\"], valid_df[\"pol_numeric\"])\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  Spearman correlation between veracity discernment and political orientation: r = {corr:.3f}, p = {p_val:.3f}\")\n",
    "    print(\"-------\\n\")\n",
    "\n",
    "# Process for each explanation type.\n",
    "# Adjust the file names if necessary.\n",
    "process_veracity_pol_correlation(\"t_claims.csv\", \"t_demographics.csv\", \"Text Explanation\")\n",
    "process_veracity_pol_correlation(\"m_claims.csv\", \"m_demographics.csv\", \"Meme Explanation\")\n",
    "process_veracity_pol_correlation(\"m_t_claims.csv\", \"m_t_demographics.csv\", \"Meme+Context Explanation\")"
   ],
   "id": "f42a06f72dd12904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Explanation:\n",
      "  Spearman correlation between veracity discernment and political orientation: r = -0.405, p = 0.000\n",
      "-------\n",
      "\n",
      "Meme Explanation:\n",
      "  Spearman correlation between veracity discernment and political orientation: r = -0.277, p = 0.005\n",
      "-------\n",
      "\n",
      "Meme+Context Explanation:\n",
      "  Spearman correlation between veracity discernment and political orientation: r = -0.451, p = 0.000\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Corr between veracity discernment and political orientation numeric across all explanation types",
   "id": "7a53ee81cb70997f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:22:14.588114Z",
     "start_time": "2025-03-17T17:22:14.562539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude the 'prolific_id' column.\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # For the first 4 claims, the correct answer is \"Fake News\"; for the last 4, \"Real News\".\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "# Define a list of file pairs (claims file, demographics file) for each explanation type.\n",
    "file_pairs = [\n",
    "    (\"t_claims.csv\", \"t_demographics.csv\"),\n",
    "    (\"m_claims.csv\", \"m_demographics.csv\"),\n",
    "    (\"m_t_claims.csv\", \"m_t_demographics.csv\")\n",
    "]\n",
    "\n",
    "# List to hold merged data for each explanation type.\n",
    "list_dfs = []\n",
    "\n",
    "for claims_filename, demo_filename in file_pairs:\n",
    "    # Load claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load corresponding demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where political_orientation is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo[\"political_orientation\"] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Merge on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'political_orientation']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    list_dfs.append(df_merged)\n",
    "\n",
    "# Concatenate all merged datasets.\n",
    "df_all = pd.concat(list_dfs, ignore_index=True)\n",
    "\n",
    "# Map political_orientation to an ordinal numeric scale.\n",
    "pol_mapping = {\n",
    "    \"Very Liberal\": 1,\n",
    "    \"Moderately Liberal\": 2,\n",
    "    \"Moderate\": 3,\n",
    "    \"Moderately Conservative\": 4,\n",
    "    \"Very Conservative\": 5\n",
    "}\n",
    "df_all[\"pol_numeric\"] = df_all[\"political_orientation\"].str.strip().map(pol_mapping)\n",
    "\n",
    "# Drop rows with missing values in veracity_score or pol_numeric.\n",
    "df_all = df_all.dropna(subset=[\"veracity_score\", \"pol_numeric\"])\n",
    "\n",
    "if len(df_all) < 2:\n",
    "    print(\"Insufficient data to compute correlation (n =\", len(df_all), \").\")\n",
    "else:\n",
    "    corr, p_val = pearsonr(df_all[\"veracity_score\"], df_all[\"pol_numeric\"])\n",
    "    print(\"Pearson correlation between veracity discernment and political orientation (across all explanation types):\")\n",
    "    print(f\"  r = {corr:.3f}, p = {p_val:.20f}\")"
   ],
   "id": "63a69fd68c06e47f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between veracity discernment and political orientation (across all explanation types):\n",
      "  r = -0.402, p = 0.00000000000021298597\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between veracity discernment and age group numeric colapsed to all explanation types",
   "id": "aeb2ab772cc5d77b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:30:52.789816Z",
     "start_time": "2025-03-17T17:30:52.765017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude 'prolific_id'\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # Expected answers: first 4 claims should be \"Fake News\" and last 4 \"Real News\".\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "def process_file_pair(claims_filename, demo_filename):\n",
    "    # Load claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    # Filter out rows where age_group is \"I prefer not to answer\"\n",
    "    df_demo = df_demo[df_demo['age_group'] != \"I prefer not to answer\"]\n",
    "    # Map age_group to an ordinal numeric scale.\n",
    "    age_mapping = {\n",
    "        \"18-25 years old\": 1,\n",
    "        \"26-35 years old\": 2,\n",
    "        \"36-50 years old\": 3,\n",
    "        \"Over 50 years old\": 4\n",
    "    }\n",
    "    df_demo[\"age_numeric\"] = df_demo[\"age_group\"].str.strip().map(age_mapping)\n",
    "    \n",
    "    # Merge on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'age_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "# Define file pairs for each explanation type.\n",
    "file_pairs = [\n",
    "    (\"t_claims.csv\", \"t_demographics.csv\"),\n",
    "    (\"m_claims.csv\", \"m_demographics.csv\"),\n",
    "    (\"m_t_claims.csv\", \"m_t_demographics.csv\")\n",
    "]\n",
    "\n",
    "# Process each pair and collect the merged data.\n",
    "list_dfs = []\n",
    "for claims_file, demo_file in file_pairs:\n",
    "    df_merged = process_file_pair(claims_file, demo_file)\n",
    "    list_dfs.append(df_merged)\n",
    "\n",
    "# Concatenate all merged data into one dataframe.\n",
    "df_all = pd.concat(list_dfs, ignore_index=True)\n",
    "df_all = df_all.dropna(subset=[\"veracity_score\", \"age_numeric\"])\n",
    "\n",
    "if len(df_all) < 2:\n",
    "    print(\"Insufficient data to compute correlation (n = {})\".format(len(df_all)))\n",
    "else:\n",
    "    corr, p_val = spearmanr(df_all[\"veracity_score\"], df_all[\"age_numeric\"])\n",
    "    print(\"Spearman correlation between veracity discernment and age group (across all explanation types):\")\n",
    "    print(f\"  r = {corr:.3f}, p = {p_val:.12f}\")"
   ],
   "id": "e8328b4b6f9bec08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between veracity discernment and age group (across all explanation types):\n",
      "  r = 0.202, p = 0.000336090161\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between veracity discernment and engagement with political content numeric colapse for all explanation types",
   "id": "2a1c5d3b5f6b001"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:35:05.170184Z",
     "start_time": "2025-03-17T17:35:05.121982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Function to compute veracity score from a claims file.\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude the 'prolific_id' column.\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # Expected answers: first 4 should be \"Fake News\" and last 4 \"Real News\".\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "# Process one file pair (claims and demographics) and return the merged DataFrame.\n",
    "def process_file_pair_engagement(claims_filename, demo_filename):\n",
    "    # Load the claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load the demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where engagement_with_political_content is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo[\"engagement_with_political_content\"] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Map engagement_with_political_content to an ordinal numeric scale.\n",
    "    engagement_mapping = {\n",
    "        \"Never\": 1,\n",
    "        \"Rarely\": 2,\n",
    "        \"Sometimes\": 3,\n",
    "        \"Often\": 4,\n",
    "        \"Very Frequently\": 5\n",
    "    }\n",
    "    df_demo[\"engagement_numeric\"] = df_demo[\"engagement_with_political_content\"].str.strip().map(engagement_mapping)\n",
    "    \n",
    "    # Merge on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'engagement_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "# Define file pairs for each explanation type.\n",
    "file_pairs = [\n",
    "    (\"t_claims.csv\", \"t_demographics.csv\"),\n",
    "    (\"m_claims.csv\", \"m_demographics.csv\"),\n",
    "    (\"m_t_claims.csv\", \"m_t_demographics.csv\")\n",
    "]\n",
    "\n",
    "list_dfs = []\n",
    "for claims_file, demo_file in file_pairs:\n",
    "    df_merged = process_file_pair_engagement(claims_file, demo_file)\n",
    "    list_dfs.append(df_merged)\n",
    "\n",
    "# Concatenate all merged data into one DataFrame.\n",
    "df_all = pd.concat(list_dfs, ignore_index=True)\n",
    "df_all = df_all.dropna(subset=[\"veracity_score\", \"engagement_numeric\"])\n",
    "\n",
    "if len(df_all) < 2:\n",
    "    print(\"Insufficient data to compute correlation (n =\", len(df_all), \").\")\n",
    "else:\n",
    "    # Compute Spearman correlation between veracity score and political engagement.\n",
    "    corr, p_val = spearmanr(df_all[\"veracity_score\"], df_all[\"engagement_numeric\"])\n",
    "    print(\"Spearman correlation between veracity discernment and political engagement (across all explanation types):\")\n",
    "    print(f\"  r = {corr:.3f}, p = {p_val:.3f}\")"
   ],
   "id": "8a01ad3440bb6df9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between veracity discernment and political engagement (across all explanation types):\n",
      "  r = -0.020, p = 0.721\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between veracity discernment and education level numeric for all explanation types",
   "id": "e5fe86de5582a9cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:40:16.588523Z",
     "start_time": "2025-03-17T17:40:16.522617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Function to compute veracity score from a claims file.\n",
    "# Assumes that aside from 'prolific_id', the file contains 8 claim columns,\n",
    "# where the first 4 should be \"Fake News\" and the last 4 \"Real News\".\n",
    "def compute_veracity_score(df_claims):\n",
    "    # Exclude 'prolific_id'\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    # Define expected responses.\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "# Process one file pair (claims file and demographics file) and return merged DataFrame.\n",
    "def process_file_pair_education(claims_filename, demo_filename):\n",
    "    # Load the claims file and compute veracity score.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load the demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where education_level is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo[\"education_level\"] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Map education_level to an ordinal numeric scale.\n",
    "    education_mapping = {\n",
    "        \"Elementary education\": 1,\n",
    "        \"High school diploma or equivalent\": 2,\n",
    "        \"Bachelor's Degree\": 3,\n",
    "        \"Master's Degree\": 4,\n",
    "        \"Doctoral degree (PhD)\": 5\n",
    "    }\n",
    "    df_demo[\"education_numeric\"] = df_demo[\"education_level\"].str.strip().map(education_mapping)\n",
    "    \n",
    "    # Merge the claims and demographics data on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'education_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "# Define file pairs for each explanation type.\n",
    "file_pairs = [\n",
    "    (\"t_claims.csv\", \"t_demographics.csv\"),\n",
    "    (\"m_claims.csv\", \"m_demographics.csv\"),\n",
    "    (\"m_t_claims.csv\", \"m_t_demographics.csv\")\n",
    "]\n",
    "\n",
    "# Process each pair and combine them.\n",
    "list_dfs = []\n",
    "for claims_file, demo_file in file_pairs:\n",
    "    df_merged = process_file_pair_education(claims_file, demo_file)\n",
    "    list_dfs.append(df_merged)\n",
    "\n",
    "# Concatenate all merged data into one DataFrame.\n",
    "df_all = pd.concat(list_dfs, ignore_index=True)\n",
    "df_all = df_all.dropna(subset=[\"veracity_score\", \"education_numeric\"])\n",
    "\n",
    "if len(df_all) < 2:\n",
    "    print(\"Insufficient data to compute correlation (n =\", len(df_all), \").\")\n",
    "else:\n",
    "    corr, p_val = spearmanr(df_all[\"veracity_score\"], df_all[\"education_numeric\"])\n",
    "    print(\"Spearman correlation between veracity discernment and education level (across all explanation types):\")\n",
    "    print(f\"  r = {corr:.3f}, p = {p_val:.3f}\")"
   ],
   "id": "d128ffc0c5a4bb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between veracity discernment and education level (across all explanation types):\n",
      "  r = 0.119, p = 0.165\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between veracity discernment and meme familiarity numeric",
   "id": "d89c98c47c8a199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:45:34.568690Z",
     "start_time": "2025-03-17T17:45:34.522562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Function to compute veracity score from a claims file.\n",
    "# Assumes that, aside from 'prolific_id', the file contains 8 claim columns,\n",
    "# where the first 4 should be answered as \"Fake News\" and the last 4 as \"Real News\".\n",
    "def compute_veracity_score(df_claims):\n",
    "    cols = [col for col in df_claims.columns if col != \"prolific_id\"]\n",
    "    expected = [\"Fake News\"] * 4 + [\"Real News\"] * 4\n",
    "    scores = []\n",
    "    for idx, row in df_claims.iterrows():\n",
    "        score = 0\n",
    "        for col, exp in zip(cols, expected):\n",
    "            if isinstance(row[col], str) and row[col].strip() == exp:\n",
    "                score += 1\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df_claims.index)\n",
    "\n",
    "# Process one file pair: load claims file and demographics file, compute veracity score,\n",
    "# filter out rows with \"I prefer not to answer\" for meme culture familiarity,\n",
    "# and map meme culture familiarity to a numeric ordinal scale.\n",
    "def process_file_pair_meme_culture(claims_filename, demo_filename):\n",
    "    # Load claims file.\n",
    "    claims_path = get_working_dir() / 'data' / 'processed' / claims_filename\n",
    "    df_claims = pd.read_csv(claims_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    df_claims[\"veracity_score\"] = compute_veracity_score(df_claims)\n",
    "    \n",
    "    # Load demographics file.\n",
    "    demo_path = get_working_dir() / 'data' / 'processed' / demo_filename\n",
    "    df_demo = pd.read_csv(demo_path, encoding='utf-8', dtype={'prolific_id': str})\n",
    "    \n",
    "    # Filter out rows where meme_culture_familiarity is \"I prefer not to answer\".\n",
    "    df_demo = df_demo[df_demo[\"meme_culture_familiarity\"] != \"I prefer not to answer\"]\n",
    "    \n",
    "    # Map meme_culture_familiarity to an ordinal numeric scale.\n",
    "    meme_culture_mapping = {\n",
    "        \"Not familiar at all (I rarely understand meme references)\": 1,\n",
    "        \"Slightly familiar (I understand basic, widely-known memes)\": 2,\n",
    "        \"Moderately familiar (I understand most popular memes and their variations)\": 3,\n",
    "        \"Very familiar (I understand complex meme references and their evolution)\": 4,\n",
    "        \"Extremely familiar (I actively follow meme trends and their cultural context)\": 5\n",
    "    }\n",
    "    df_demo[\"meme_culture_numeric\"] = df_demo[\"meme_culture_familiarity\"].str.strip().map(meme_culture_mapping)\n",
    "    \n",
    "    # Merge the two datasets on prolific_id.\n",
    "    df_merged = pd.merge(\n",
    "        df_claims[['prolific_id', 'veracity_score']],\n",
    "        df_demo[['prolific_id', 'meme_culture_numeric']],\n",
    "        on=\"prolific_id\", how=\"inner\"\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "# Define file pairs for the meme-based explanation types.\n",
    "file_pairs = [\n",
    "    (\"m_claims.csv\", \"m_demographics.csv\"),\n",
    "    (\"m_t_claims.csv\", \"m_t_demographics.csv\")\n",
    "]\n",
    "\n",
    "# Process each file pair and collect merged data.\n",
    "merged_dfs = []\n",
    "for claims_file, demo_file in file_pairs:\n",
    "    df_merged = process_file_pair_meme_culture(claims_file, demo_file)\n",
    "    merged_dfs.append(df_merged)\n",
    "\n",
    "# Concatenate merged data from both explanation types.\n",
    "df_all = pd.concat(merged_dfs, ignore_index=True)\n",
    "df_all = df_all.dropna(subset=[\"veracity_score\", \"meme_culture_numeric\"])\n",
    "\n",
    "if len(df_all) < 2:\n",
    "    print(\"Insufficient data to compute correlation (n =\", len(df_all), \").\")\n",
    "else:\n",
    "    corr, p_val = spearmanr(df_all[\"veracity_score\"], df_all[\"meme_culture_numeric\"])\n",
    "    print(\"Spearman correlation between veracity discernment and meme culture familiarity (collapsed across meme-based explanation types):\")\n",
    "    print(f\"  r = {corr:.3f}, p = {p_val:.3f}\")\n"
   ],
   "id": "c4524873c6b52b27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between veracity discernment and meme culture familiarity (collapsed across meme-based explanation types):\n",
      "  r = -0.076, p = 0.280\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checking how v changes with age_group values and political_orientation values",
   "id": "4f70883631943a48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T18:16:34.197012Z",
     "start_time": "2025-03-12T18:16:34.032048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- Step 1: Load Demographics Data -----\n",
    "df_t_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_demographics.csv', encoding='utf-8')\n",
    "df_m_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_demographics.csv', encoding='utf-8')\n",
    "df_mt_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_demographics.csv', encoding='utf-8')\n",
    "\n",
    "# Concatenate demographics and clean whitespace\n",
    "df_dem = pd.concat([df_t_dem, df_m_dem, df_mt_dem], ignore_index=True)\n",
    "for col in ['age_group', 'political_orientation', 'engagement_with_political_content']:\n",
    "    df_dem[col] = df_dem[col].astype(str).str.strip()\n",
    "\n",
    "# Filter out rows with \"I prefer not to answer\"\n",
    "selected_dem = ['age_group', 'political_orientation', 'engagement_with_political_content']\n",
    "for var in selected_dem:\n",
    "    df_dem = df_dem[df_dem[var] != \"I prefer not to answer\"]\n",
    "\n",
    "# ----- Step 2: Load MIST Claims Data and Compute v -----\n",
    "def compute_v_metric(df):\n",
    "    # Assumes columns starting with \"claim_\" are ordered: first 4 fake, next 4 real.\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    f_score = df[fake_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"fake news\"),\n",
    "                                   axis=1)\n",
    "    r_score = df[real_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"real news\"),\n",
    "                                   axis=1)\n",
    "    return f_score + r_score\n",
    "\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "df_t_claims['v']  = compute_v_metric(df_t_claims)\n",
    "df_m_claims['v']  = compute_v_metric(df_m_claims)\n",
    "df_mt_claims['v'] = compute_v_metric(df_mt_claims)\n",
    "\n",
    "# Retain only prolific_id and v, and then combine.\n",
    "df_claims = pd.concat([df_t_claims[['prolific_id', 'v']],\n",
    "                       df_m_claims[['prolific_id', 'v']],\n",
    "                       df_mt_claims[['prolific_id', 'v']]],\n",
    "                      ignore_index=True)\n",
    "\n",
    "# ----- Step 3: Merge Demographics with Claims Data -----\n",
    "df_merged = pd.merge(df_claims, df_dem, on=\"prolific_id\", how=\"inner\")\n",
    "\n",
    "# Clean up political_orientation:\n",
    "# Replace \"Very Conservative\" with \"Conservative\" to match the desired order.\n",
    "df_merged['political_orientation'] = df_merged['political_orientation']\n",
    "\n",
    "# Print unique values for diagnostic purposes.\n",
    "print(\"Unique political_orientation values after merge:\")\n",
    "print(df_merged[\"political_orientation\"].unique())\n",
    "\n",
    "# ----- Step 4: Plot v by Age Group -----\n",
    "age_stats = df_merged.groupby('age_group')['v'].agg(['mean', 'std', 'count'])\n",
    "age_stats['sem'] = age_stats['std'] / np.sqrt(age_stats['count'])\n",
    "print(\"\\nDescriptive Statistics for v by Age Group:\")\n",
    "print(age_stats)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.bar(age_stats.index, age_stats['mean'], yerr=age_stats['sem'], capsize=5)\n",
    "ax.set_xlabel(\"Age Group\")\n",
    "ax.set_ylabel(\"Mean of Veracity Discernment\")\n",
    "ax.set_title(\"Mean of Veracity Discernment by Age Group\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(results_dir / 'v_by_age_group.png')\n",
    "plt.close(fig)\n",
    "\n",
    "# ----- Step 5: Plot v by Political Orientation -----\n",
    "# Define the desired order.\n",
    "ordered_pol = [\"Very Liberal\", \"Moderately Liberal\", \"Moderate\", \"Moderately Conservative\", \"Very Conservative\"]\n",
    "\n",
    "# Convert political_orientation to a categorical variable with specified order.\n",
    "df_merged['political_orientation'] = pd.Categorical(df_merged['political_orientation'],\n",
    "                                                    categories=ordered_pol,\n",
    "                                                    ordered=True)\n",
    "\n",
    "pol_stats = df_merged.groupby('political_orientation', observed=True)['v'].agg(['mean', 'std', 'count'])\n",
    "pol_stats['sem'] = pol_stats['std'] / np.sqrt(pol_stats['count'])\n",
    "# Reindex to desired order.\n",
    "pol_stats = pol_stats.loc[ordered_pol]\n",
    "print(\"\\nDescriptive Statistics for v by Political Orientation:\")\n",
    "print(pol_stats)\n",
    "\n",
    "# Abbreviation mapping for display.\n",
    "abbr_dict = {\"Very Liberal\": \"VL\", \n",
    "             \"Moderately Liberal\": \"ML\", \n",
    "             \"Moderate\": \"Mod\", \n",
    "             \"Moderately Conservative\": \"MC\", \n",
    "             \"Very Conservative\": \"VC\"}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_pos = np.arange(len(pol_stats.index))\n",
    "ax.bar(x_pos, pol_stats['mean'], yerr=pol_stats['sem'], capsize=5)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([abbr_dict[label] for label in pol_stats.index])\n",
    "ax.set_xlabel(\"Political Orientation\")\n",
    "ax.set_ylabel(\"Mean of Veracity Discernment\")\n",
    "ax.set_title(\"Mean of Veracity Discernment by Political Orientation\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(results_dir / 'v_by_political_orientation.png')\n",
    "plt.close(fig)"
   ],
   "id": "4396b02b3e88ad47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique political_orientation values after merge:\n",
      "['Moderate' 'Moderately Conservative' 'Moderately Liberal'\n",
      " 'Very Conservative' 'Very Liberal']\n",
      "\n",
      "Descriptive Statistics for v by Age Group:\n",
      "                       mean       std  count       sem\n",
      "age_group                                             \n",
      "18-25 years old    5.815789  1.798727     38  0.291792\n",
      "26-35 years old    5.678571  1.757330     84  0.191740\n",
      "36-50 years old    6.059406  1.826591    101  0.181753\n",
      "Over 50 years old  6.611765  1.456464     85  0.157976\n",
      "\n",
      "Descriptive Statistics for v by Political Orientation:\n",
      "                             mean       std  count       sem\n",
      "political_orientation                                       \n",
      "Very Liberal             6.962963  0.889413     54  0.121034\n",
      "Moderately Liberal       6.481481  1.475730     81  0.163970\n",
      "Moderate                 6.311475  1.698440     61  0.217463\n",
      "Moderately Conservative  5.482353  1.931023     85  0.209449\n",
      "Very Conservative        4.444444  1.577079     27  0.303509\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Veracity discernment per age and political orientation",
   "id": "b42c48ad53c70117"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T18:40:04.941182Z",
     "start_time": "2025-03-12T18:40:04.777415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- Build df_merged -----\n",
    "# Load demographics data\n",
    "df_t_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_demographics.csv', encoding='utf-8')\n",
    "df_m_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_demographics.csv', encoding='utf-8')\n",
    "df_mt_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_demographics.csv', encoding='utf-8')\n",
    "df_dem = pd.concat([df_t_dem, df_m_dem, df_mt_dem], ignore_index=True)\n",
    "for col in ['age_group', 'political_orientation', 'engagement_with_political_content']:\n",
    "    df_dem[col] = df_dem[col].astype(str).str.strip()\n",
    "# Filter out \"I prefer not to answer\" rows for selected demographics.\n",
    "for var in ['age_group', 'political_orientation', 'engagement_with_political_content']:\n",
    "    df_dem = df_dem[df_dem[var] != \"I prefer not to answer\"]\n",
    "\n",
    "# Load MIST claims data and compute overall veracity score (v)\n",
    "def compute_v_metric(df):\n",
    "    # Assumes columns starting with \"claim_\" are arranged so that the first 4 are fake items\n",
    "    # and the next 4 are real items.\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    f_score = df[fake_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"fake news\"), axis=1)\n",
    "    r_score = df[real_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"real news\"), axis=1)\n",
    "    return f_score + r_score\n",
    "\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "df_t_claims['v']  = compute_v_metric(df_t_claims)\n",
    "df_m_claims['v']  = compute_v_metric(df_m_claims)\n",
    "df_mt_claims['v'] = compute_v_metric(df_mt_claims)\n",
    "\n",
    "df_claims = pd.concat([df_t_claims[['prolific_id', 'v']],\n",
    "                       df_m_claims[['prolific_id', 'v']],\n",
    "                       df_mt_claims[['prolific_id', 'v']]],\n",
    "                      ignore_index=True)\n",
    "\n",
    "# Merge the claims data with demographics on prolific_id\n",
    "df_merged = pd.merge(df_claims, df_dem, on=\"prolific_id\", how=\"inner\")\n",
    "\n",
    "# Clean up political_orientation.\n",
    "# In your data, you have \"Very Conservative\"; for our desired order, we want \"Conservative\".\n",
    "df_merged['political_orientation'] = df_merged['political_orientation']\n",
    "\n",
    "# ----- Define desired order for age_group and political_orientation -----\n",
    "age_order = [\"18-25 years old\", \"26-35 years old\", \"36-50 years old\", \"Over 50 years old\"]\n",
    "pol_order = [\"Very Liberal\", \"Moderately Liberal\", \"Moderate\", \"Moderately Conservative\", \"Very Conservative\"]\n",
    "\n",
    "df_merged['age_group'] = pd.Categorical(df_merged['age_group'], categories=age_order, ordered=True)\n",
    "df_merged['political_orientation'] = pd.Categorical(df_merged['political_orientation'], \n",
    "                                                    categories=pol_order, ordered=True)\n",
    "\n",
    "# ----- Step 1: Compute subgroup summary statistics for v -----\n",
    "subgroup_stats = df_merged.groupby(['age_group', 'political_orientation'])['v'].agg(['mean', 'std', 'count']).reset_index()\n",
    "subgroup_stats['sem'] = subgroup_stats['std'] / np.sqrt(subgroup_stats['count'])\n",
    "print(\"Veracity discernment (v) by Age Group and Political Orientation:\")\n",
    "print(subgroup_stats)\n",
    "\n",
    "# ----- Step 2: Plot using seaborn for better visualization -----\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create a bar plot with age_group on the x-axis and political_orientation as the hue.\n",
    "ax = sns.barplot(data=df_merged, x=\"age_group\", y=\"v\", hue=\"political_orientation\", \n",
    "                 order=age_order, hue_order=pol_order, ci=\"sd\")\n",
    "ax.set_xlabel(\"Age Group\")\n",
    "ax.set_ylabel(\"Mean Veracity Discernment (v)\")\n",
    "ax.set_title(\"Mean Veracity Discernment by Age Group and Political Orientation\")\n",
    "# Optionally, abbreviate the political_orientation in the legend.\n",
    "abbr_dict = {\"Very Liberal\": \"VL\", \"Moderately Liberal\": \"ML\", \"Moderate\": \"Mod\", \n",
    "             \"Moderately Conservative\": \"MC\", \"Very Conservative\": \"C\"}\n",
    "new_labels = [abbr_dict.get(label.get_text(), label.get_text()) for label in ax.get_legend().get_texts()]\n",
    "for text, new in zip(ax.get_legend().get_texts(), new_labels):\n",
    "    text.set_text(new)\n",
    "ax.legend(title=\"Political Orientation\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"v_by_age_and_pol.png\")\n",
    "plt.close()\n"
   ],
   "id": "c68456a39ac8ce07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veracity discernment (v) by Age Group and Political Orientation:\n",
      "            age_group    political_orientation      mean       std  count  \\\n",
      "0     18-25 years old             Very Liberal  7.100000  0.737865     10   \n",
      "1     18-25 years old       Moderately Liberal  6.428571  1.272418      7   \n",
      "2     18-25 years old                 Moderate  6.285714  1.704336      7   \n",
      "3     18-25 years old  Moderately Conservative  4.454545  1.863525     11   \n",
      "4     18-25 years old        Very Conservative  4.000000  1.000000      3   \n",
      "5     26-35 years old             Very Liberal  6.785714  0.801784     14   \n",
      "6     26-35 years old       Moderately Liberal  6.217391  1.650249     23   \n",
      "7     26-35 years old                 Moderate  5.500000  1.689065     18   \n",
      "8     26-35 years old  Moderately Conservative  5.400000  1.729009     20   \n",
      "9     26-35 years old        Very Conservative  3.555556  1.424001      9   \n",
      "10    36-50 years old             Very Liberal  7.066667  1.032796     15   \n",
      "11    36-50 years old       Moderately Liberal  6.478261  1.274561     23   \n",
      "12    36-50 years old                 Moderate  6.600000  1.788854     20   \n",
      "13    36-50 years old  Moderately Conservative  5.250000  2.155264     32   \n",
      "14    36-50 years old        Very Conservative  5.181818  1.537412     11   \n",
      "15  Over 50 years old             Very Liberal  6.933333  0.961150     15   \n",
      "16  Over 50 years old       Moderately Liberal  6.714286  1.560084     28   \n",
      "17  Over 50 years old                 Moderate  6.875000  1.360147     16   \n",
      "18  Over 50 years old  Moderately Conservative  6.409091  1.469016     22   \n",
      "19  Over 50 years old        Very Conservative  4.750000  1.707825      4   \n",
      "\n",
      "         sem  \n",
      "0   0.233333  \n",
      "1   0.480929  \n",
      "2   0.644179  \n",
      "3   0.561874  \n",
      "4   0.577350  \n",
      "5   0.214286  \n",
      "6   0.344101  \n",
      "7   0.398116  \n",
      "8   0.386618  \n",
      "9   0.474667  \n",
      "10  0.266667  \n",
      "11  0.265764  \n",
      "12  0.400000  \n",
      "13  0.381000  \n",
      "14  0.463547  \n",
      "15  0.248168  \n",
      "16  0.294828  \n",
      "17  0.340037  \n",
      "18  0.313195  \n",
      "19  0.853913  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/vf3sjpn92ql19dv_1bf4dl1m0000gn/T/ipykernel_93442/1334178522.py:67: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  subgroup_stats = df_merged.groupby(['age_group', 'political_orientation'])['v'].agg(['mean', 'std', 'count']).reset_index()\n",
      "/var/folders/fv/vf3sjpn92ql19dv_1bf4dl1m0000gn/T/ipykernel_93442/1334178522.py:76: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.\n",
      "\n",
      "  ax = sns.barplot(data=df_merged, x=\"age_group\", y=\"v\", hue=\"political_orientation\",\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correlation between affect change and veracity discernment",
   "id": "e69be978bdc0ca8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:38:54.187386Z",
     "start_time": "2025-03-13T14:38:54.042108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# Step 1: Load and Process Affect Data\n",
    "# ============================\n",
    "# Assume the affect CSV files include columns for both pre and post affect measures.\n",
    "# For Positive Affect (PA): pre_active, pre_determined, pre_attentive, pre_inspired, pre_alert\n",
    "# For Negative Affect (NA): pre_afraid, pre_nervous, pre_upset, pre_hostile, pre_ashamed\n",
    "# Similarly for post_ variables.\n",
    "\n",
    "# Mapping for ordinal responses\n",
    "affect_mapping = {\n",
    "    \"Not at all\": 1,\n",
    "    \"Slightly\": 2,\n",
    "    \"Moderately\": 3,\n",
    "    \"Very\": 4,\n",
    "    \"Extremely\": 5,\n",
    "    \"I prefer not to answer\": np.nan\n",
    "}\n",
    "\n",
    "def map_affect(df):\n",
    "    # Process both pre and post columns (selecting columns that start with \"pre_\" or \"post_\")\n",
    "    affect_cols = [col for col in df.columns if col.startswith(\"pre_\") or col.startswith(\"post_\")]\n",
    "    for col in affect_cols:\n",
    "        df[col] = df[col].map(affect_mapping)\n",
    "    return df\n",
    "\n",
    "# Load the three affect files.\n",
    "df_t_affect   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_affect.csv', encoding='utf-8')\n",
    "df_m_affect   = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_affect.csv', encoding='utf-8')\n",
    "df_mt_affect  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_affect.csv', encoding='utf-8')\n",
    "\n",
    "# Map ordinal responses to numeric values.\n",
    "df_t_affect  = map_affect(df_t_affect)\n",
    "df_m_affect  = map_affect(df_m_affect)\n",
    "df_mt_affect = map_affect(df_mt_affect)\n",
    "\n",
    "# Define the lists of items.\n",
    "pa_items = ['active', 'determined', 'attentive', 'inspired', 'alert']\n",
    "na_items = ['afraid', 'nervous', 'upset', 'hostile', 'ashamed']\n",
    "\n",
    "def compute_affect_change(df):\n",
    "    # For each scale, assume columns are named as pre_<item> and post_<item>\n",
    "    pa_pre_cols = [f\"pre_{item}\" for item in pa_items]\n",
    "    pa_post_cols = [f\"post_{item}\" for item in pa_items]\n",
    "    na_pre_cols = [f\"pre_{item}\" for item in na_items]\n",
    "    na_post_cols = [f\"post_{item}\" for item in na_items]\n",
    "    \n",
    "    # Compute composite scores (mean) for pre and post.\n",
    "    df['PA_pre'] = df[pa_pre_cols].mean(axis=1)\n",
    "    df['PA_post'] = df[pa_post_cols].mean(axis=1)\n",
    "    df['NA_pre'] = df[na_pre_cols].mean(axis=1)\n",
    "    df['NA_post'] = df[na_post_cols].mean(axis=1)\n",
    "    \n",
    "    # Compute change scores (post - pre)\n",
    "    df['PA_change'] = df['PA_post'] - df['PA_pre']\n",
    "    df['NA_change'] = df['NA_post'] - df['NA_pre']\n",
    "    \n",
    "    # Optionally, drop rows with NaN in the computed change scores.\n",
    "    df = df.dropna(subset=['PA_change', 'NA_change'])\n",
    "    # Retain only the unique identifier and the change scores.\n",
    "    return df[['prolific_id', 'PA_change', 'NA_change']]\n",
    "\n",
    "# Compute affect change for each file.\n",
    "df_t_affect_change  = compute_affect_change(df_t_affect)\n",
    "df_m_affect_change  = compute_affect_change(df_m_affect)\n",
    "df_mt_affect_change = compute_affect_change(df_mt_affect)\n",
    "\n",
    "# Combine affect change data.\n",
    "df_affect_change = pd.concat([df_t_affect_change, df_m_affect_change, df_mt_affect_change], ignore_index=True)\n",
    "# If a participant appears more than once, you might average their scores:\n",
    "df_affect_change = df_affect_change.groupby('prolific_id', as_index=False).mean()\n",
    "\n",
    "# ============================\n",
    "# Step 2: Load and Process MIST Claims Data (Compute v)\n",
    "# ============================\n",
    "def compute_v_metric(df):\n",
    "    # Assumes columns starting with \"claim_\" are ordered: first 4 fake items, next 4 real items.\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    f_score = df[fake_cols].apply(lambda row: sum(1 for x in row if isinstance(x, str) and x.strip().lower() == \"fake news\"), axis=1)\n",
    "    r_score = df[real_cols].apply(lambda row: sum(1 for x in row if isinstance(x, str) and x.strip().lower() == \"real news\"), axis=1)\n",
    "    return f_score + r_score\n",
    "\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "df_t_claims['v']  = compute_v_metric(df_t_claims)\n",
    "df_m_claims['v']  = compute_v_metric(df_m_claims)\n",
    "df_mt_claims['v'] = compute_v_metric(df_mt_claims)\n",
    "\n",
    "df_claims = pd.concat([df_t_claims[['prolific_id', 'v']],\n",
    "                       df_m_claims[['prolific_id', 'v']],\n",
    "                       df_mt_claims[['prolific_id', 'v']]],\n",
    "                      ignore_index=True)\n",
    "# In case participants appear in multiple files, average their v scores.\n",
    "df_claims = df_claims.groupby('prolific_id', as_index=False).mean()\n",
    "\n",
    "# ============================\n",
    "# Step 3: Merge Affect Change with MIST v\n",
    "# ============================\n",
    "df_corr = pd.merge(df_affect_change, df_claims, on=\"prolific_id\", how=\"inner\")\n",
    "print(\"Merged dataset for correlation:\")\n",
    "print(df_corr.head())\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Step 4: Run Correlations\n",
    "# ============================\n",
    "# Pearson correlation between PA_change and v\n",
    "r_pa, p_pa = pearsonr(df_corr['PA_change'], df_corr['v'])\n",
    "# Pearson correlation between NA_change and v\n",
    "r_na, p_na = pearsonr(df_corr['NA_change'], df_corr['v'])\n",
    "\n",
    "print(\"\\nCorrelation between Positive Affect Change and v:\")\n",
    "print(f\"  Pearson r = {r_pa:.3f}, p-value = {p_pa:.3e}\")\n",
    "\n",
    "print(\"\\nCorrelation between Negative Affect Change and v:\")\n",
    "print(f\"  Pearson r = {r_na:.3f}, p-value = {p_na:.3e}\")"
   ],
   "id": "f293f0ae04703f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset for correlation:\n",
      "                prolific_id  PA_change  NA_change    v\n",
      "0  5484620efdf99b0379939c6a        0.0        0.0  7.0\n",
      "1  56bfcce79f7a1e0005fdca9e       -0.8        0.0  7.0\n",
      "2  56c984eb10a82f0006ffd111       -0.6        0.0  7.0\n",
      "3  57321c8ec63b5c000f367bb2        0.4        0.6  6.0\n",
      "4  5751f576a9de4b0006e557b9       -1.0       -0.4  7.0\n",
      "\n",
      "Correlation between Positive Affect Change and v:\n",
      "  Pearson r = -0.026, p-value = 6.453e-01\n",
      "\n",
      "Correlation between Negative Affect Change and v:\n",
      "  Pearson r = 0.073, p-value = 1.973e-01\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison with MIST Norm Values",
   "id": "bd12e5bc5fc6f311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T15:13:01.790778Z",
     "start_time": "2025-03-13T15:13:01.317693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Load Norms Data from Excel (for reference)\n",
    "# ===============================\n",
    "excel_path = get_working_dir() / 'data' / 'MIST8 - US Norm Tables - v2020.10.16.xlsx'\n",
    "norms_df = pd.read_excel(excel_path, sheet_name=\"Norms\")\n",
    "age_norms_df = pd.read_excel(excel_path, sheet_name=\"Age\")\n",
    "\n",
    "print(\"US Norms Table (Overall):\")\n",
    "print(norms_df.head())\n",
    "print(\"\\nUS Age Norms Table:\")\n",
    "print(age_norms_df.head())\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load and Process MIST Claims Data\n",
    "# ===============================\n",
    "def compute_f_r(df):\n",
    "    # Assumes the columns starting with \"claim_\" are ordered:\n",
    "    # first 4 = fake items, next 4 = real items.\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    f_score = df[fake_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"fake news\"),\n",
    "                                   axis=1)\n",
    "    r_score = df[real_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"real news\"),\n",
    "                                   axis=1)\n",
    "    return f_score, r_score\n",
    "\n",
    "# Load the three claims files.\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "# Compute f, r and v for each file.\n",
    "f_t, r_t = compute_f_r(df_t_claims)\n",
    "f_m, r_m = compute_f_r(df_m_claims)\n",
    "f_mt, r_mt = compute_f_r(df_mt_claims)\n",
    "\n",
    "df_t_claims['f']  = f_t\n",
    "df_t_claims['r']  = r_t\n",
    "df_t_claims['v']  = f_t + r_t\n",
    "\n",
    "df_m_claims['f']  = f_m\n",
    "df_m_claims['r']  = r_m\n",
    "df_m_claims['v']  = f_m + r_m\n",
    "\n",
    "df_mt_claims['f'] = f_mt\n",
    "df_mt_claims['r'] = r_mt\n",
    "df_mt_claims['v'] = f_mt + r_mt\n",
    "\n",
    "# Combine all claims data (average in case of duplicates).\n",
    "df_claims = pd.concat([\n",
    "    df_t_claims[['prolific_id', 'f', 'r', 'v']],\n",
    "    df_m_claims[['prolific_id', 'f', 'r', 'v']],\n",
    "    df_mt_claims[['prolific_id', 'f', 'r', 'v']]\n",
    "], ignore_index=True)\n",
    "df_claims = df_claims.groupby('prolific_id', as_index=False).mean()\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Compute Overall Sample Percentiles for v, f, and r\n",
    "# ===============================\n",
    "v_percentiles = np.percentile(df_claims['v'], [5,10,25,50,75,90,95])\n",
    "f_percentiles = np.percentile(df_claims['f'], [5,10,25,50,75,90,95])\n",
    "r_percentiles = np.percentile(df_claims['r'], [5,10,25,50,75,90,95])\n",
    "\n",
    "overall_results = pd.DataFrame({\n",
    "    \"Measure\": [\"v\", \"f\", \"r\"],\n",
    "    \"5th\": [v_percentiles[0], f_percentiles[0], r_percentiles[0]],\n",
    "    \"10th\": [v_percentiles[1], f_percentiles[1], r_percentiles[1]],\n",
    "    \"25th\": [v_percentiles[2], f_percentiles[2], r_percentiles[2]],\n",
    "    \"50th\": [v_percentiles[3], f_percentiles[3], r_percentiles[3]],\n",
    "    \"75th\": [v_percentiles[4], f_percentiles[4], r_percentiles[4]],\n",
    "    \"90th\": [v_percentiles[5], f_percentiles[5], r_percentiles[5]],\n",
    "    \"95th\": [v_percentiles[6], f_percentiles[6], r_percentiles[6]]\n",
    "})\n",
    "print(\"Overall Sample Percentiles:\")\n",
    "print(overall_results)\n",
    "\n",
    "overall_results.to_csv(results_dir / \"overall_percentiles.csv\", index=False)\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Compute Age Group Percentiles for v\n",
    "# ===============================\n",
    "# Load demographics data and extract age_group.\n",
    "df_t_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_demographics.csv', encoding='utf-8')\n",
    "df_m_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_demographics.csv', encoding='utf-8')\n",
    "df_mt_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_demographics.csv', encoding='utf-8')\n",
    "df_dem = pd.concat([df_t_dem, df_m_dem, df_mt_dem], ignore_index=True)\n",
    "df_dem['age_group'] = df_dem['age_group'].astype(str).str.strip()\n",
    "# Filter out rows with \"I prefer not to answer\"\n",
    "df_dem = df_dem[df_dem['age_group'] != \"I prefer not to answer\"]\n",
    "\n",
    "# Merge with claims data on prolific_id.\n",
    "df_merged = pd.merge(df_claims, df_dem[['prolific_id', 'age_group']], on=\"prolific_id\", how=\"inner\")\n",
    "\n",
    "# Our sample age groups may differ from the norm categories. \n",
    "# For demonstration, we compute percentiles for each age group in our sample.\n",
    "age_results = []\n",
    "age_groups = df_merged['age_group'].unique()\n",
    "for age in sorted(age_groups):\n",
    "    group = df_merged[df_merged['age_group'] == age]\n",
    "    v_pct = np.percentile(group['v'], [5,10,25,50,75,90,95])\n",
    "    age_results.append({\n",
    "         \"age_group\": age,\n",
    "         \"5th\": v_pct[0],\n",
    "         \"10th\": v_pct[1],\n",
    "         \"25th\": v_pct[2],\n",
    "         \"50th\": v_pct[3],\n",
    "         \"75th\": v_pct[4],\n",
    "         \"90th\": v_pct[5],\n",
    "         \"95th\": v_pct[6]\n",
    "    })\n",
    "age_results_df = pd.DataFrame(age_results)\n",
    "print(\"\\nAge Group Percentiles for v:\")\n",
    "print(age_results_df)\n",
    "\n",
    "age_results_df.to_csv(results_dir / \"age_group_percentiles.csv\", index=False)\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Manual Comparison Instructions\n",
    "# ===============================\n",
    "print(\"\\nPlease compare the CSV files 'overall_percentiles.csv' and 'age_group_percentiles.csv' saved in the directory:\")\n",
    "print(results_dir)\n",
    "print(\"with the published norm values from the Norms and Age sheets of the provided Excel file.\")\n"
   ],
   "id": "fa9fde3a319a5b70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Norms Table (Overall):\n",
      "  General (N = 3,479) - Format A  Unnamed: 1  Unnamed: 2 Unnamed: 3  \\\n",
      "0                    Discernment         NaN         NaN  Fake News   \n",
      "1                          Score  Percentile         NaN      Score   \n",
      "2                              1           1         NaN          1   \n",
      "3                              2           5         NaN          2   \n",
      "4                              3          12         NaN          3   \n",
      "\n",
      "   Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
      "0         NaN         NaN  Real News         NaN  \n",
      "1  Percentile         NaN      Score  Percentile  \n",
      "2          15         NaN          1          16  \n",
      "3          33         NaN          2          38  \n",
      "4          63         NaN          3          69  \n",
      "\n",
      "US Age Norms Table:\n",
      "  18-25: Emerging Adulthood (n = 390)  Unnamed: 1  Unnamed: 2 Unnamed: 3  \\\n",
      "0                         Discernment         NaN         NaN  Fake News   \n",
      "1                               Score  Percentile         NaN      Score   \n",
      "2                                   1           1         NaN          1   \n",
      "3                                   2           6         NaN          2   \n",
      "4                                   3          16         NaN          3   \n",
      "\n",
      "   Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  Unnamed: 8  Age   n  \\\n",
      "0         NaN         NaN  Real News         NaN         NaN   18  52   \n",
      "1  Percentile         NaN      Score  Percentile         NaN   19  37   \n",
      "2          18         NaN          1          19         NaN   20  65   \n",
      "3          42         NaN          2          44         NaN   21  37   \n",
      "4          71         NaN          3          73         NaN   22  45   \n",
      "\n",
      "   Unnamed: 11                    Category     n.1  \n",
      "0          NaN  Emerging Adulthood (18-25)   390.0  \n",
      "1          NaN     Early Adulthood (26-39)  1126.0  \n",
      "2          NaN    Middle Adulthood (40-59)  1122.0  \n",
      "3          NaN        Late Adulthood (60+)   825.0  \n",
      "4          NaN                         NaN     NaN  \n",
      "Overall Sample Percentiles:\n",
      "  Measure  5th  10th  25th  50th  75th  90th  95th\n",
      "0       v  3.0   3.0   5.0   7.0   7.0   8.0   8.0\n",
      "1       f  1.0   1.0   2.0   3.0   4.0   4.0   4.0\n",
      "2       r  1.0   2.0   3.0   4.0   4.0   4.0   4.0\n",
      "\n",
      "Age Group Percentiles for v:\n",
      "           age_group   5th  10th  25th  50th  75th  90th  95th\n",
      "0    18-25 years old  2.85   3.0  4.25   6.0   7.0   8.0   8.0\n",
      "1    26-35 years old  3.00   3.0  4.00   6.0   7.0   8.0   8.0\n",
      "2    36-50 years old  2.00   3.0  5.00   7.0   7.0   8.0   8.0\n",
      "3  Over 50 years old  3.30   4.6  6.00   7.0   8.0   8.0   8.0\n",
      "\n",
      "Please compare the CSV files 'overall_percentiles.csv' and 'age_group_percentiles.csv' saved in the directory:\n",
      "/Users/sergiopinto/Desktop/MemeFact/meta_study/results/mist\n",
      "with the published norm values from the Norms and Age sheets of the provided Excel file.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Veracity dscernment political orientation and education level",
   "id": "109d2abec272e29c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# Create plots directory for this analysis.\n",
    "results_dir = get_working_dir() / 'plots' / 'mist'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- Load and Merge Demographics Data -----\n",
    "# Load demographics files.\n",
    "df_t_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_demographics.csv', encoding='utf-8')\n",
    "df_m_dem  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_demographics.csv', encoding='utf-8')\n",
    "df_mt_dem = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_demographics.csv', encoding='utf-8')\n",
    "df_dem = pd.concat([df_t_dem, df_m_dem, df_mt_dem], ignore_index=True)\n",
    "\n",
    "# Ensure string type and strip whitespace for selected columns.\n",
    "for col in ['age_group', 'political_orientation', 'engagement_with_political_content', 'education_level']:\n",
    "    df_dem[col] = df_dem[col].astype(str).str.strip()\n",
    "\n",
    "# Filter out \"I prefer not to answer\" responses for select variables.\n",
    "for var in ['age_group', 'political_orientation', 'engagement_with_political_content']:\n",
    "    df_dem = df_dem[df_dem[var] != \"I prefer not to answer\"]\n",
    "\n",
    "# ----- Load Claims Data and Compute Veracity Metric (v) -----\n",
    "def compute_v_metric(df):\n",
    "    # Assumes columns starting with \"claim_\" are arranged so that the first 4 are fake items and the next 4 are real items.\n",
    "    claim_cols = [col for col in df.columns if col.startswith(\"claim_\")]\n",
    "    fake_cols = claim_cols[:4]\n",
    "    real_cols = claim_cols[4:8]\n",
    "    f_score = df[fake_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"fake news\"), axis=1)\n",
    "    r_score = df[real_cols].apply(lambda row: sum(1 for x in row \n",
    "                                                  if isinstance(x, str) and x.strip().lower() == \"real news\"), axis=1)\n",
    "    return f_score + r_score\n",
    "\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8')\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8')\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8')\n",
    "\n",
    "df_t_claims['v']  = compute_v_metric(df_t_claims)\n",
    "df_m_claims['v']  = compute_v_metric(df_m_claims)\n",
    "df_mt_claims['v'] = compute_v_metric(df_mt_claims)\n",
    "\n",
    "df_claims = pd.concat([\n",
    "    df_t_claims[['prolific_id', 'v']],\n",
    "    df_m_claims[['prolific_id', 'v']],\n",
    "    df_mt_claims[['prolific_id', 'v']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Merge the claims data with demographics on prolific_id.\n",
    "df_merged = pd.merge(df_claims, df_dem, on=\"prolific_id\", how=\"inner\")\n",
    "\n",
    "# ----- Prepare Categorical Variables -----\n",
    "# Define the desired orders.\n",
    "pol_order = [\"Very Liberal\", \"Moderately Liberal\", \"Moderate\", \"Moderately Conservative\", \"Very Conservative\"]\n",
    "edu_order = [\"Elementary Education\", \"High School\", \"Bachelor's\", \"Master's\", \"Phd\"]\n",
    "\n",
    "df_merged['political_orientation'] = pd.Categorical(df_merged['political_orientation'], categories=pol_order, ordered=True)\n",
    "df_merged['education_level'] = pd.Categorical(df_merged['education_level'], categories=edu_order, ordered=True)\n",
    "\n",
    "# ----- Step 1: Compute Subgroup Summary Statistics for v -----\n",
    "subgroup_stats = df_merged.groupby(['political_orientation', 'education_level'])['v'] \\\n",
    "                          .agg(['mean', 'std', 'count']).reset_index()\n",
    "subgroup_stats['sem'] = subgroup_stats['std'] / np.sqrt(subgroup_stats['count'])\n",
    "print(\"Veracity discernment (v) by Political Orientation and Education Level:\")\n",
    "print(subgroup_stats)\n",
    "\n",
    "# ----- Step 2: Visualize the Data with a Bar Plot -----\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a bar plot with political_orientation on x and education_level as the hue.\n",
    "ax = sns.barplot(data=df_merged, x=\"political_orientation\", y=\"v\", hue=\"education_level\", \n",
    "                 order=pol_order, hue_order=edu_order, ci=\"sd\")\n",
    "ax.set_xlabel(\"Political Orientation\")\n",
    "ax.set_ylabel(\"Mean Veracity Discernment (v)\")\n",
    "ax.set_title(\"Mean Veracity Discernment by Political Orientation and Education Level\")\n",
    "\n",
    "# Optionally, abbreviate education level labels for the legend.\n",
    "edu_abbr = {\"Elementary Education\": \"Elem\", \"High School\": \"HS\", \"Bachelor's\": \"BA\", \"Master's\": \"MA\", \"Phd\": \"PhD\"}\n",
    "new_labels = [edu_abbr.get(label.get_text(), label.get_text()) for label in ax.get_legend().get_texts()]\n",
    "for text, new in zip(ax.get_legend().get_texts(), new_labels):\n",
    "    text.set_text(new)\n",
    "ax.legend(title=\"Education Level\", loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"v_by_pol_and_edu.png\")\n",
    "plt.close()"
   ],
   "id": "5c0114a3826252d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def get_working_dir():\n",
    "    return Path.cwd()\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Aggregate Claims Data\n",
    "# ---------------------------\n",
    "# Load each file from the 'data/processed' directory.\n",
    "df_t_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 't_claims.csv', encoding='utf-8', dtype={'prolific_id': str})\n",
    "df_m_claims  = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_claims.csv', encoding='utf-8', dtype={'prolific_id': str})\n",
    "df_mt_claims = pd.read_csv(get_working_dir() / 'data' / 'processed' / 'm_t_claims.csv', encoding='utf-8', dtype={'prolific_id': str})\n",
    "\n",
    "# Concatenate all data frames.\n",
    "df_claims = pd.concat([df_t_claims, df_m_claims, df_mt_claims], ignore_index=True)\n",
    "print(\"Combined claims data shape:\", df_claims.shape)\n",
    "print(\"Head of combined claims data:\")\n",
    "print(df_claims.head())\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Identify MIST-8 Item Columns and Score Them\n",
    "# ---------------------------\n",
    "# Fetch all columns starting with \"claim_\"\n",
    "claim_cols = [col for col in df_claims.columns if col.startswith(\"claim_\")]\n",
    "\n",
    "# Assuming the first four are fake items and the next four are real items.\n",
    "fake_cols = claim_cols[:4]\n",
    "real_cols = claim_cols[4:8]\n",
    "\n",
    "print(\"\\nFake item columns:\", fake_cols)\n",
    "print(\"Real item columns:\", real_cols)\n",
    "\n",
    "# Define a scoring function: for fake items, 1 if response is \"fake news\"; for real items, 1 if response is \"real news\".\n",
    "def score_item(response, correct_answer):\n",
    "    if isinstance(response, str) and response.strip().lower() == correct_answer:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Build a new DataFrame X with the 8 binary scores.\n",
    "X = pd.DataFrame(index=df_claims.index)\n",
    "for col in fake_cols:\n",
    "    X[col] = df_claims[col].apply(lambda x: score_item(x, \"fake news\"))\n",
    "for col in real_cols:\n",
    "    X[col] = df_claims[col].apply(lambda x: score_item(x, \"real news\"))\n",
    "\n",
    "print(\"\\nFirst 5 rows of binary scored MIST-8 items:\")\n",
    "print(X.head())\n",
    "print(\"\\nDescriptive statistics for each item:\")\n",
    "print(X.describe())\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Compute McDonald’s Omega\n",
    "# ---------------------------\n",
    "# Step 1: Compute the correlation (or covariance) matrix of the 8 items.\n",
    "R = X.corr()\n",
    "print(\"\\nItem correlation matrix (R):\")\n",
    "print(R)\n",
    "\n",
    "# Step 2: Perform eigenvalue decomposition of the correlation matrix.\n",
    "eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "print(\"\\nEigenvalues of R:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "# Identify the largest eigenvalue and corresponding eigenvector.\n",
    "i_max = np.argmax(eigenvalues.real)\n",
    "r1 = eigenvalues.real[i_max]\n",
    "v1 = eigenvectors[:, i_max].real\n",
    "print(f\"\\nDominant eigenvalue (r1): {r1}\")\n",
    "print(\"Corresponding eigenvector (v1):\")\n",
    "print(v1)\n",
    "\n",
    "# Step 3: Estimate factor loadings.\n",
    "lambda_vec = np.sqrt(r1) * v1\n",
    "print(\"\\nEstimated factor loadings (lambda_vec):\")\n",
    "print(lambda_vec)\n",
    "\n",
    "# Compute unique variances (psi_i = 1 - lambda_i^2 for standardized items).\n",
    "unique_variances = 1 - lambda_vec**2\n",
    "print(\"\\nUnique variances for items (psi_i):\")\n",
    "print(unique_variances)\n",
    "\n",
    "# Total common variance is the square of the sum of loadings.\n",
    "common_variance = (lambda_vec.sum())**2\n",
    "# Total variance = common variance + sum of unique variances.\n",
    "total_variance = common_variance + unique_variances.sum()\n",
    "# McDonald's Omega\n",
    "omega_total = common_variance / total_variance\n",
    "\n",
    "print(f\"\\nMcDonald's Omega for the aggregated MIST-8 scale: {omega_total:.3f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Alternative Omega Calculation (Optional)\n",
    "# ---------------------------\n",
    "# Using an alternative formula that uses the number of items (k).\n",
    "k = X.shape[1]\n",
    "omega_alternative = (lambda_vec.sum()**2) / ((lambda_vec.sum()**2) + (k - (lambda_vec**2).sum()))\n",
    "print(f\"McDonald's Omega (alternative formula) for the aggregated MIST-8 scale: {omega_alternative:.3f}\")\n"
   ],
   "id": "9408ac6e06fa452a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
